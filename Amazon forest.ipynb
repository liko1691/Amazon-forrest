{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Data mining\\\\Kaggle Amazon Rainforest\\\\'\n",
    "train_dir = core_dir + 'train-jpg\\\\'\n",
    "test_dir = core_dir + 'test-jpg\\\\'\n",
    "\n",
    "cathegories = ['agriculture', 'artisinal_mine', 'bare_ground', \n",
    "                      'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', \n",
    "                      'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', \n",
    "                      'road', 'selective_logging', 'slash_burn', 'water']\n",
    "\n",
    "train_data = pd.read_csv(core_dir + 'train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подготовка обучающей выборки по имени категории\n",
    "принимает категорию, возвращает массив из 0 и 1 (1 - файл содержит категорию, 0 - иначе)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TrainingSetByCathegory(cathegory):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['image_name'] = train_data.image_name.values\n",
    "    df['indicator'] = np.zeros(train_data.shape[0])\n",
    "    \n",
    "    df.loc[df['image_name'].isin(cathegory_dict[cathegory]), ['indicator']] = 1\n",
    "    \n",
    "    return df.indicator.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция предназначенная для конвертации .jpg изображения в numpy массив\n",
    "если transparency = False, то каждая точка представляется 3 числами иначе - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ImageToNumpy(img_name, img_dir = train_dir, img_type = '.jpg', transparency = False):\n",
    "    img = Image.open(train_dir + img_name + img_type)\n",
    "    img.load()\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    img.close()\n",
    "    \n",
    "    if transparency:\n",
    "        return data\n",
    "    else:\n",
    "        return data[:, :, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создадим словарь категорий\n",
    "каждой категории будет соответствовать список файлов, в которых данная категория присутствует"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cathegory_dict = {cathegory: [] for cathegory in cathegories}\n",
    "\n",
    "ind = 0\n",
    "for tag in train_data.tags.values:\n",
    "    file_name = train_data.image_name[ind]\n",
    "    \n",
    "    cathegory_names_list = tag.split(' ')\n",
    "    \n",
    "    for cathegory in cathegory_names_list:\n",
    "        cathegory_dict[cathegory].append(file_name)\n",
    "    \n",
    "    ind += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(train_dir + train_data.image_name[0] + '.jpg')\n",
    "img.shape\n",
    "res = cv2.resize(img, (32, 32))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40479/40479 [08:25<00:00, 80.09it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for img_name in tqdm(train_data.image_name.values):\n",
    "    \n",
    "    img = cv2.imread(train_dir + img_name + '.jpg')\n",
    "    res = cv2.resize(img, (32, 32))\n",
    "    X.append(res)\n",
    "    \n",
    "X = np.array(X, np.float16) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40479, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Настройка и  обучение сети для одной категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 20 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# загрузка изображений и их приведение к подходящему для обработки виду\n",
    "num_train = 40479\n",
    "depth = 32 \n",
    "height = 32\n",
    "width = 3 \n",
    "num_classes = len(cathegories)\n",
    "y_train = TrainingSetByCathegory(cathegories[0]) #y_train labels (для категории_0 1, если подходит, 0-иначе)\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "X_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\horch\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  app.launch_new_instance()\n",
      "D:\\Users\\horch\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\horch\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\horch\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "D:\\Users\\horch\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\Users\\horch\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/20\n",
      "36431/36431 [==============================] - 791s - loss: 0.5240 - acc: 0.7367 - val_loss: 0.4459 - val_acc: 0.7789\n",
      "Epoch 2/20\n",
      "36431/36431 [==============================] - 820s - loss: 0.4010 - acc: 0.8165 - val_loss: 0.4270 - val_acc: 0.8281\n",
      "Epoch 3/20\n",
      "36431/36431 [==============================] - 792s - loss: 0.3600 - acc: 0.8409 - val_loss: 0.3464 - val_acc: 0.8446\n",
      "Epoch 4/20\n",
      "36431/36431 [==============================] - 788s - loss: 0.3497 - acc: 0.8450 - val_loss: 0.3346 - val_acc: 0.8530\n",
      "Epoch 5/20\n",
      "36431/36431 [==============================] - 783s - loss: 0.3459 - acc: 0.8503 - val_loss: 0.3548 - val_acc: 0.8461\n",
      "Epoch 6/20\n",
      "36431/36431 [==============================] - 781s - loss: 0.3373 - acc: 0.8528 - val_loss: 0.3381 - val_acc: 0.8582\n",
      "Epoch 7/20\n",
      "36431/36431 [==============================] - 784s - loss: 0.3321 - acc: 0.8536 - val_loss: 0.3352 - val_acc: 0.8466\n",
      "Epoch 8/20\n",
      "36431/36431 [==============================] - 781s - loss: 0.3270 - acc: 0.8579 - val_loss: 0.3210 - val_acc: 0.8634\n",
      "Epoch 9/20\n",
      "36431/36431 [==============================] - 782s - loss: 0.3269 - acc: 0.8565 - val_loss: 0.3346 - val_acc: 0.8542\n",
      "Epoch 10/20\n",
      "36431/36431 [==============================] - 783s - loss: 0.3241 - acc: 0.8583 - val_loss: 0.3459 - val_acc: 0.8582\n",
      "Epoch 11/20\n",
      "36431/36431 [==============================] - 783s - loss: 0.3239 - acc: 0.8609 - val_loss: 0.3271 - val_acc: 0.8661\n",
      "Epoch 12/20\n",
      "36431/36431 [==============================] - 782s - loss: 0.3154 - acc: 0.8636 - val_loss: 0.3242 - val_acc: 0.8617\n",
      "Epoch 13/20\n",
      "36431/36431 [==============================] - 782s - loss: 0.3113 - acc: 0.8660 - val_loss: 0.3132 - val_acc: 0.8671\n",
      "Epoch 14/20\n",
      "36431/36431 [==============================] - 783s - loss: 0.3119 - acc: 0.8671 - val_loss: 0.3171 - val_acc: 0.8654\n",
      "Epoch 15/20\n",
      "36431/36431 [==============================] - 783s - loss: 0.3055 - acc: 0.8710 - val_loss: 0.3177 - val_acc: 0.8661\n",
      "Epoch 16/20\n",
      "36431/36431 [==============================] - 797s - loss: 0.3051 - acc: 0.8703 - val_loss: 0.3234 - val_acc: 0.8656\n",
      "Epoch 17/20\n",
      "36431/36431 [==============================] - 785s - loss: 0.3010 - acc: 0.8726 - val_loss: 0.3265 - val_acc: 0.8599\n",
      "Epoch 18/20\n",
      "36431/36431 [==============================] - 784s - loss: 0.2990 - acc: 0.8738 - val_loss: 0.3088 - val_acc: 0.8664\n",
      "Epoch 19/20\n",
      "36431/36431 [==============================] - 783s - loss: 0.2922 - acc: 0.8766 - val_loss: 0.3316 - val_acc: 0.8540\n",
      "Epoch 20/20\n",
      "36431/36431 [==============================] - 792s - loss: 0.2898 - acc: 0.8775 - val_loss: 0.3201 - val_acc: 0.8664\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b39b260e40ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m           verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Evaluate the trained model on the test set!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(depth, height, width)) # N.B. depth goes first in Keras!\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(X_train, Y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"model.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
