{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from natsort import natsorted, ns\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PathDetermination(user):\n",
    "    if user == 'litvin_home':\n",
    "        core_dir = 'C:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "    \n",
    "    if user == 'litvin_office':\n",
    "        core_dir = 'D:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "        train_dir = core_dir + 'train-jpg\\\\'\n",
    "        test_dir = core_dir + 'test-jpg\\\\'\n",
    "        add_test_dir = core_dir + 'test-jpg-additional\\\\'\n",
    "    \n",
    "    if user == 'savina':\n",
    "        core_dir = 'Z:\\\\Kaggle Amazon Rainforest\\\\'\n",
    "        train_dir = ''\n",
    "        test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg\\\\'\n",
    "        add_test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg-additional\\\\'\n",
    "    \n",
    "    return core_dir, train_dir, test_dir, add_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_dir, train_dir, test_dir, add_test_dir = PathDetermination('litvin_office')\n",
    "#core_dir, test_dir, add_test_dir = path_determination('savina')\n",
    "\n",
    "test_data_names = natsorted(os.listdir(test_dir), key=lambda y: y.lower())\n",
    "add_test_data_names = os.listdir(add_test_dir)\n",
    "\n",
    "\n",
    "cathegories = ['agriculture', 'artisinal_mine', 'bare_ground', \n",
    "                      'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', \n",
    "                      'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', \n",
    "                      'road', 'selective_logging', 'slash_burn', 'water']\n",
    "\n",
    "train_data = pd.read_csv(core_dir + 'train_v2.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BinimialPrediction(x, treshold = 0.5):\n",
    "    result = np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if x[i, j] >= treshold:\n",
    "                result[i, j] = 1\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def FBettaScore(x_true, x_predicted, betta = 2):\n",
    "    if len(x_true) == len(x_predicted):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        for i in range(len(x_predicted)):\n",
    "            if x_true[i] == 1 and x_predicted[i] == 1:\n",
    "                tp += 1\n",
    "            \n",
    "            if x_true[i] == 0 and x_predicted[i] == 1:\n",
    "                fp += 1\n",
    "            \n",
    "            if x_true[i] == 1 and x_predicted[i] == 0:\n",
    "                fn += 1\n",
    "        \n",
    "        if tp == 0 or (tp + fp) == 0 or (tp + fn) == 0:\n",
    "            return(0)\n",
    "        else:\n",
    "            precision = tp/(tp + fp)\n",
    "            recall = tp/(tp + fn)\n",
    "            \n",
    "            return((1 + betta**2)*precision*recall/(betta**2*precision + recall))\n",
    "    else:\n",
    "        print('FBettaScore error! len(x_true) != len(x_predicted)')\n",
    "\n",
    "def AvgFBettaScore(x_true, x_predicted, betta = 2):\n",
    "    result = 0\n",
    "    n = x_true.shape[0]\n",
    "    \n",
    "    x_predicted = BinimialPrediction(x_predicted)\n",
    "    \n",
    "    for i in range(n):\n",
    "        result += FBettaScore(x_true[i, :], x_predicted[i, :], betta)\n",
    "    \n",
    "    return(result/n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DataPreperation(data_type, img_size = (32, 32), rotation = [0], \n",
    "                    test_img_dir = [test_dir, add_test_dir], shuffle = False):\n",
    "    if data_type == 'train':\n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        for img_name in tqdm(train_data.image_name.values):\n",
    "    \n",
    "            img = cv2.imread(train_dir + img_name + '.jpg')\n",
    "            img_resized = cv2.resize(img, img_size)\n",
    "            \n",
    "            img_tags = train_data[train_data['image_name'] == img_name]['tags'].values[0].split(' ')\n",
    "            y = np.zeros(len(cathegories))\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                if cathegories[i] in img_tags:\n",
    "                    y[i] = 1\n",
    "            \n",
    "            for angle in rotation:\n",
    "                img = imutils.rotate(img_resized, angle)\n",
    "                X.append(img)\n",
    "                Y.append(y) \n",
    "        \n",
    "        X = np.array(X, np.float16) / 255.\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        if shuffle:\n",
    "            ind = np.random.choice(np.arange(X.shape[0]), X.shape[0], replace= False)\n",
    "            X = X[ind]\n",
    "            Y = Y[ind]\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    if data_type == 'test':\n",
    "        X_test = []\n",
    "        X_test_names = []\n",
    "\n",
    "        for img_dir in test_img_dir:\n",
    "            img_dir_names = os.listdir(img_dir)\n",
    "            \n",
    "            for img_name in tqdm(img_dir_names):\n",
    "                if img_name.endswith(\"jpg\"):\n",
    "                    img = cv2.imread(img_dir + img_name)\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    \n",
    "                    X_test.append(img)\n",
    "                    X_test_names.append(img_name)\n",
    "    \n",
    "        X_test = np.array(X_test, np.float16) / 255.\n",
    "        \n",
    "        return X_test, X_test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40479/40479 [08:08<00:00, 82.91it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = DataPreperation('train', rotation = [0, 90, 180, 270], shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиение выборки на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Настройка и  обучение сети для всех категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 50 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# загрузка изображений и их приведение к подходящему для обработки виду\n",
    "num_train = X.shape[0]\n",
    "depth = 32 \n",
    "height = 32\n",
    "width = 3 \n",
    "\n",
    "num_classes = len(cathegories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113341 samples, validate on 48575 samples\n",
      "Epoch 1/50\n",
      "113341/113341 [==============================] - 644s - loss: 0.1896 - acc: 0.9259 - val_loss: 0.1530 - val_acc: 0.9400\n",
      "Epoch 2/50\n",
      "113341/113341 [==============================] - 642s - loss: 0.1461 - acc: 0.9427 - val_loss: 0.1323 - val_acc: 0.9473\n",
      "Epoch 3/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1357 - acc: 0.9464 - val_loss: 0.1243 - val_acc: 0.9501\n",
      "Epoch 4/50\n",
      "113341/113341 [==============================] - 642s - loss: 0.1290 - acc: 0.9492 - val_loss: 0.1234 - val_acc: 0.9515\n",
      "Epoch 5/50\n",
      "113341/113341 [==============================] - 642s - loss: 0.1251 - acc: 0.9509 - val_loss: 0.1179 - val_acc: 0.9535\n",
      "Epoch 6/50\n",
      "113341/113341 [==============================] - 640s - loss: 0.1205 - acc: 0.9528 - val_loss: 0.1176 - val_acc: 0.9539\n",
      "Epoch 7/50\n",
      "113341/113341 [==============================] - 640s - loss: 0.1175 - acc: 0.9541 - val_loss: 0.1136 - val_acc: 0.9553\n",
      "Epoch 8/50\n",
      "113341/113341 [==============================] - 640s - loss: 0.1151 - acc: 0.9550 - val_loss: 0.1093 - val_acc: 0.9569\n",
      "Epoch 9/50\n",
      "113341/113341 [==============================] - 640s - loss: 0.1129 - acc: 0.9559 - val_loss: 0.1132 - val_acc: 0.9554\n",
      "Epoch 10/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1118 - acc: 0.9562 - val_loss: 0.1090 - val_acc: 0.9568\n",
      "Epoch 11/50\n",
      "113341/113341 [==============================] - 642s - loss: 0.1100 - acc: 0.9569 - val_loss: 0.1084 - val_acc: 0.9576\n",
      "Epoch 12/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1081 - acc: 0.9574 - val_loss: 0.1091 - val_acc: 0.9571\n",
      "Epoch 13/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1073 - acc: 0.9579 - val_loss: 0.1079 - val_acc: 0.9575\n",
      "Epoch 14/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.1056 - acc: 0.9583 - val_loss: 0.1086 - val_acc: 0.9575\n",
      "Epoch 15/50\n",
      "113341/113341 [==============================] - 644s - loss: 0.1041 - acc: 0.9589 - val_loss: 0.1085 - val_acc: 0.9578\n",
      "Epoch 16/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1034 - acc: 0.9592 - val_loss: 0.1079 - val_acc: 0.9578\n",
      "Epoch 17/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1020 - acc: 0.9597 - val_loss: 0.1110 - val_acc: 0.9569\n",
      "Epoch 18/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.1007 - acc: 0.9601 - val_loss: 0.1067 - val_acc: 0.9585\n",
      "Epoch 19/50\n",
      "113341/113341 [==============================] - 641s - loss: 0.0998 - acc: 0.9604 - val_loss: 0.1069 - val_acc: 0.9584\n",
      "Epoch 20/50\n",
      "113341/113341 [==============================] - 640s - loss: 0.0986 - acc: 0.9608 - val_loss: 0.1077 - val_acc: 0.9576\n",
      "Epoch 21/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0970 - acc: 0.9614 - val_loss: 0.1112 - val_acc: 0.9570\n",
      "Epoch 22/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0965 - acc: 0.9615 - val_loss: 0.1100 - val_acc: 0.9581\n",
      "Epoch 23/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0950 - acc: 0.9622 - val_loss: 0.1098 - val_acc: 0.9580\n",
      "Epoch 24/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0940 - acc: 0.9625 - val_loss: 0.1091 - val_acc: 0.9582\n",
      "Epoch 25/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0935 - acc: 0.9628 - val_loss: 0.1113 - val_acc: 0.9575\n",
      "Epoch 26/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0921 - acc: 0.9634 - val_loss: 0.1121 - val_acc: 0.9580\n",
      "Epoch 27/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0915 - acc: 0.9634 - val_loss: 0.1089 - val_acc: 0.9580\n",
      "Epoch 28/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0907 - acc: 0.9638 - val_loss: 0.1117 - val_acc: 0.9581\n",
      "Epoch 29/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0898 - acc: 0.9640 - val_loss: 0.1085 - val_acc: 0.9584\n",
      "Epoch 30/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0891 - acc: 0.9642 - val_loss: 0.1106 - val_acc: 0.9576\n",
      "Epoch 31/50\n",
      "113341/113341 [==============================] - 651s - loss: 0.0880 - acc: 0.9648 - val_loss: 0.1111 - val_acc: 0.9582\n",
      "Epoch 32/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0875 - acc: 0.9648 - val_loss: 0.1116 - val_acc: 0.9581\n",
      "Epoch 33/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0870 - acc: 0.9651 - val_loss: 0.1115 - val_acc: 0.9576\n",
      "Epoch 34/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0865 - acc: 0.9653 - val_loss: 0.1139 - val_acc: 0.9581\n",
      "Epoch 35/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0854 - acc: 0.9659 - val_loss: 0.1139 - val_acc: 0.9570\n",
      "Epoch 36/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0849 - acc: 0.9659 - val_loss: 0.1135 - val_acc: 0.9576\n",
      "Epoch 37/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0844 - acc: 0.9660 - val_loss: 0.1143 - val_acc: 0.9575\n",
      "Epoch 38/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0838 - acc: 0.9664 - val_loss: 0.1152 - val_acc: 0.9574\n",
      "Epoch 39/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0832 - acc: 0.9665 - val_loss: 0.1153 - val_acc: 0.9571\n",
      "Epoch 40/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0827 - acc: 0.9668 - val_loss: 0.1162 - val_acc: 0.9577\n",
      "Epoch 41/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0825 - acc: 0.9668 - val_loss: 0.1149 - val_acc: 0.9577\n",
      "Epoch 42/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0814 - acc: 0.9673 - val_loss: 0.1150 - val_acc: 0.9576\n",
      "Epoch 43/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0811 - acc: 0.9674 - val_loss: 0.1200 - val_acc: 0.9571\n",
      "Epoch 44/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0808 - acc: 0.9675 - val_loss: 0.1157 - val_acc: 0.9579\n",
      "Epoch 45/50\n",
      "113341/113341 [==============================] - 648s - loss: 0.0809 - acc: 0.9675 - val_loss: 0.1171 - val_acc: 0.9565\n",
      "Epoch 46/50\n",
      "113341/113341 [==============================] - 645s - loss: 0.0799 - acc: 0.9679 - val_loss: 0.1167 - val_acc: 0.9579\n",
      "Epoch 47/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0795 - acc: 0.9681 - val_loss: 0.1159 - val_acc: 0.9578\n",
      "Epoch 48/50\n",
      "113341/113341 [==============================] - 646s - loss: 0.0795 - acc: 0.9681 - val_loss: 0.1167 - val_acc: 0.9575\n",
      "Epoch 49/50\n",
      "113341/113341 [==============================] - 648s - loss: 0.0787 - acc: 0.9683 - val_loss: 0.1195 - val_acc: 0.9569\n",
      "Epoch 50/50\n",
      "113341/113341 [==============================] - 647s - loss: 0.0788 - acc: 0.9683 - val_loss: 0.1194 - val_acc: 0.9576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x108aceb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(depth, height, width)) # N.B. depth goes first in Keras!\n",
    "\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "\n",
    "flat = Flatten()(drop_2)\n",
    "\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "out = Dense(num_classes, activation='sigmoid')(drop_3)\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"test_model_50e rotate.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "\n",
    "model.save_weights(\"test_model_50e rotate.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('test_model_50e rotate.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('test_model_50e rotate.h5')\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для формирования тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40669/40669 [09:21<00:00, 72.45it/s]\n",
      "100%|████████████████████████████████████| 20522/20522 [03:51<00:00, 88.50it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, X_test_names = DataPreperation('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakePrediction(data, model, img_names = X_test_names, treshold = 0.4, file_name = 'prediction.csv'):\n",
    "    if type(treshold) == float:\n",
    "        prediction = model.predict(data)\n",
    "        f = open(file_name, 'w')\n",
    "        f.write('image_name,tags\\n')\n",
    "        \n",
    "        for i in tqdm(range(prediction.shape[0])):\n",
    "            line = img_names[i][0:-4] + ','\n",
    "            for j in range(prediction.shape[1]):\n",
    "                if prediction[i, j] >= treshold:\n",
    "                    line += cathegories[j] + ' '\n",
    "            \n",
    "            f.write(line + '\\n')\n",
    "        \n",
    "        f.close()\n",
    "    \n",
    "    elif type(treshold) == list:\n",
    "        if len(treshold) == data.shape[1]:\n",
    "            prediction = model.predict(data)\n",
    "            f = open(file_name, 'w')\n",
    "            f.write('image_name,tags\\n')\n",
    "\n",
    "            for i in tqdm(range(prediction.shape[0])):\n",
    "                line = img_names[i][0:-4] + ','\n",
    "                for j in range(prediction.shape[1]):\n",
    "                    if prediction[i, j] >= treshold[j]:\n",
    "                        line += cathegories[j] + ' '\n",
    "                \n",
    "                f.write(line + '\\n')\n",
    "            \n",
    "            f.close()\n",
    "        \n",
    "        else:\n",
    "            print('wrong treshold length. must be %d' %data.shape[1])\n",
    "    \n",
    "    else:\n",
    "        print('treshold type must be float or list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 61191/61191 [00:02<00:00, 30143.35it/s]\n"
     ]
    }
   ],
   "source": [
    "MakePrediction(X_test, loaded_model, treshold = 0.2, file_name = 'prediction 02 rot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
