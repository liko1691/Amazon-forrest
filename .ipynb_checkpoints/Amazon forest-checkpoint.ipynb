{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from natsort import natsorted, ns\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#core_dir = 'Z:\\\\Kaggle Amazon Rainforest\\\\'\n",
    "#core_dir = 'C:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "core_dir = 'D:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "\n",
    "train_dir = core_dir + 'train-jpg\\\\'\n",
    "test_dir = core_dir + 'test-jpg\\\\'\n",
    "test_data_names = natsorted(os.listdir(test_dir), key=lambda y: y.lower())\n",
    "\n",
    "\n",
    "cathegories = ['agriculture', 'artisinal_mine', 'bare_ground', \n",
    "                      'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', \n",
    "                      'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', \n",
    "                      'road', 'selective_logging', 'slash_burn', 'water']\n",
    "\n",
    "#train_data = pd.read_csv(core_dir + 'train.csv')\n",
    "train_data = pd.read_csv(core_dir + 'train_v2.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подготовка тренировочной выборки по имени категории\n",
    "принимает категорию, возвращает массив из 0 и 1 (1 - файл содержит категорию, 0 - иначе)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TrainingSetByCathegory(cathegory):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['image_name'] = train_data.image_name.values\n",
    "    df['indicator'] = np.zeros(train_data.shape[0])\n",
    "    \n",
    "    df.loc[df['image_name'].isin(cathegory_dict[cathegory]), ['indicator']] = 1\n",
    "    \n",
    "    return df.indicator.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование полной (по всем категориям) тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TestSetWhole(data, col_name = 'tags'):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(cathegories)\n",
    "    \n",
    "    result = np.zeros((data.shape[0], len(cathegories)))\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        tag = data[col_name][i]\n",
    "        tag = tag.split(' ')\n",
    "        v = encoder.transform(tag)\n",
    "        \n",
    "        for j in v:\n",
    "            result[i, j] = 1\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция предназначенная для конвертации .jpg изображения в numpy массив\n",
    "если transparency = False, то каждая точка представляется 3 числами иначе - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ImageToNumpy(img_name, img_dir = train_dir, img_type = '.jpg', transparency = False):\n",
    "    img = Image.open(train_dir + img_name + img_type)\n",
    "    img.load()\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    img.close()\n",
    "    \n",
    "    if transparency:\n",
    "        return data\n",
    "    else:\n",
    "        return data[:, :, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для конвертации вектора компоненты которого принадлежат [0, 1] в вектор из 0 и 1 при заданном пороге (treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BinimialPrediction(x, treshold = 0.5):\n",
    "    result = np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if x[i, j] >= treshold:\n",
    "                result[i, j] = 1\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def FBettaScore(x_true, x_predicted, betta = 2):\n",
    "    if len(x_true) == len(x_predicted):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        for i in range(len(x_predicted)):\n",
    "            if x_true[i] == 1 and x_predicted[i] == 1:\n",
    "                tp += 1\n",
    "            \n",
    "            if x_true[i] == 0 and x_predicted[i] == 1:\n",
    "                fp += 1\n",
    "            \n",
    "            if x_true[i] == 1 and x_predicted[i] == 0:\n",
    "                fn += 1\n",
    "        \n",
    "        if tp == 0 or (tp + fp) == 0 or (tp + fn) == 0:\n",
    "            return(0)\n",
    "        else:\n",
    "            precision = tp/(tp + fp)\n",
    "            recall = tp/(tp + fn)\n",
    "            \n",
    "            #print(precision, recall)\n",
    "            return((1 + betta**2)*precision*recall/(betta**2*precision + recall))\n",
    "    else:\n",
    "        print('FBettaScore error! len(x_true) != len(x_predicted)')\n",
    "\n",
    "def AvgFBettaScore(x_true, x_predicted, betta = 2):\n",
    "    result = 0\n",
    "    n = x_true.shape[0]\n",
    "    \n",
    "    x_predicted = BinimialPrediction(x_predicted)\n",
    "    \n",
    "    for i in range(n):\n",
    "        result += FBettaScore(x_true[i, :], x_predicted[i, :], betta)\n",
    "    \n",
    "    return(result/n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создадим словарь категорий\n",
    "каждой категории будет соответствовать список файлов, в которых данная категория присутствует"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cathegory_dict = {cathegory: [] for cathegory in cathegories}\n",
    "\n",
    "ind = 0\n",
    "for tag in train_data.tags.values:\n",
    "    file_name = train_data.image_name[ind]\n",
    "    \n",
    "    cathegory_names_list = tag.split(' ')\n",
    "    \n",
    "    for cathegory in cathegory_names_list:\n",
    "        cathegory_dict[cathegory].append(file_name)\n",
    "    \n",
    "    ind += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 40479/40479 [01:07<00:00, 601.25it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for img_name in tqdm(train_data.image_name.values):\n",
    "    \n",
    "    img = cv2.imread(train_dir + img_name + '.jpg')\n",
    "    res = cv2.resize(img, (32, 32))\n",
    "    X.append(res)\n",
    "    \n",
    "X = np.array(X, np.float16) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = TestSetWhole(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиение выборки на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Настройка и  обучение сети для всех категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 500 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# загрузка изображений и их приведение к подходящему для обработки виду\n",
    "num_train = X.shape[0]\n",
    "depth = 32 \n",
    "height = 32\n",
    "width = 3 \n",
    "\n",
    "num_classes = len(cathegories)\n",
    "\n",
    "#y_train = TrainingSetByCathegory(cathegories[0]) #y_train labels (для категории_0 1, если подходит, 0-иначе)\n",
    "#Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "#X_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef f2_metric(y_true, y_pred):\\n    result = []\\n    for i in range(y_true.shape[0]):\\n        result.append(fbeta_score(y_true[i, :], y_pred[i, :], 2))\\n    \\n    result = np.mean(result)\\n    \\n    return(result)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# переделать\n",
    "'''\n",
    "def f2_metric(y_true, y_pred):\n",
    "    result = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        result.append(fbeta_score(y_true[i, :], y_pred[i, :], 2))\n",
    "    \n",
    "    result = np.mean(result)\n",
    "    \n",
    "    return(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22668 samples, validate on 5667 samples\n",
      "Epoch 1/500\n",
      "22668/22668 [==============================] - 106s - loss: 0.2421 - acc: 0.9076 - val_loss: 0.2051 - val_acc: 0.9173\n",
      "Epoch 2/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.1897 - acc: 0.9252 - val_loss: 0.1739 - val_acc: 0.9311\n",
      "Epoch 3/500\n",
      "22668/22668 [==============================] - 104s - loss: 0.1707 - acc: 0.9331 - val_loss: 0.1657 - val_acc: 0.9353\n",
      "Epoch 4/500\n",
      "22668/22668 [==============================] - 106s - loss: 0.1608 - acc: 0.9366 - val_loss: 0.1735 - val_acc: 0.9317\n",
      "Epoch 5/500\n",
      "22668/22668 [==============================] - 105s - loss: 0.1582 - acc: 0.9379 - val_loss: 0.1480 - val_acc: 0.9407\n",
      "Epoch 6/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.1478 - acc: 0.9419 - val_loss: 0.1500 - val_acc: 0.9404\n",
      "Epoch 7/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.1468 - acc: 0.9421 - val_loss: 0.1460 - val_acc: 0.9430\n",
      "Epoch 8/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.1411 - acc: 0.9443 - val_loss: 0.1434 - val_acc: 0.9436\n",
      "Epoch 9/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.1384 - acc: 0.9454 - val_loss: 0.1427 - val_acc: 0.9436\n",
      "Epoch 10/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.1357 - acc: 0.9458 - val_loss: 0.1384 - val_acc: 0.9450\n",
      "Epoch 11/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.1322 - acc: 0.9472 - val_loss: 0.1376 - val_acc: 0.9465\n",
      "Epoch 12/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.1327 - acc: 0.9469 - val_loss: 0.1385 - val_acc: 0.9441\n",
      "Epoch 13/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.1293 - acc: 0.9481 - val_loss: 0.1317 - val_acc: 0.9473\n",
      "Epoch 14/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.1273 - acc: 0.9489 - val_loss: 0.1309 - val_acc: 0.9484\n",
      "Epoch 15/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.1262 - acc: 0.9497 - val_loss: 0.1316 - val_acc: 0.9488\n",
      "Epoch 16/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.1247 - acc: 0.9504 - val_loss: 0.1293 - val_acc: 0.9493\n",
      "Epoch 17/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.1229 - acc: 0.9509 - val_loss: 0.1315 - val_acc: 0.9481\n",
      "Epoch 18/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.1204 - acc: 0.9521 - val_loss: 0.1238 - val_acc: 0.9511\n",
      "Epoch 19/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.1195 - acc: 0.9523 - val_loss: 0.1288 - val_acc: 0.9497\n",
      "Epoch 20/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.1174 - acc: 0.9527 - val_loss: 0.1247 - val_acc: 0.9517\n",
      "Epoch 21/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.1175 - acc: 0.9530 - val_loss: 0.1265 - val_acc: 0.9500\n",
      "Epoch 22/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.1145 - acc: 0.9539 - val_loss: 0.1495 - val_acc: 0.9418\n",
      "Epoch 23/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.1137 - acc: 0.9543 - val_loss: 0.1265 - val_acc: 0.9500\n",
      "Epoch 24/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.1116 - acc: 0.9554 - val_loss: 0.1243 - val_acc: 0.9517\n",
      "Epoch 25/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.1110 - acc: 0.9555 - val_loss: 0.1270 - val_acc: 0.9506\n",
      "Epoch 26/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.1083 - acc: 0.9566 - val_loss: 0.1270 - val_acc: 0.9507\n",
      "Epoch 27/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.1065 - acc: 0.9571 - val_loss: 0.1282 - val_acc: 0.9515\n",
      "Epoch 28/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.1051 - acc: 0.9578 - val_loss: 0.1266 - val_acc: 0.9513\n",
      "Epoch 29/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.1023 - acc: 0.9591 - val_loss: 0.1270 - val_acc: 0.9528\n",
      "Epoch 30/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.1004 - acc: 0.9596 - val_loss: 0.1257 - val_acc: 0.9531\n",
      "Epoch 31/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0981 - acc: 0.9606 - val_loss: 0.1260 - val_acc: 0.9528\n",
      "Epoch 32/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0971 - acc: 0.9608 - val_loss: 0.1261 - val_acc: 0.9530\n",
      "Epoch 33/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0946 - acc: 0.9618 - val_loss: 0.1268 - val_acc: 0.9537\n",
      "Epoch 34/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0935 - acc: 0.9621 - val_loss: 0.1305 - val_acc: 0.9523\n",
      "Epoch 35/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0953 - acc: 0.9614 - val_loss: 0.1298 - val_acc: 0.9523\n",
      "Epoch 36/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0894 - acc: 0.9638 - val_loss: 0.1319 - val_acc: 0.9527\n",
      "Epoch 37/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0888 - acc: 0.9639 - val_loss: 0.1404 - val_acc: 0.9504\n",
      "Epoch 38/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0873 - acc: 0.9644 - val_loss: 0.1362 - val_acc: 0.9515\n",
      "Epoch 39/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0851 - acc: 0.9656 - val_loss: 0.1327 - val_acc: 0.9537\n",
      "Epoch 40/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0848 - acc: 0.9657 - val_loss: 0.1399 - val_acc: 0.9496\n",
      "Epoch 41/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0836 - acc: 0.9663 - val_loss: 0.1368 - val_acc: 0.9520\n",
      "Epoch 42/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0823 - acc: 0.9664 - val_loss: 0.1401 - val_acc: 0.9525\n",
      "Epoch 43/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0801 - acc: 0.9679 - val_loss: 0.1427 - val_acc: 0.9509\n",
      "Epoch 44/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0793 - acc: 0.9680 - val_loss: 0.1433 - val_acc: 0.9504\n",
      "Epoch 45/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0788 - acc: 0.9682 - val_loss: 0.1418 - val_acc: 0.9514\n",
      "Epoch 46/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0768 - acc: 0.9688 - val_loss: 0.1393 - val_acc: 0.9524\n",
      "Epoch 47/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0753 - acc: 0.9698 - val_loss: 0.1463 - val_acc: 0.9528\n",
      "Epoch 48/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0747 - acc: 0.9698 - val_loss: 0.1431 - val_acc: 0.9529\n",
      "Epoch 49/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0725 - acc: 0.9706 - val_loss: 0.1488 - val_acc: 0.9512\n",
      "Epoch 50/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0702 - acc: 0.9712 - val_loss: 0.1508 - val_acc: 0.9520\n",
      "Epoch 51/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0705 - acc: 0.9715 - val_loss: 0.1485 - val_acc: 0.9526\n",
      "Epoch 52/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0700 - acc: 0.9717 - val_loss: 0.1544 - val_acc: 0.9515\n",
      "Epoch 53/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0680 - acc: 0.9726 - val_loss: 0.1465 - val_acc: 0.9517\n",
      "Epoch 54/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0701 - acc: 0.9717 - val_loss: 0.1486 - val_acc: 0.9521\n",
      "Epoch 55/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0650 - acc: 0.9734 - val_loss: 0.1531 - val_acc: 0.9528\n",
      "Epoch 56/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0650 - acc: 0.9738 - val_loss: 0.1573 - val_acc: 0.9512\n",
      "Epoch 57/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0641 - acc: 0.9742 - val_loss: 0.1580 - val_acc: 0.9511\n",
      "Epoch 58/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0650 - acc: 0.9738 - val_loss: 0.1544 - val_acc: 0.9527\n",
      "Epoch 59/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0645 - acc: 0.9740 - val_loss: 0.1600 - val_acc: 0.9523\n",
      "Epoch 60/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0613 - acc: 0.9755 - val_loss: 0.1564 - val_acc: 0.9522\n",
      "Epoch 61/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0604 - acc: 0.9757 - val_loss: 0.1599 - val_acc: 0.9503\n",
      "Epoch 62/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0613 - acc: 0.9753 - val_loss: 0.1576 - val_acc: 0.9509\n",
      "Epoch 63/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0592 - acc: 0.9763 - val_loss: 0.1686 - val_acc: 0.9520\n",
      "Epoch 64/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0584 - acc: 0.9766 - val_loss: 0.1607 - val_acc: 0.9521\n",
      "Epoch 65/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0581 - acc: 0.9766 - val_loss: 0.1625 - val_acc: 0.9521\n",
      "Epoch 66/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0563 - acc: 0.9774 - val_loss: 0.1665 - val_acc: 0.9518\n",
      "Epoch 67/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0561 - acc: 0.9773 - val_loss: 0.1714 - val_acc: 0.9514\n",
      "Epoch 68/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0558 - acc: 0.9777 - val_loss: 0.1695 - val_acc: 0.9505\n",
      "Epoch 69/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0555 - acc: 0.9778 - val_loss: 0.1718 - val_acc: 0.9510\n",
      "Epoch 70/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0561 - acc: 0.9776 - val_loss: 0.1713 - val_acc: 0.9510\n",
      "Epoch 71/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0533 - acc: 0.9786 - val_loss: 0.1685 - val_acc: 0.9506\n",
      "Epoch 72/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0540 - acc: 0.9783 - val_loss: 0.1815 - val_acc: 0.9515\n",
      "Epoch 73/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0520 - acc: 0.9794 - val_loss: 0.1738 - val_acc: 0.9520\n",
      "Epoch 74/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0517 - acc: 0.9794 - val_loss: 0.1725 - val_acc: 0.9509\n",
      "Epoch 75/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0518 - acc: 0.9792 - val_loss: 0.1717 - val_acc: 0.9512\n",
      "Epoch 76/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0513 - acc: 0.9796 - val_loss: 0.1749 - val_acc: 0.9522\n",
      "Epoch 77/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0504 - acc: 0.9798 - val_loss: 0.1747 - val_acc: 0.9509\n",
      "Epoch 78/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0479 - acc: 0.9809 - val_loss: 0.1778 - val_acc: 0.9520\n",
      "Epoch 79/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0494 - acc: 0.9803 - val_loss: 0.1794 - val_acc: 0.9517\n",
      "Epoch 80/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0494 - acc: 0.9803 - val_loss: 0.1756 - val_acc: 0.9508\n",
      "Epoch 81/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0485 - acc: 0.9807 - val_loss: 0.1868 - val_acc: 0.9511\n",
      "Epoch 82/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0469 - acc: 0.9813 - val_loss: 0.1833 - val_acc: 0.9517\n",
      "Epoch 83/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0467 - acc: 0.9815 - val_loss: 0.1770 - val_acc: 0.9514\n",
      "Epoch 84/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0488 - acc: 0.9806 - val_loss: 0.1775 - val_acc: 0.9514\n",
      "Epoch 85/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0464 - acc: 0.9817 - val_loss: 0.1973 - val_acc: 0.9511\n",
      "Epoch 86/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0452 - acc: 0.9821 - val_loss: 0.1804 - val_acc: 0.9515\n",
      "Epoch 87/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0460 - acc: 0.9816 - val_loss: 0.1832 - val_acc: 0.9515\n",
      "Epoch 88/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0455 - acc: 0.9820 - val_loss: 0.1768 - val_acc: 0.9507\n",
      "Epoch 89/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0441 - acc: 0.9826 - val_loss: 0.1999 - val_acc: 0.9515\n",
      "Epoch 90/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0464 - acc: 0.9817 - val_loss: 0.1936 - val_acc: 0.9495\n",
      "Epoch 91/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0435 - acc: 0.9826 - val_loss: 0.1863 - val_acc: 0.9502\n",
      "Epoch 92/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0435 - acc: 0.9828 - val_loss: 0.1996 - val_acc: 0.9502\n",
      "Epoch 93/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0435 - acc: 0.9825 - val_loss: 0.1931 - val_acc: 0.9507\n",
      "Epoch 94/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0432 - acc: 0.9829 - val_loss: 0.2020 - val_acc: 0.9507\n",
      "Epoch 95/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0437 - acc: 0.9827 - val_loss: 0.1895 - val_acc: 0.9518\n",
      "Epoch 96/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0419 - acc: 0.9835 - val_loss: 0.1969 - val_acc: 0.9501\n",
      "Epoch 97/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0418 - acc: 0.9835 - val_loss: 0.1900 - val_acc: 0.9508\n",
      "Epoch 98/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0399 - acc: 0.9842 - val_loss: 0.1989 - val_acc: 0.9507\n",
      "Epoch 99/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0404 - acc: 0.9842 - val_loss: 0.1872 - val_acc: 0.9512\n",
      "Epoch 100/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0422 - acc: 0.9833 - val_loss: 0.1939 - val_acc: 0.9516\n",
      "Epoch 101/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0396 - acc: 0.9843 - val_loss: 0.1977 - val_acc: 0.9513\n",
      "Epoch 102/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0400 - acc: 0.9843 - val_loss: 0.1976 - val_acc: 0.9515\n",
      "Epoch 103/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0398 - acc: 0.9844 - val_loss: 0.1976 - val_acc: 0.9517\n",
      "Epoch 104/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0395 - acc: 0.9845 - val_loss: 0.1942 - val_acc: 0.9502\n",
      "Epoch 105/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0387 - acc: 0.9846 - val_loss: 0.1956 - val_acc: 0.9512\n",
      "Epoch 106/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0415 - acc: 0.9837 - val_loss: 0.1968 - val_acc: 0.9515\n",
      "Epoch 107/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0392 - acc: 0.9847 - val_loss: 0.1950 - val_acc: 0.9508\n",
      "Epoch 108/500\n",
      "22668/22668 [==============================] - 97s - loss: 0.0401 - acc: 0.9842 - val_loss: 0.1970 - val_acc: 0.9518\n",
      "Epoch 109/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0381 - acc: 0.9852 - val_loss: 0.1973 - val_acc: 0.9514\n",
      "Epoch 110/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0391 - acc: 0.9847 - val_loss: 0.2031 - val_acc: 0.9521\n",
      "Epoch 111/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0378 - acc: 0.9851 - val_loss: 0.1981 - val_acc: 0.9501\n",
      "Epoch 112/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0386 - acc: 0.9850 - val_loss: 0.1956 - val_acc: 0.9512\n",
      "Epoch 113/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0368 - acc: 0.9855 - val_loss: 0.1982 - val_acc: 0.9510\n",
      "Epoch 114/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0375 - acc: 0.9854 - val_loss: 0.1980 - val_acc: 0.9510\n",
      "Epoch 115/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0377 - acc: 0.9851 - val_loss: 0.2057 - val_acc: 0.9498\n",
      "Epoch 116/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0378 - acc: 0.9852 - val_loss: 0.1966 - val_acc: 0.9520\n",
      "Epoch 117/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0359 - acc: 0.9859 - val_loss: 0.2020 - val_acc: 0.9511\n",
      "Epoch 118/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0375 - acc: 0.9853 - val_loss: 0.2000 - val_acc: 0.9509\n",
      "Epoch 119/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0354 - acc: 0.9862 - val_loss: 0.1981 - val_acc: 0.9509\n",
      "Epoch 120/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0352 - acc: 0.9862 - val_loss: 0.1975 - val_acc: 0.9512\n",
      "Epoch 121/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0372 - acc: 0.9858 - val_loss: 0.2171 - val_acc: 0.9514\n",
      "Epoch 122/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0373 - acc: 0.9854 - val_loss: 0.2043 - val_acc: 0.9504\n",
      "Epoch 123/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0357 - acc: 0.9861 - val_loss: 0.2019 - val_acc: 0.9509\n",
      "Epoch 124/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0346 - acc: 0.9867 - val_loss: 0.2232 - val_acc: 0.9500\n",
      "Epoch 125/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0365 - acc: 0.9857 - val_loss: 0.2089 - val_acc: 0.9506\n",
      "Epoch 126/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0354 - acc: 0.9862 - val_loss: 0.2066 - val_acc: 0.9505\n",
      "Epoch 127/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0341 - acc: 0.9865 - val_loss: 0.1966 - val_acc: 0.9516\n",
      "Epoch 128/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0350 - acc: 0.9863 - val_loss: 0.2028 - val_acc: 0.9516\n",
      "Epoch 129/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0343 - acc: 0.9866 - val_loss: 0.2005 - val_acc: 0.9503\n",
      "Epoch 130/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0347 - acc: 0.9865 - val_loss: 0.2112 - val_acc: 0.9501\n",
      "Epoch 131/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0342 - acc: 0.9867 - val_loss: 0.2109 - val_acc: 0.9514\n",
      "Epoch 132/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0359 - acc: 0.9860 - val_loss: 0.2102 - val_acc: 0.9502\n",
      "Epoch 133/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0354 - acc: 0.9864 - val_loss: 0.2061 - val_acc: 0.9515\n",
      "Epoch 134/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0341 - acc: 0.9868 - val_loss: 0.1990 - val_acc: 0.9505\n",
      "Epoch 135/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0336 - acc: 0.9867 - val_loss: 0.2054 - val_acc: 0.9510\n",
      "Epoch 136/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0326 - acc: 0.9874 - val_loss: 0.2147 - val_acc: 0.9511\n",
      "Epoch 137/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0336 - acc: 0.9870 - val_loss: 0.2089 - val_acc: 0.9512\n",
      "Epoch 138/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0335 - acc: 0.9869 - val_loss: 0.2079 - val_acc: 0.9505\n",
      "Epoch 139/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0333 - acc: 0.9868 - val_loss: 0.2070 - val_acc: 0.9517\n",
      "Epoch 140/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0332 - acc: 0.9872 - val_loss: 0.2042 - val_acc: 0.9509\n",
      "Epoch 141/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0323 - acc: 0.9877 - val_loss: 0.2025 - val_acc: 0.9510\n",
      "Epoch 142/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0329 - acc: 0.9871 - val_loss: 0.1995 - val_acc: 0.9499\n",
      "Epoch 143/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0331 - acc: 0.9871 - val_loss: 0.2067 - val_acc: 0.9516\n",
      "Epoch 144/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0321 - acc: 0.9876 - val_loss: 0.2149 - val_acc: 0.9511\n",
      "Epoch 145/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0336 - acc: 0.9869 - val_loss: 0.2078 - val_acc: 0.9517\n",
      "Epoch 146/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0322 - acc: 0.9875 - val_loss: 0.2091 - val_acc: 0.9521\n",
      "Epoch 147/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0308 - acc: 0.9880 - val_loss: 0.2114 - val_acc: 0.9505\n",
      "Epoch 148/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0319 - acc: 0.9878 - val_loss: 0.2158 - val_acc: 0.9511\n",
      "Epoch 149/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0326 - acc: 0.9874 - val_loss: 0.2066 - val_acc: 0.9511\n",
      "Epoch 150/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0321 - acc: 0.9877 - val_loss: 0.2058 - val_acc: 0.9505\n",
      "Epoch 151/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0331 - acc: 0.9873 - val_loss: 0.2160 - val_acc: 0.9508\n",
      "Epoch 152/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0313 - acc: 0.9878 - val_loss: 0.2114 - val_acc: 0.9512\n",
      "Epoch 153/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0306 - acc: 0.9880 - val_loss: 0.2081 - val_acc: 0.9515\n",
      "Epoch 154/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0311 - acc: 0.9880 - val_loss: 0.2124 - val_acc: 0.9505\n",
      "Epoch 155/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0298 - acc: 0.9884 - val_loss: 0.2125 - val_acc: 0.9507\n",
      "Epoch 156/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0301 - acc: 0.9883 - val_loss: 0.2193 - val_acc: 0.9511\n",
      "Epoch 157/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0310 - acc: 0.9880 - val_loss: 0.2117 - val_acc: 0.9515\n",
      "Epoch 158/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0292 - acc: 0.9887 - val_loss: 0.2148 - val_acc: 0.9510\n",
      "Epoch 159/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0321 - acc: 0.9876 - val_loss: 0.2109 - val_acc: 0.9508\n",
      "Epoch 160/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0287 - acc: 0.9890 - val_loss: 0.2149 - val_acc: 0.9513\n",
      "Epoch 161/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0324 - acc: 0.9874 - val_loss: 0.2141 - val_acc: 0.9512\n",
      "Epoch 162/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0326 - acc: 0.9876 - val_loss: 0.2158 - val_acc: 0.9515\n",
      "Epoch 163/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0299 - acc: 0.9886 - val_loss: 0.2417 - val_acc: 0.9461\n",
      "Epoch 164/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0421 - acc: 0.9843 - val_loss: 0.2139 - val_acc: 0.9509\n",
      "Epoch 165/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0287 - acc: 0.9891 - val_loss: 0.2257 - val_acc: 0.9511\n",
      "Epoch 166/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0287 - acc: 0.9890 - val_loss: 0.2241 - val_acc: 0.9515\n",
      "Epoch 167/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0287 - acc: 0.9891 - val_loss: 0.2241 - val_acc: 0.9517\n",
      "Epoch 168/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0281 - acc: 0.9892 - val_loss: 0.2172 - val_acc: 0.9516\n",
      "Epoch 169/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0290 - acc: 0.9888 - val_loss: 0.2186 - val_acc: 0.9519\n",
      "Epoch 170/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0301 - acc: 0.9884 - val_loss: 0.2125 - val_acc: 0.9504\n",
      "Epoch 171/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0289 - acc: 0.9889 - val_loss: 0.2196 - val_acc: 0.9512\n",
      "Epoch 172/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0300 - acc: 0.9883 - val_loss: 0.2159 - val_acc: 0.9509\n",
      "Epoch 173/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0306 - acc: 0.9880 - val_loss: 0.2170 - val_acc: 0.9513\n",
      "Epoch 174/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0282 - acc: 0.9893 - val_loss: 0.2166 - val_acc: 0.9505\n",
      "Epoch 175/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0282 - acc: 0.9891 - val_loss: 0.2215 - val_acc: 0.9516\n",
      "Epoch 176/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0287 - acc: 0.9890 - val_loss: 0.2164 - val_acc: 0.9509\n",
      "Epoch 177/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0286 - acc: 0.9891 - val_loss: 0.2149 - val_acc: 0.9493\n",
      "Epoch 178/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0306 - acc: 0.9881 - val_loss: 0.2193 - val_acc: 0.9515\n",
      "Epoch 179/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0299 - acc: 0.9887 - val_loss: 0.2107 - val_acc: 0.9503\n",
      "Epoch 180/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0277 - acc: 0.9895 - val_loss: 0.2234 - val_acc: 0.9519\n",
      "Epoch 181/500\n",
      "22668/22668 [==============================] - 98s - loss: 0.0279 - acc: 0.9893 - val_loss: 0.2191 - val_acc: 0.9507\n",
      "Epoch 182/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0280 - acc: 0.9895 - val_loss: 0.2242 - val_acc: 0.9516\n",
      "Epoch 183/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0273 - acc: 0.9894 - val_loss: 0.2160 - val_acc: 0.9499\n",
      "Epoch 184/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0272 - acc: 0.9894 - val_loss: 0.2172 - val_acc: 0.9505\n",
      "Epoch 185/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0282 - acc: 0.9893 - val_loss: 0.2208 - val_acc: 0.9517\n",
      "Epoch 186/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0285 - acc: 0.9890 - val_loss: 0.2196 - val_acc: 0.9503\n",
      "Epoch 187/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0281 - acc: 0.9891 - val_loss: 0.2223 - val_acc: 0.9504\n",
      "Epoch 188/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0306 - acc: 0.9881 - val_loss: 0.2191 - val_acc: 0.9488\n",
      "Epoch 189/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0272 - acc: 0.9894 - val_loss: 0.2241 - val_acc: 0.9518\n",
      "Epoch 190/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0275 - acc: 0.9895 - val_loss: 0.2152 - val_acc: 0.9505\n",
      "Epoch 191/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0271 - acc: 0.9896 - val_loss: 0.2272 - val_acc: 0.9510\n",
      "Epoch 192/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0285 - acc: 0.9890 - val_loss: 0.2133 - val_acc: 0.9518\n",
      "Epoch 193/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0266 - acc: 0.9899 - val_loss: 0.2159 - val_acc: 0.9513\n",
      "Epoch 194/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0273 - acc: 0.9897 - val_loss: 0.2232 - val_acc: 0.9509\n",
      "Epoch 195/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0273 - acc: 0.9897 - val_loss: 0.2166 - val_acc: 0.9496\n",
      "Epoch 196/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0281 - acc: 0.9893 - val_loss: 0.2220 - val_acc: 0.9507\n",
      "Epoch 197/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0278 - acc: 0.9892 - val_loss: 0.2228 - val_acc: 0.9510\n",
      "Epoch 198/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0289 - acc: 0.9891 - val_loss: 0.2207 - val_acc: 0.9510\n",
      "Epoch 199/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0265 - acc: 0.9897 - val_loss: 0.2306 - val_acc: 0.9516\n",
      "Epoch 200/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0263 - acc: 0.9899 - val_loss: 0.2215 - val_acc: 0.9516\n",
      "Epoch 201/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0266 - acc: 0.9895 - val_loss: 0.2270 - val_acc: 0.9507\n",
      "Epoch 202/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0264 - acc: 0.9900 - val_loss: 0.2224 - val_acc: 0.9517\n",
      "Epoch 203/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0267 - acc: 0.9899 - val_loss: 0.2204 - val_acc: 0.9504\n",
      "Epoch 204/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0270 - acc: 0.9898 - val_loss: 0.2283 - val_acc: 0.9509\n",
      "Epoch 205/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0262 - acc: 0.9900 - val_loss: 0.2212 - val_acc: 0.9512\n",
      "Epoch 206/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0254 - acc: 0.9903 - val_loss: 0.2316 - val_acc: 0.9506\n",
      "Epoch 207/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0268 - acc: 0.9900 - val_loss: 0.2181 - val_acc: 0.9507\n",
      "Epoch 208/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0253 - acc: 0.9903 - val_loss: 0.2289 - val_acc: 0.9516\n",
      "Epoch 209/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0263 - acc: 0.9902 - val_loss: 0.2206 - val_acc: 0.9501\n",
      "Epoch 210/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0259 - acc: 0.9902 - val_loss: 0.2252 - val_acc: 0.9511\n",
      "Epoch 211/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0255 - acc: 0.9904 - val_loss: 0.2175 - val_acc: 0.9491\n",
      "Epoch 212/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0268 - acc: 0.9895 - val_loss: 0.2252 - val_acc: 0.9505\n",
      "Epoch 213/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0278 - acc: 0.9894 - val_loss: 0.2206 - val_acc: 0.9504\n",
      "Epoch 214/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0260 - acc: 0.9901 - val_loss: 0.2329 - val_acc: 0.9515\n",
      "Epoch 215/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0258 - acc: 0.9903 - val_loss: 0.2281 - val_acc: 0.9508\n",
      "Epoch 216/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0251 - acc: 0.9906 - val_loss: 0.2292 - val_acc: 0.9507\n",
      "Epoch 217/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0244 - acc: 0.9908 - val_loss: 0.2304 - val_acc: 0.9498\n",
      "Epoch 218/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0267 - acc: 0.9900 - val_loss: 0.2281 - val_acc: 0.9505\n",
      "Epoch 219/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0249 - acc: 0.9905 - val_loss: 0.2365 - val_acc: 0.9512\n",
      "Epoch 220/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0250 - acc: 0.9906 - val_loss: 0.2334 - val_acc: 0.9507\n",
      "Epoch 221/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0257 - acc: 0.9904 - val_loss: 0.2303 - val_acc: 0.9510\n",
      "Epoch 222/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0259 - acc: 0.9902 - val_loss: 0.2324 - val_acc: 0.9516\n",
      "Epoch 223/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0255 - acc: 0.9901 - val_loss: 0.2285 - val_acc: 0.9513\n",
      "Epoch 224/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0269 - acc: 0.9896 - val_loss: 0.2272 - val_acc: 0.9498\n",
      "Epoch 225/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0251 - acc: 0.9907 - val_loss: 0.2304 - val_acc: 0.9519\n",
      "Epoch 226/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0246 - acc: 0.9907 - val_loss: 0.2264 - val_acc: 0.9512\n",
      "Epoch 227/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0255 - acc: 0.9902 - val_loss: 0.2267 - val_acc: 0.9509\n",
      "Epoch 228/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0249 - acc: 0.9908 - val_loss: 0.2287 - val_acc: 0.9499\n",
      "Epoch 229/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0253 - acc: 0.9902 - val_loss: 0.2345 - val_acc: 0.9509\n",
      "Epoch 230/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0236 - acc: 0.9910 - val_loss: 0.2319 - val_acc: 0.9501\n",
      "Epoch 231/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0258 - acc: 0.9900 - val_loss: 0.2202 - val_acc: 0.9510\n",
      "Epoch 232/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0247 - acc: 0.9906 - val_loss: 0.2266 - val_acc: 0.9497\n",
      "Epoch 233/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0252 - acc: 0.9906 - val_loss: 0.2214 - val_acc: 0.9493\n",
      "Epoch 234/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0259 - acc: 0.9900 - val_loss: 0.2299 - val_acc: 0.9512\n",
      "Epoch 235/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0239 - acc: 0.9908 - val_loss: 0.2309 - val_acc: 0.9513\n",
      "Epoch 236/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0238 - acc: 0.9908 - val_loss: 0.2273 - val_acc: 0.9502\n",
      "Epoch 237/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0248 - acc: 0.9905 - val_loss: 0.2337 - val_acc: 0.9511\n",
      "Epoch 238/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0242 - acc: 0.9907 - val_loss: 0.2274 - val_acc: 0.9510\n",
      "Epoch 239/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0242 - acc: 0.9906 - val_loss: 0.2348 - val_acc: 0.9511\n",
      "Epoch 240/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0244 - acc: 0.9907 - val_loss: 0.2467 - val_acc: 0.9504\n",
      "Epoch 241/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0243 - acc: 0.9908 - val_loss: 0.2326 - val_acc: 0.9512\n",
      "Epoch 242/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0247 - acc: 0.9905 - val_loss: 0.2295 - val_acc: 0.9507\n",
      "Epoch 243/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0244 - acc: 0.9908 - val_loss: 0.2260 - val_acc: 0.9501\n",
      "Epoch 244/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0239 - acc: 0.9907 - val_loss: 0.2353 - val_acc: 0.9506\n",
      "Epoch 245/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0255 - acc: 0.9904 - val_loss: 0.2370 - val_acc: 0.9513\n",
      "Epoch 246/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0234 - acc: 0.9910 - val_loss: 0.2359 - val_acc: 0.9512\n",
      "Epoch 247/500\n",
      "22668/22668 [==============================] - 99s - loss: 0.0239 - acc: 0.9908 - val_loss: 0.2279 - val_acc: 0.9502\n",
      "Epoch 248/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0239 - acc: 0.9909 - val_loss: 0.2400 - val_acc: 0.9511\n",
      "Epoch 249/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0263 - acc: 0.9898 - val_loss: 0.2324 - val_acc: 0.9494\n",
      "Epoch 250/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0256 - acc: 0.9903 - val_loss: 0.2312 - val_acc: 0.9495\n",
      "Epoch 251/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0243 - acc: 0.9909 - val_loss: 0.2303 - val_acc: 0.9503\n",
      "Epoch 252/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0249 - acc: 0.9905 - val_loss: 0.2283 - val_acc: 0.9504\n",
      "Epoch 253/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0244 - acc: 0.9908 - val_loss: 0.2264 - val_acc: 0.9510\n",
      "Epoch 254/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0244 - acc: 0.9908 - val_loss: 0.2358 - val_acc: 0.9506\n",
      "Epoch 255/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0223 - acc: 0.9916 - val_loss: 0.2325 - val_acc: 0.9505\n",
      "Epoch 256/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0225 - acc: 0.9914 - val_loss: 0.2387 - val_acc: 0.9506\n",
      "Epoch 257/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0225 - acc: 0.9914 - val_loss: 0.2314 - val_acc: 0.9506\n",
      "Epoch 258/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0241 - acc: 0.9908 - val_loss: 0.2279 - val_acc: 0.9507\n",
      "Epoch 259/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0244 - acc: 0.9908 - val_loss: 0.2331 - val_acc: 0.9508\n",
      "Epoch 260/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0228 - acc: 0.9913 - val_loss: 0.2350 - val_acc: 0.9501\n",
      "Epoch 261/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0228 - acc: 0.9914 - val_loss: 0.2299 - val_acc: 0.9509\n",
      "Epoch 262/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0233 - acc: 0.9913 - val_loss: 0.2302 - val_acc: 0.9513\n",
      "Epoch 263/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0231 - acc: 0.9913 - val_loss: 0.2251 - val_acc: 0.9506\n",
      "Epoch 264/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0223 - acc: 0.9913 - val_loss: 0.2311 - val_acc: 0.9507\n",
      "Epoch 265/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0236 - acc: 0.9913 - val_loss: 0.2241 - val_acc: 0.9498\n",
      "Epoch 266/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0236 - acc: 0.9910 - val_loss: 0.2362 - val_acc: 0.9507\n",
      "Epoch 267/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0252 - acc: 0.9908 - val_loss: 0.2355 - val_acc: 0.9496\n",
      "Epoch 268/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0239 - acc: 0.9910 - val_loss: 0.2300 - val_acc: 0.9506\n",
      "Epoch 269/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0234 - acc: 0.9912 - val_loss: 0.2331 - val_acc: 0.9503\n",
      "Epoch 270/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0232 - acc: 0.9913 - val_loss: 0.2269 - val_acc: 0.9491\n",
      "Epoch 271/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0236 - acc: 0.9911 - val_loss: 0.2345 - val_acc: 0.9513\n",
      "Epoch 272/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0219 - acc: 0.9917 - val_loss: 0.2432 - val_acc: 0.9500\n",
      "Epoch 273/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0232 - acc: 0.9912 - val_loss: 0.2397 - val_acc: 0.9516\n",
      "Epoch 274/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0236 - acc: 0.9909 - val_loss: 0.2381 - val_acc: 0.9508\n",
      "Epoch 275/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0228 - acc: 0.9914 - val_loss: 0.2373 - val_acc: 0.9506\n",
      "Epoch 276/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0221 - acc: 0.9916 - val_loss: 0.2421 - val_acc: 0.9510\n",
      "Epoch 277/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0228 - acc: 0.9914 - val_loss: 0.2359 - val_acc: 0.9494\n",
      "Epoch 278/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0233 - acc: 0.9911 - val_loss: 0.2439 - val_acc: 0.9517\n",
      "Epoch 279/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0217 - acc: 0.9917 - val_loss: 0.2333 - val_acc: 0.9502\n",
      "Epoch 280/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0220 - acc: 0.9918 - val_loss: 0.2387 - val_acc: 0.9496\n",
      "Epoch 281/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0219 - acc: 0.9918 - val_loss: 0.2546 - val_acc: 0.9504\n",
      "Epoch 282/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0220 - acc: 0.9916 - val_loss: 0.2213 - val_acc: 0.9488\n",
      "Epoch 283/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0238 - acc: 0.9910 - val_loss: 0.2368 - val_acc: 0.9504\n",
      "Epoch 284/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0241 - acc: 0.9911 - val_loss: 0.2361 - val_acc: 0.9512\n",
      "Epoch 285/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0213 - acc: 0.9921 - val_loss: 0.2409 - val_acc: 0.9506\n",
      "Epoch 286/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0222 - acc: 0.9917 - val_loss: 0.2348 - val_acc: 0.9506\n",
      "Epoch 287/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0220 - acc: 0.9918 - val_loss: 0.2344 - val_acc: 0.9502\n",
      "Epoch 288/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0227 - acc: 0.9915 - val_loss: 0.2353 - val_acc: 0.9507\n",
      "Epoch 289/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0226 - acc: 0.9914 - val_loss: 0.2250 - val_acc: 0.9505\n",
      "Epoch 290/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0215 - acc: 0.9918 - val_loss: 0.2385 - val_acc: 0.9505\n",
      "Epoch 291/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0212 - acc: 0.9920 - val_loss: 0.2395 - val_acc: 0.9509\n",
      "Epoch 292/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0222 - acc: 0.9916 - val_loss: 0.2276 - val_acc: 0.9495\n",
      "Epoch 293/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0211 - acc: 0.9921 - val_loss: 0.2381 - val_acc: 0.9509\n",
      "Epoch 294/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0235 - acc: 0.9911 - val_loss: 0.2263 - val_acc: 0.9507\n",
      "Epoch 295/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0222 - acc: 0.9915 - val_loss: 0.2439 - val_acc: 0.9512\n",
      "Epoch 296/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0222 - acc: 0.9917 - val_loss: 0.2462 - val_acc: 0.9507\n",
      "Epoch 297/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0223 - acc: 0.9915 - val_loss: 0.2539 - val_acc: 0.9497\n",
      "Epoch 298/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0239 - acc: 0.9912 - val_loss: 0.2285 - val_acc: 0.9508\n",
      "Epoch 299/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0213 - acc: 0.9920 - val_loss: 0.2322 - val_acc: 0.9504\n",
      "Epoch 300/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0210 - acc: 0.9921 - val_loss: 0.2388 - val_acc: 0.9506\n",
      "Epoch 301/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0204 - acc: 0.9924 - val_loss: 0.2543 - val_acc: 0.9501\n",
      "Epoch 302/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0222 - acc: 0.9917 - val_loss: 0.2273 - val_acc: 0.9501\n",
      "Epoch 303/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0207 - acc: 0.9920 - val_loss: 0.2369 - val_acc: 0.9503\n",
      "Epoch 304/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0217 - acc: 0.9920 - val_loss: 0.2425 - val_acc: 0.9507\n",
      "Epoch 305/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0208 - acc: 0.9921 - val_loss: 0.2448 - val_acc: 0.9507\n",
      "Epoch 306/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0233 - acc: 0.9913 - val_loss: 0.2378 - val_acc: 0.9505\n",
      "Epoch 307/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0216 - acc: 0.9920 - val_loss: 0.2376 - val_acc: 0.9502\n",
      "Epoch 308/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0211 - acc: 0.9919 - val_loss: 0.2410 - val_acc: 0.9499\n",
      "Epoch 309/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0223 - acc: 0.9917 - val_loss: 0.2371 - val_acc: 0.9500\n",
      "Epoch 310/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0210 - acc: 0.9921 - val_loss: 0.2389 - val_acc: 0.9508\n",
      "Epoch 311/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0234 - acc: 0.9911 - val_loss: 0.2380 - val_acc: 0.9504\n",
      "Epoch 312/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0213 - acc: 0.9918 - val_loss: 0.2343 - val_acc: 0.9507\n",
      "Epoch 313/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9921 - val_loss: 0.2464 - val_acc: 0.9510\n",
      "Epoch 314/500\n",
      "22668/22668 [==============================] - 100s - loss: 0.0212 - acc: 0.9920 - val_loss: 0.2441 - val_acc: 0.9499\n",
      "Epoch 315/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0213 - acc: 0.9921 - val_loss: 0.2341 - val_acc: 0.9514\n",
      "Epoch 316/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0226 - acc: 0.9914 - val_loss: 0.2379 - val_acc: 0.9509\n",
      "Epoch 317/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0213 - acc: 0.9918 - val_loss: 0.2398 - val_acc: 0.9509\n",
      "Epoch 318/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0218 - acc: 0.9918 - val_loss: 0.2424 - val_acc: 0.9505\n",
      "Epoch 319/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0213 - acc: 0.9919 - val_loss: 0.2304 - val_acc: 0.9513\n",
      "Epoch 320/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0211 - acc: 0.9921 - val_loss: 0.2383 - val_acc: 0.9509\n",
      "Epoch 321/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0209 - acc: 0.9923 - val_loss: 0.2366 - val_acc: 0.9509\n",
      "Epoch 322/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0210 - acc: 0.9922 - val_loss: 0.2485 - val_acc: 0.9504\n",
      "Epoch 323/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0207 - acc: 0.9923 - val_loss: 0.2354 - val_acc: 0.9502\n",
      "Epoch 324/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0219 - acc: 0.9920 - val_loss: 0.2421 - val_acc: 0.9509\n",
      "Epoch 325/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0215 - acc: 0.9920 - val_loss: 0.2534 - val_acc: 0.9512\n",
      "Epoch 326/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0214 - acc: 0.9921 - val_loss: 0.2291 - val_acc: 0.9503\n",
      "Epoch 327/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9923 - val_loss: 0.2449 - val_acc: 0.9508\n",
      "Epoch 328/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0207 - acc: 0.9922 - val_loss: 0.2389 - val_acc: 0.9502\n",
      "Epoch 329/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0217 - acc: 0.9917 - val_loss: 0.2431 - val_acc: 0.9499\n",
      "Epoch 330/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0199 - acc: 0.9927 - val_loss: 0.2477 - val_acc: 0.9514\n",
      "Epoch 331/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0221 - acc: 0.9919 - val_loss: 0.2404 - val_acc: 0.9498\n",
      "Epoch 332/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0225 - acc: 0.9917 - val_loss: 0.2342 - val_acc: 0.9508\n",
      "Epoch 333/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0198 - acc: 0.9926 - val_loss: 0.2387 - val_acc: 0.9504\n",
      "Epoch 334/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9923 - val_loss: 0.2381 - val_acc: 0.9508\n",
      "Epoch 335/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0201 - acc: 0.9924 - val_loss: 0.2378 - val_acc: 0.9509\n",
      "Epoch 336/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0202 - acc: 0.9924 - val_loss: 0.2524 - val_acc: 0.9508\n",
      "Epoch 337/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9923 - val_loss: 0.2316 - val_acc: 0.9483\n",
      "Epoch 338/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0212 - acc: 0.9921 - val_loss: 0.2397 - val_acc: 0.9487\n",
      "Epoch 339/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0208 - acc: 0.9921 - val_loss: 0.2467 - val_acc: 0.9511\n",
      "Epoch 340/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0215 - acc: 0.9918 - val_loss: 0.2401 - val_acc: 0.9503\n",
      "Epoch 341/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0202 - acc: 0.9924 - val_loss: 0.2490 - val_acc: 0.9513\n",
      "Epoch 342/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0202 - acc: 0.9924 - val_loss: 0.2400 - val_acc: 0.9509\n",
      "Epoch 343/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0208 - acc: 0.9921 - val_loss: 0.2416 - val_acc: 0.9504\n",
      "Epoch 344/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9922 - val_loss: 0.2473 - val_acc: 0.9504\n",
      "Epoch 345/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0221 - acc: 0.9918 - val_loss: 0.2403 - val_acc: 0.9495\n",
      "Epoch 346/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0200 - acc: 0.9924 - val_loss: 0.2485 - val_acc: 0.9516\n",
      "Epoch 347/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9924 - val_loss: 0.2410 - val_acc: 0.9504\n",
      "Epoch 348/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0190 - acc: 0.9928 - val_loss: 0.2416 - val_acc: 0.9498\n",
      "Epoch 349/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0219 - acc: 0.9918 - val_loss: 0.2357 - val_acc: 0.9507\n",
      "Epoch 350/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0205 - acc: 0.9924 - val_loss: 0.2362 - val_acc: 0.9505\n",
      "Epoch 351/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0208 - acc: 0.9923 - val_loss: 0.2283 - val_acc: 0.9503\n",
      "Epoch 352/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0203 - acc: 0.9923 - val_loss: 0.2372 - val_acc: 0.9503\n",
      "Epoch 353/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0197 - acc: 0.9926 - val_loss: 0.2513 - val_acc: 0.9509\n",
      "Epoch 354/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0203 - acc: 0.9924 - val_loss: 0.2420 - val_acc: 0.9495\n",
      "Epoch 355/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0221 - acc: 0.9917 - val_loss: 0.2497 - val_acc: 0.9507\n",
      "Epoch 356/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0199 - acc: 0.9925 - val_loss: 0.2425 - val_acc: 0.9509\n",
      "Epoch 357/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0204 - acc: 0.9924 - val_loss: 0.2440 - val_acc: 0.9507\n",
      "Epoch 358/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0187 - acc: 0.9929 - val_loss: 0.2517 - val_acc: 0.9510\n",
      "Epoch 359/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0197 - acc: 0.9927 - val_loss: 0.2385 - val_acc: 0.9506\n",
      "Epoch 360/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0205 - acc: 0.9923 - val_loss: 0.2425 - val_acc: 0.9508\n",
      "Epoch 361/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0205 - acc: 0.9923 - val_loss: 0.2379 - val_acc: 0.9501\n",
      "Epoch 362/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0206 - acc: 0.9923 - val_loss: 0.2464 - val_acc: 0.9499\n",
      "Epoch 363/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0243 - acc: 0.9910 - val_loss: 0.2523 - val_acc: 0.9511\n",
      "Epoch 364/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0199 - acc: 0.9927 - val_loss: 0.2409 - val_acc: 0.9499\n",
      "Epoch 365/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0198 - acc: 0.9928 - val_loss: 0.2443 - val_acc: 0.9499\n",
      "Epoch 366/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0203 - acc: 0.9923 - val_loss: 0.2462 - val_acc: 0.9510\n",
      "Epoch 367/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0198 - acc: 0.9926 - val_loss: 0.2433 - val_acc: 0.9505\n",
      "Epoch 368/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0201 - acc: 0.9925 - val_loss: 0.2481 - val_acc: 0.9502\n",
      "Epoch 369/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0204 - acc: 0.9923 - val_loss: 0.2460 - val_acc: 0.9501\n",
      "Epoch 370/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0190 - acc: 0.9927 - val_loss: 0.2440 - val_acc: 0.9511\n",
      "Epoch 371/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0200 - acc: 0.9926 - val_loss: 0.2370 - val_acc: 0.9502\n",
      "Epoch 372/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0209 - acc: 0.9923 - val_loss: 0.2389 - val_acc: 0.9513\n",
      "Epoch 373/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0207 - acc: 0.9921 - val_loss: 0.2441 - val_acc: 0.9497\n",
      "Epoch 374/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9929 - val_loss: 0.2335 - val_acc: 0.9504\n",
      "Epoch 375/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0194 - acc: 0.9928 - val_loss: 0.2466 - val_acc: 0.9509\n",
      "Epoch 376/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0204 - acc: 0.9924 - val_loss: 0.2410 - val_acc: 0.9505\n",
      "Epoch 377/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0215 - acc: 0.9920 - val_loss: 0.2499 - val_acc: 0.9499\n",
      "Epoch 378/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0190 - acc: 0.9927 - val_loss: 0.2486 - val_acc: 0.9513\n",
      "Epoch 379/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9929 - val_loss: 0.2427 - val_acc: 0.9507\n",
      "Epoch 380/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0236 - acc: 0.9912 - val_loss: 0.2405 - val_acc: 0.9511\n",
      "Epoch 381/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0209 - acc: 0.9923 - val_loss: 0.2326 - val_acc: 0.9504\n",
      "Epoch 382/500\n",
      "22668/22668 [==============================] - 101s - loss: 0.0202 - acc: 0.9925 - val_loss: 0.2451 - val_acc: 0.9501\n",
      "Epoch 383/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9927 - val_loss: 0.2374 - val_acc: 0.9511\n",
      "Epoch 384/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0198 - acc: 0.9925 - val_loss: 0.2449 - val_acc: 0.9514\n",
      "Epoch 385/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0188 - acc: 0.9929 - val_loss: 0.2354 - val_acc: 0.9508\n",
      "Epoch 386/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9927 - val_loss: 0.2478 - val_acc: 0.9503\n",
      "Epoch 387/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0195 - acc: 0.9927 - val_loss: 0.2468 - val_acc: 0.9506\n",
      "Epoch 388/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0203 - acc: 0.9924 - val_loss: 0.2400 - val_acc: 0.9510\n",
      "Epoch 389/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0190 - acc: 0.9929 - val_loss: 0.2452 - val_acc: 0.9512\n",
      "Epoch 390/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0188 - acc: 0.9930 - val_loss: 0.2482 - val_acc: 0.9502\n",
      "Epoch 391/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9927 - val_loss: 0.2484 - val_acc: 0.9511\n",
      "Epoch 392/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0179 - acc: 0.9932 - val_loss: 0.2485 - val_acc: 0.9505\n",
      "Epoch 393/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0187 - acc: 0.9930 - val_loss: 0.2361 - val_acc: 0.9503\n",
      "Epoch 394/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0202 - acc: 0.9926 - val_loss: 0.2508 - val_acc: 0.9506\n",
      "Epoch 395/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0202 - acc: 0.9925 - val_loss: 0.2499 - val_acc: 0.9498\n",
      "Epoch 396/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0199 - acc: 0.9924 - val_loss: 0.2439 - val_acc: 0.9502\n",
      "Epoch 397/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0197 - acc: 0.9927 - val_loss: 0.2461 - val_acc: 0.9505\n",
      "Epoch 398/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0190 - acc: 0.9930 - val_loss: 0.2449 - val_acc: 0.9513\n",
      "Epoch 399/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0209 - acc: 0.9920 - val_loss: 0.2434 - val_acc: 0.9507\n",
      "Epoch 400/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0180 - acc: 0.9933 - val_loss: 0.2428 - val_acc: 0.9514\n",
      "Epoch 401/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0183 - acc: 0.9931 - val_loss: 0.2452 - val_acc: 0.9506\n",
      "Epoch 402/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0183 - acc: 0.9932 - val_loss: 0.2549 - val_acc: 0.9504\n",
      "Epoch 403/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0191 - acc: 0.9927 - val_loss: 0.2444 - val_acc: 0.9508\n",
      "Epoch 404/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0186 - acc: 0.9931 - val_loss: 0.2384 - val_acc: 0.9504\n",
      "Epoch 405/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0189 - acc: 0.9931 - val_loss: 0.2551 - val_acc: 0.9509\n",
      "Epoch 406/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0192 - acc: 0.9928 - val_loss: 0.2468 - val_acc: 0.9507\n",
      "Epoch 407/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9930 - val_loss: 0.2442 - val_acc: 0.9501\n",
      "Epoch 408/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0189 - acc: 0.9929 - val_loss: 0.2500 - val_acc: 0.9512\n",
      "Epoch 409/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9931 - val_loss: 0.2598 - val_acc: 0.9510\n",
      "Epoch 410/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0191 - acc: 0.9929 - val_loss: 0.2660 - val_acc: 0.9502\n",
      "Epoch 411/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0212 - acc: 0.9921 - val_loss: 0.2359 - val_acc: 0.9506\n",
      "Epoch 412/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0187 - acc: 0.9932 - val_loss: 0.2464 - val_acc: 0.9507\n",
      "Epoch 413/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0187 - acc: 0.9931 - val_loss: 0.2450 - val_acc: 0.9508\n",
      "Epoch 414/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0219 - acc: 0.9919 - val_loss: 0.2436 - val_acc: 0.9499\n",
      "Epoch 415/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9930 - val_loss: 0.2328 - val_acc: 0.9508\n",
      "Epoch 416/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9931 - val_loss: 0.2487 - val_acc: 0.9506\n",
      "Epoch 417/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0203 - acc: 0.9924 - val_loss: 0.2541 - val_acc: 0.9505\n",
      "Epoch 418/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0188 - acc: 0.9930 - val_loss: 0.2474 - val_acc: 0.9512\n",
      "Epoch 419/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9931 - val_loss: 0.2510 - val_acc: 0.9505\n",
      "Epoch 420/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0183 - acc: 0.9933 - val_loss: 0.2513 - val_acc: 0.9511\n",
      "Epoch 421/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0187 - acc: 0.9932 - val_loss: 0.2472 - val_acc: 0.9504\n",
      "Epoch 422/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0198 - acc: 0.9928 - val_loss: 0.2333 - val_acc: 0.9496\n",
      "Epoch 423/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0196 - acc: 0.9928 - val_loss: 0.2403 - val_acc: 0.9501\n",
      "Epoch 424/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9929 - val_loss: 0.2447 - val_acc: 0.9504\n",
      "Epoch 425/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0174 - acc: 0.9936 - val_loss: 0.2509 - val_acc: 0.9506\n",
      "Epoch 426/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0181 - acc: 0.9933 - val_loss: 0.2556 - val_acc: 0.9499\n",
      "Epoch 427/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0190 - acc: 0.9929 - val_loss: 0.2386 - val_acc: 0.9507\n",
      "Epoch 428/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0210 - acc: 0.9923 - val_loss: 0.2395 - val_acc: 0.9500\n",
      "Epoch 429/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0186 - acc: 0.9933 - val_loss: 0.2459 - val_acc: 0.9503\n",
      "Epoch 430/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0183 - acc: 0.9933 - val_loss: 0.2360 - val_acc: 0.9509\n",
      "Epoch 431/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0184 - acc: 0.9931 - val_loss: 0.2498 - val_acc: 0.9506\n",
      "Epoch 432/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0174 - acc: 0.9937 - val_loss: 0.2458 - val_acc: 0.9500\n",
      "Epoch 433/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0179 - acc: 0.9932 - val_loss: 0.2587 - val_acc: 0.9516\n",
      "Epoch 434/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0183 - acc: 0.9932 - val_loss: 0.2541 - val_acc: 0.9500\n",
      "Epoch 435/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0222 - acc: 0.9918 - val_loss: 0.2409 - val_acc: 0.9503\n",
      "Epoch 436/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0176 - acc: 0.9935 - val_loss: 0.2471 - val_acc: 0.9509\n",
      "Epoch 437/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0175 - acc: 0.9936 - val_loss: 0.2385 - val_acc: 0.9503\n",
      "Epoch 438/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0179 - acc: 0.9935 - val_loss: 0.2549 - val_acc: 0.9510\n",
      "Epoch 439/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0181 - acc: 0.9934 - val_loss: 0.2393 - val_acc: 0.9509\n",
      "Epoch 440/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9933 - val_loss: 0.2468 - val_acc: 0.9498\n",
      "Epoch 441/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0179 - acc: 0.9934 - val_loss: 0.2429 - val_acc: 0.9510\n",
      "Epoch 442/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0186 - acc: 0.9930 - val_loss: 0.2447 - val_acc: 0.9509\n",
      "Epoch 443/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0201 - acc: 0.9927 - val_loss: 0.2456 - val_acc: 0.9514\n",
      "Epoch 444/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0193 - acc: 0.9928 - val_loss: 0.2429 - val_acc: 0.9509\n",
      "Epoch 445/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0179 - acc: 0.9933 - val_loss: 0.2455 - val_acc: 0.9507\n",
      "Epoch 446/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0192 - acc: 0.9931 - val_loss: 0.2317 - val_acc: 0.9494\n",
      "Epoch 447/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0204 - acc: 0.9926 - val_loss: 0.2389 - val_acc: 0.9507\n",
      "Epoch 448/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0178 - acc: 0.9934 - val_loss: 0.2373 - val_acc: 0.9506\n",
      "Epoch 449/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0188 - acc: 0.9931 - val_loss: 0.2428 - val_acc: 0.9485\n",
      "Epoch 450/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0200 - acc: 0.9925 - val_loss: 0.2516 - val_acc: 0.9499\n",
      "Epoch 451/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0186 - acc: 0.9933 - val_loss: 0.2487 - val_acc: 0.9502\n",
      "Epoch 452/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0177 - acc: 0.9934 - val_loss: 0.2506 - val_acc: 0.9509\n",
      "Epoch 453/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0175 - acc: 0.9934 - val_loss: 0.2465 - val_acc: 0.9498\n",
      "Epoch 454/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0188 - acc: 0.9930 - val_loss: 0.2470 - val_acc: 0.9504\n",
      "Epoch 455/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0178 - acc: 0.9935 - val_loss: 0.2445 - val_acc: 0.9507\n",
      "Epoch 456/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0194 - acc: 0.9929 - val_loss: 0.2616 - val_acc: 0.9509\n",
      "Epoch 457/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0182 - acc: 0.9932 - val_loss: 0.2504 - val_acc: 0.9499\n",
      "Epoch 458/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0173 - acc: 0.9934 - val_loss: 0.2598 - val_acc: 0.9506\n",
      "Epoch 459/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0175 - acc: 0.9935 - val_loss: 0.2514 - val_acc: 0.9503\n",
      "Epoch 460/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0181 - acc: 0.9933 - val_loss: 0.2492 - val_acc: 0.9502\n",
      "Epoch 461/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0178 - acc: 0.9934 - val_loss: 0.2599 - val_acc: 0.9503\n",
      "Epoch 462/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0184 - acc: 0.9931 - val_loss: 0.2609 - val_acc: 0.9506\n",
      "Epoch 463/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9933 - val_loss: 0.2546 - val_acc: 0.9501\n",
      "Epoch 464/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0230 - acc: 0.9918 - val_loss: 0.2535 - val_acc: 0.9507\n",
      "Epoch 465/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0189 - acc: 0.9930 - val_loss: 0.2575 - val_acc: 0.9510\n",
      "Epoch 466/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0176 - acc: 0.9935 - val_loss: 0.2502 - val_acc: 0.9503\n",
      "Epoch 467/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0178 - acc: 0.9934 - val_loss: 0.2445 - val_acc: 0.9504\n",
      "Epoch 468/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0183 - acc: 0.9934 - val_loss: 0.2518 - val_acc: 0.9501\n",
      "Epoch 469/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0186 - acc: 0.9932 - val_loss: 0.2507 - val_acc: 0.9505\n",
      "Epoch 470/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0171 - acc: 0.9936 - val_loss: 0.2520 - val_acc: 0.9508\n",
      "Epoch 471/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0173 - acc: 0.9935 - val_loss: 0.2550 - val_acc: 0.9512\n",
      "Epoch 472/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0171 - acc: 0.9938 - val_loss: 0.2445 - val_acc: 0.9500\n",
      "Epoch 473/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0173 - acc: 0.9936 - val_loss: 0.2586 - val_acc: 0.9501\n",
      "Epoch 474/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0177 - acc: 0.9933 - val_loss: 0.2517 - val_acc: 0.9506\n",
      "Epoch 475/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0174 - acc: 0.9936 - val_loss: 0.2481 - val_acc: 0.9500\n",
      "Epoch 476/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0182 - acc: 0.9933 - val_loss: 0.2550 - val_acc: 0.9496\n",
      "Epoch 477/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0184 - acc: 0.9930 - val_loss: 0.2489 - val_acc: 0.9496\n",
      "Epoch 478/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0190 - acc: 0.9929 - val_loss: 0.2390 - val_acc: 0.9504\n",
      "Epoch 479/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0167 - acc: 0.9938 - val_loss: 0.2640 - val_acc: 0.9501\n",
      "Epoch 480/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0195 - acc: 0.9929 - val_loss: 0.2502 - val_acc: 0.9505\n",
      "Epoch 481/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0180 - acc: 0.9934 - val_loss: 0.2551 - val_acc: 0.9505\n",
      "Epoch 482/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0178 - acc: 0.9935 - val_loss: 0.2343 - val_acc: 0.9487\n",
      "Epoch 483/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0177 - acc: 0.9935 - val_loss: 0.2531 - val_acc: 0.9499\n",
      "Epoch 484/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0189 - acc: 0.9929 - val_loss: 0.2561 - val_acc: 0.9508\n",
      "Epoch 485/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0176 - acc: 0.9935 - val_loss: 0.2437 - val_acc: 0.9499\n",
      "Epoch 486/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0185 - acc: 0.9932 - val_loss: 0.2391 - val_acc: 0.9501\n",
      "Epoch 487/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0175 - acc: 0.9936 - val_loss: 0.2549 - val_acc: 0.9505\n",
      "Epoch 488/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0180 - acc: 0.9934 - val_loss: 0.2473 - val_acc: 0.9502\n",
      "Epoch 489/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0173 - acc: 0.9936 - val_loss: 0.2540 - val_acc: 0.9496\n",
      "Epoch 490/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0195 - acc: 0.9930 - val_loss: 0.2527 - val_acc: 0.9504\n",
      "Epoch 491/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0177 - acc: 0.9935 - val_loss: 0.2555 - val_acc: 0.9512\n",
      "Epoch 492/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0174 - acc: 0.9935 - val_loss: 0.2517 - val_acc: 0.9505\n",
      "Epoch 493/500\n",
      "22668/22668 [==============================] - 102s - loss: 0.0174 - acc: 0.9935 - val_loss: 0.2483 - val_acc: 0.9501\n",
      "Epoch 494/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0177 - acc: 0.9934 - val_loss: 0.2553 - val_acc: 0.9501\n",
      "Epoch 495/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0171 - acc: 0.9938 - val_loss: 0.2483 - val_acc: 0.9504\n",
      "Epoch 496/500\n",
      "22668/22668 [==============================] - 104s - loss: 0.0188 - acc: 0.9931 - val_loss: 0.2398 - val_acc: 0.9490\n",
      "Epoch 497/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0169 - acc: 0.9938 - val_loss: 0.2554 - val_acc: 0.9504\n",
      "Epoch 498/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0179 - acc: 0.9935 - val_loss: 0.2493 - val_acc: 0.9499\n",
      "Epoch 499/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0184 - acc: 0.9932 - val_loss: 0.2532 - val_acc: 0.9507\n",
      "Epoch 500/500\n",
      "22668/22668 [==============================] - 103s - loss: 0.0166 - acc: 0.9939 - val_loss: 0.2573 - val_acc: 0.9511\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(depth, height, width)) # N.B. depth goes first in Keras!\n",
    "\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "\n",
    "flat = Flatten()(drop_2)\n",
    "\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "out = Dense(num_classes, activation='sigmoid')(drop_3)\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_split=0.2) # ...holding out 10% of the data for validation\n",
    "\n",
    "#model.evaluate(x_test, y_test, verbose=1) # Evaluate the trained model on the test set!\n",
    "cnn_prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"model_v1_500e.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "\n",
    "model.save_weights(\"model_v1_500e.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('Z:\\\\Kaggle Amazon Rainforest\\\\model2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('Z:\\\\Kaggle Amazon Rainforest\\\\model2.h5')\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для формирования тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40669/40669 [19:55<00:00, 67.01it/s]\n"
     ]
    }
   ],
   "source": [
    "X_pred = []\n",
    "\n",
    "for img_name in tqdm(test_data_names):\n",
    "    if img_name.endswith(\"jpg\"):\n",
    "        img = cv2.imread(test_dir + img_name)\n",
    "        res = cv2.resize(img, (32, 32))\n",
    "        X_pred.append(res)\n",
    "    else:\n",
    "        print(\"This file is not jpg: \" + img_name)\n",
    "    \n",
    "    \n",
    "X_pred = np.array(X_pred, np.float16) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_prediction = loaded_model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.95028913e-02,   2.44770731e-10,   1.46920516e-04,\n",
       "         1.91186206e-10,   7.95338906e-09,   2.09312118e-03,\n",
       "         2.11043917e-02,   1.89864000e-08,   2.01471034e-03,\n",
       "         1.25273596e-04,   9.95246828e-01,   2.79361684e-05,\n",
       "         9.94588614e-01,   8.99066925e-02,   6.53332544e-09,\n",
       "         7.05595538e-09,   1.04903862e-01], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_prediction[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 40670/40670 [00:05<00:00, 6995.99it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file_dir = local_dir + 'submission.csv'\n",
    "submission_csv = open(csv_file_dir, 'w')\n",
    "submission_csv.write('image_name, tags\\n')\n",
    "\n",
    "for i in tqdm(range(len(test_data_names))):\n",
    "    if test_data_names[i].endswith(\"jpg\"):\n",
    "        line = test_data_names[i] + ','\n",
    "        \n",
    "        for j in range(len(img_prediction[i])):\n",
    "            if img_prediction[i][j] >= 0.95:\n",
    "                line += cathegories[j] + ' '\n",
    "\n",
    "        submission_csv.write(line + '\\n')\n",
    "submission_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_0.jpg',\n",
       " 'test_1.jpg',\n",
       " 'test_2.jpg',\n",
       " 'test_3.jpg',\n",
       " 'test_4.jpg',\n",
       " 'test_5.jpg',\n",
       " 'test_6.jpg',\n",
       " 'test_7.jpg',\n",
       " 'test_8.jpg',\n",
       " 'test_9.jpg',\n",
       " 'test_10.jpg',\n",
       " 'test_11.jpg',\n",
       " 'test_12.jpg',\n",
       " 'test_13.jpg',\n",
       " 'test_14.jpg',\n",
       " 'test_15.jpg',\n",
       " 'test_16.jpg',\n",
       " 'test_17.jpg',\n",
       " 'test_18.jpg',\n",
       " 'test_19.jpg',\n",
       " 'test_20.jpg',\n",
       " 'test_21.jpg',\n",
       " 'test_22.jpg',\n",
       " 'test_23.jpg',\n",
       " 'test_24.jpg',\n",
       " 'test_25.jpg',\n",
       " 'test_26.jpg',\n",
       " 'test_27.jpg',\n",
       " 'test_28.jpg',\n",
       " 'test_29.jpg',\n",
       " 'test_30.jpg',\n",
       " 'test_31.jpg',\n",
       " 'test_32.jpg',\n",
       " 'test_33.jpg',\n",
       " 'test_34.jpg',\n",
       " 'test_35.jpg',\n",
       " 'test_36.jpg',\n",
       " 'test_37.jpg',\n",
       " 'test_38.jpg',\n",
       " 'test_39.jpg',\n",
       " 'test_40.jpg',\n",
       " 'test_41.jpg',\n",
       " 'test_42.jpg',\n",
       " 'test_43.jpg',\n",
       " 'test_44.jpg',\n",
       " 'test_45.jpg',\n",
       " 'test_46.jpg',\n",
       " 'test_47.jpg',\n",
       " 'test_48.jpg',\n",
       " 'test_49.jpg',\n",
       " 'test_50.jpg',\n",
       " 'test_51.jpg',\n",
       " 'test_52.jpg',\n",
       " 'test_53.jpg',\n",
       " 'test_54.jpg',\n",
       " 'test_55.jpg',\n",
       " 'test_56.jpg',\n",
       " 'test_57.jpg',\n",
       " 'test_58.jpg',\n",
       " 'test_59.jpg',\n",
       " 'test_60.jpg',\n",
       " 'test_61.jpg',\n",
       " 'test_62.jpg',\n",
       " 'test_63.jpg',\n",
       " 'test_64.jpg',\n",
       " 'test_65.jpg',\n",
       " 'test_66.jpg',\n",
       " 'test_67.jpg',\n",
       " 'test_68.jpg',\n",
       " 'test_69.jpg',\n",
       " 'test_70.jpg',\n",
       " 'test_71.jpg',\n",
       " 'test_72.jpg',\n",
       " 'test_73.jpg',\n",
       " 'test_74.jpg',\n",
       " 'test_75.jpg',\n",
       " 'test_76.jpg',\n",
       " 'test_77.jpg',\n",
       " 'test_78.jpg',\n",
       " 'test_79.jpg',\n",
       " 'test_80.jpg',\n",
       " 'test_81.jpg',\n",
       " 'test_82.jpg',\n",
       " 'test_83.jpg',\n",
       " 'test_84.jpg',\n",
       " 'test_85.jpg',\n",
       " 'test_86.jpg',\n",
       " 'test_87.jpg',\n",
       " 'test_88.jpg',\n",
       " 'test_89.jpg',\n",
       " 'test_90.jpg',\n",
       " 'test_91.jpg',\n",
       " 'test_92.jpg',\n",
       " 'test_93.jpg',\n",
       " 'test_94.jpg',\n",
       " 'test_95.jpg',\n",
       " 'test_96.jpg',\n",
       " 'test_97.jpg',\n",
       " 'test_98.jpg',\n",
       " 'test_99.jpg',\n",
       " 'test_100.jpg',\n",
       " 'test_101.jpg',\n",
       " 'test_102.jpg',\n",
       " 'test_103.jpg',\n",
       " 'test_104.jpg',\n",
       " 'test_105.jpg',\n",
       " 'test_106.jpg',\n",
       " 'test_107.jpg',\n",
       " 'test_108.jpg',\n",
       " 'test_109.jpg',\n",
       " 'test_110.jpg',\n",
       " 'test_111.jpg',\n",
       " 'test_112.jpg',\n",
       " 'test_113.jpg',\n",
       " 'test_114.jpg',\n",
       " 'test_115.jpg',\n",
       " 'test_116.jpg',\n",
       " 'test_117.jpg',\n",
       " 'test_118.jpg',\n",
       " 'test_119.jpg',\n",
       " 'test_120.jpg',\n",
       " 'test_121.jpg',\n",
       " 'test_122.jpg',\n",
       " 'test_123.jpg',\n",
       " 'test_124.jpg',\n",
       " 'test_125.jpg',\n",
       " 'test_126.jpg',\n",
       " 'test_127.jpg',\n",
       " 'test_128.jpg',\n",
       " 'test_129.jpg',\n",
       " 'test_130.jpg',\n",
       " 'test_131.jpg',\n",
       " 'test_132.jpg',\n",
       " 'test_133.jpg',\n",
       " 'test_134.jpg',\n",
       " 'test_135.jpg',\n",
       " 'test_136.jpg',\n",
       " 'test_137.jpg',\n",
       " 'test_138.jpg',\n",
       " 'test_139.jpg',\n",
       " 'test_140.jpg',\n",
       " 'test_141.jpg',\n",
       " 'test_142.jpg',\n",
       " 'test_143.jpg',\n",
       " 'test_144.jpg',\n",
       " 'test_145.jpg',\n",
       " 'test_146.jpg',\n",
       " 'test_147.jpg',\n",
       " 'test_148.jpg',\n",
       " 'test_149.jpg',\n",
       " 'test_150.jpg',\n",
       " 'test_151.jpg',\n",
       " 'test_152.jpg',\n",
       " 'test_153.jpg',\n",
       " 'test_154.jpg',\n",
       " 'test_155.jpg',\n",
       " 'test_156.jpg',\n",
       " 'test_157.jpg',\n",
       " 'test_158.jpg',\n",
       " 'test_159.jpg',\n",
       " 'test_160.jpg',\n",
       " 'test_161.jpg',\n",
       " 'test_162.jpg',\n",
       " 'test_163.jpg',\n",
       " 'test_164.jpg',\n",
       " 'test_165.jpg',\n",
       " 'test_166.jpg',\n",
       " 'test_167.jpg',\n",
       " 'test_168.jpg',\n",
       " 'test_169.jpg',\n",
       " 'test_170.jpg',\n",
       " 'test_171.jpg',\n",
       " 'test_172.jpg',\n",
       " 'test_173.jpg',\n",
       " 'test_174.jpg',\n",
       " 'test_175.jpg',\n",
       " 'test_176.jpg',\n",
       " 'test_177.jpg',\n",
       " 'test_178.jpg',\n",
       " 'test_179.jpg',\n",
       " 'test_180.jpg',\n",
       " 'test_181.jpg',\n",
       " 'test_182.jpg',\n",
       " 'test_183.jpg',\n",
       " 'test_184.jpg',\n",
       " 'test_185.jpg',\n",
       " 'test_186.jpg',\n",
       " 'test_187.jpg',\n",
       " 'test_188.jpg',\n",
       " 'test_189.jpg',\n",
       " 'test_190.jpg',\n",
       " 'test_191.jpg',\n",
       " 'test_192.jpg',\n",
       " 'test_193.jpg',\n",
       " 'test_194.jpg',\n",
       " 'test_195.jpg',\n",
       " 'test_196.jpg',\n",
       " 'test_197.jpg',\n",
       " 'test_198.jpg',\n",
       " 'test_199.jpg',\n",
       " 'test_200.jpg',\n",
       " 'test_201.jpg',\n",
       " 'test_202.jpg',\n",
       " 'test_203.jpg',\n",
       " 'test_204.jpg',\n",
       " 'test_205.jpg',\n",
       " 'test_206.jpg',\n",
       " 'test_207.jpg',\n",
       " 'test_208.jpg',\n",
       " 'test_209.jpg',\n",
       " 'test_210.jpg',\n",
       " 'test_211.jpg',\n",
       " 'test_212.jpg',\n",
       " 'test_213.jpg',\n",
       " 'test_214.jpg',\n",
       " 'test_215.jpg',\n",
       " 'test_216.jpg',\n",
       " 'test_217.jpg',\n",
       " 'test_218.jpg',\n",
       " 'test_219.jpg',\n",
       " 'test_220.jpg',\n",
       " 'test_221.jpg',\n",
       " 'test_222.jpg',\n",
       " 'test_223.jpg',\n",
       " 'test_224.jpg',\n",
       " 'test_225.jpg',\n",
       " 'test_226.jpg',\n",
       " 'test_227.jpg',\n",
       " 'test_228.jpg',\n",
       " 'test_229.jpg',\n",
       " 'test_230.jpg',\n",
       " 'test_231.jpg',\n",
       " 'test_232.jpg',\n",
       " 'test_233.jpg',\n",
       " 'test_234.jpg',\n",
       " 'test_235.jpg',\n",
       " 'test_236.jpg',\n",
       " 'test_237.jpg',\n",
       " 'test_238.jpg',\n",
       " 'test_239.jpg',\n",
       " 'test_240.jpg',\n",
       " 'test_241.jpg',\n",
       " 'test_242.jpg',\n",
       " 'test_243.jpg',\n",
       " 'test_244.jpg',\n",
       " 'test_245.jpg',\n",
       " 'test_246.jpg',\n",
       " 'test_247.jpg',\n",
       " 'test_248.jpg',\n",
       " 'test_249.jpg',\n",
       " 'test_250.jpg',\n",
       " 'test_251.jpg',\n",
       " 'test_252.jpg',\n",
       " 'test_253.jpg',\n",
       " 'test_254.jpg',\n",
       " 'test_255.jpg',\n",
       " 'test_256.jpg',\n",
       " 'test_257.jpg',\n",
       " 'test_258.jpg',\n",
       " 'test_259.jpg',\n",
       " 'test_260.jpg',\n",
       " 'test_261.jpg',\n",
       " 'test_262.jpg',\n",
       " 'test_263.jpg',\n",
       " 'test_264.jpg',\n",
       " 'test_265.jpg',\n",
       " 'test_266.jpg',\n",
       " 'test_267.jpg',\n",
       " 'test_268.jpg',\n",
       " 'test_269.jpg',\n",
       " 'test_270.jpg',\n",
       " 'test_271.jpg',\n",
       " 'test_272.jpg',\n",
       " 'test_273.jpg',\n",
       " 'test_274.jpg',\n",
       " 'test_275.jpg',\n",
       " 'test_276.jpg',\n",
       " 'test_277.jpg',\n",
       " 'test_278.jpg',\n",
       " 'test_279.jpg',\n",
       " 'test_280.jpg',\n",
       " 'test_281.jpg',\n",
       " 'test_282.jpg',\n",
       " 'test_283.jpg',\n",
       " 'test_284.jpg',\n",
       " 'test_285.jpg',\n",
       " 'test_286.jpg',\n",
       " 'test_287.jpg',\n",
       " 'test_288.jpg',\n",
       " 'test_289.jpg',\n",
       " 'test_290.jpg',\n",
       " 'test_291.jpg',\n",
       " 'test_292.jpg',\n",
       " 'test_293.jpg',\n",
       " 'test_294.jpg',\n",
       " 'test_295.jpg',\n",
       " 'test_296.jpg',\n",
       " 'test_297.jpg',\n",
       " 'test_298.jpg',\n",
       " 'test_299.jpg',\n",
       " 'test_300.jpg',\n",
       " 'test_301.jpg',\n",
       " 'test_302.jpg',\n",
       " 'test_303.jpg',\n",
       " 'test_304.jpg',\n",
       " 'test_305.jpg',\n",
       " 'test_306.jpg',\n",
       " 'test_307.jpg',\n",
       " 'test_308.jpg',\n",
       " 'test_309.jpg',\n",
       " 'test_310.jpg',\n",
       " 'test_311.jpg',\n",
       " 'test_312.jpg',\n",
       " 'test_313.jpg',\n",
       " 'test_314.jpg',\n",
       " 'test_315.jpg',\n",
       " 'test_316.jpg',\n",
       " 'test_317.jpg',\n",
       " 'test_318.jpg',\n",
       " 'test_319.jpg',\n",
       " 'test_320.jpg',\n",
       " 'test_321.jpg',\n",
       " 'test_322.jpg',\n",
       " 'test_323.jpg',\n",
       " 'test_324.jpg',\n",
       " 'test_325.jpg',\n",
       " 'test_326.jpg',\n",
       " 'test_327.jpg',\n",
       " 'test_328.jpg',\n",
       " 'test_329.jpg',\n",
       " 'test_330.jpg',\n",
       " 'test_331.jpg',\n",
       " 'test_332.jpg',\n",
       " 'test_333.jpg',\n",
       " 'test_334.jpg',\n",
       " 'test_335.jpg',\n",
       " 'test_336.jpg',\n",
       " 'test_337.jpg',\n",
       " 'test_338.jpg',\n",
       " 'test_339.jpg',\n",
       " 'test_340.jpg',\n",
       " 'test_341.jpg',\n",
       " 'test_342.jpg',\n",
       " 'test_343.jpg',\n",
       " 'test_344.jpg',\n",
       " 'test_345.jpg',\n",
       " 'test_346.jpg',\n",
       " 'test_347.jpg',\n",
       " 'test_348.jpg',\n",
       " 'test_349.jpg',\n",
       " 'test_350.jpg',\n",
       " 'test_351.jpg',\n",
       " 'test_352.jpg',\n",
       " 'test_353.jpg',\n",
       " 'test_354.jpg',\n",
       " 'test_355.jpg',\n",
       " 'test_356.jpg',\n",
       " 'test_357.jpg',\n",
       " 'test_358.jpg',\n",
       " 'test_359.jpg',\n",
       " 'test_360.jpg',\n",
       " 'test_361.jpg',\n",
       " 'test_362.jpg',\n",
       " 'test_363.jpg',\n",
       " 'test_364.jpg',\n",
       " 'test_365.jpg',\n",
       " 'test_366.jpg',\n",
       " 'test_367.jpg',\n",
       " 'test_368.jpg',\n",
       " 'test_369.jpg',\n",
       " 'test_370.jpg',\n",
       " 'test_371.jpg',\n",
       " 'test_372.jpg',\n",
       " 'test_373.jpg',\n",
       " 'test_374.jpg',\n",
       " 'test_375.jpg',\n",
       " 'test_376.jpg',\n",
       " 'test_377.jpg',\n",
       " 'test_378.jpg',\n",
       " 'test_379.jpg',\n",
       " 'test_380.jpg',\n",
       " 'test_381.jpg',\n",
       " 'test_382.jpg',\n",
       " 'test_383.jpg',\n",
       " 'test_384.jpg',\n",
       " 'test_385.jpg',\n",
       " 'test_386.jpg',\n",
       " 'test_387.jpg',\n",
       " 'test_388.jpg',\n",
       " 'test_389.jpg',\n",
       " 'test_390.jpg',\n",
       " 'test_391.jpg',\n",
       " 'test_392.jpg',\n",
       " 'test_393.jpg',\n",
       " 'test_394.jpg',\n",
       " 'test_395.jpg',\n",
       " 'test_396.jpg',\n",
       " 'test_397.jpg',\n",
       " 'test_398.jpg',\n",
       " 'test_399.jpg',\n",
       " 'test_400.jpg',\n",
       " 'test_401.jpg',\n",
       " 'test_402.jpg',\n",
       " 'test_403.jpg',\n",
       " 'test_404.jpg',\n",
       " 'test_405.jpg',\n",
       " 'test_406.jpg',\n",
       " 'test_407.jpg',\n",
       " 'test_408.jpg',\n",
       " 'test_409.jpg',\n",
       " 'test_410.jpg',\n",
       " 'test_411.jpg',\n",
       " 'test_412.jpg',\n",
       " 'test_413.jpg',\n",
       " 'test_414.jpg',\n",
       " 'test_415.jpg',\n",
       " 'test_416.jpg',\n",
       " 'test_417.jpg',\n",
       " 'test_418.jpg',\n",
       " 'test_419.jpg',\n",
       " 'test_420.jpg',\n",
       " 'test_421.jpg',\n",
       " 'test_422.jpg',\n",
       " 'test_423.jpg',\n",
       " 'test_424.jpg',\n",
       " 'test_425.jpg',\n",
       " 'test_426.jpg',\n",
       " 'test_427.jpg',\n",
       " 'test_428.jpg',\n",
       " 'test_429.jpg',\n",
       " 'test_430.jpg',\n",
       " 'test_431.jpg',\n",
       " 'test_432.jpg',\n",
       " 'test_433.jpg',\n",
       " 'test_434.jpg',\n",
       " 'test_435.jpg',\n",
       " 'test_436.jpg',\n",
       " 'test_437.jpg',\n",
       " 'test_438.jpg',\n",
       " 'test_439.jpg',\n",
       " 'test_440.jpg',\n",
       " 'test_441.jpg',\n",
       " 'test_442.jpg',\n",
       " 'test_443.jpg',\n",
       " 'test_444.jpg',\n",
       " 'test_445.jpg',\n",
       " 'test_446.jpg',\n",
       " 'test_447.jpg',\n",
       " 'test_448.jpg',\n",
       " 'test_449.jpg',\n",
       " 'test_450.jpg',\n",
       " 'test_451.jpg',\n",
       " 'test_452.jpg',\n",
       " 'test_453.jpg',\n",
       " 'test_454.jpg',\n",
       " 'test_455.jpg',\n",
       " 'test_456.jpg',\n",
       " 'test_457.jpg',\n",
       " 'test_458.jpg',\n",
       " 'test_459.jpg',\n",
       " 'test_460.jpg',\n",
       " 'test_461.jpg',\n",
       " 'test_462.jpg',\n",
       " 'test_463.jpg',\n",
       " 'test_464.jpg',\n",
       " 'test_465.jpg',\n",
       " 'test_466.jpg',\n",
       " 'test_467.jpg',\n",
       " 'test_468.jpg',\n",
       " 'test_469.jpg',\n",
       " 'test_470.jpg',\n",
       " 'test_471.jpg',\n",
       " 'test_472.jpg',\n",
       " 'test_473.jpg',\n",
       " 'test_474.jpg',\n",
       " 'test_475.jpg',\n",
       " 'test_476.jpg',\n",
       " 'test_477.jpg',\n",
       " 'test_478.jpg',\n",
       " 'test_479.jpg',\n",
       " 'test_480.jpg',\n",
       " 'test_481.jpg',\n",
       " 'test_482.jpg',\n",
       " 'test_483.jpg',\n",
       " 'test_484.jpg',\n",
       " 'test_485.jpg',\n",
       " 'test_486.jpg',\n",
       " 'test_487.jpg',\n",
       " 'test_488.jpg',\n",
       " 'test_489.jpg',\n",
       " 'test_490.jpg',\n",
       " 'test_491.jpg',\n",
       " 'test_492.jpg',\n",
       " 'test_493.jpg',\n",
       " 'test_494.jpg',\n",
       " 'test_495.jpg',\n",
       " 'test_496.jpg',\n",
       " 'test_497.jpg',\n",
       " 'test_498.jpg',\n",
       " 'test_499.jpg',\n",
       " 'test_500.jpg',\n",
       " 'test_501.jpg',\n",
       " 'test_502.jpg',\n",
       " 'test_503.jpg',\n",
       " 'test_504.jpg',\n",
       " 'test_505.jpg',\n",
       " 'test_506.jpg',\n",
       " 'test_507.jpg',\n",
       " 'test_508.jpg',\n",
       " 'test_509.jpg',\n",
       " 'test_510.jpg',\n",
       " 'test_511.jpg',\n",
       " 'test_512.jpg',\n",
       " 'test_513.jpg',\n",
       " 'test_514.jpg',\n",
       " 'test_515.jpg',\n",
       " 'test_516.jpg',\n",
       " 'test_517.jpg',\n",
       " 'test_518.jpg',\n",
       " 'test_519.jpg',\n",
       " 'test_520.jpg',\n",
       " 'test_521.jpg',\n",
       " 'test_522.jpg',\n",
       " 'test_523.jpg',\n",
       " 'test_524.jpg',\n",
       " 'test_525.jpg',\n",
       " 'test_526.jpg',\n",
       " 'test_527.jpg',\n",
       " 'test_528.jpg',\n",
       " 'test_529.jpg',\n",
       " 'test_530.jpg',\n",
       " 'test_531.jpg',\n",
       " 'test_532.jpg',\n",
       " 'test_533.jpg',\n",
       " 'test_534.jpg',\n",
       " 'test_535.jpg',\n",
       " 'test_536.jpg',\n",
       " 'test_537.jpg',\n",
       " 'test_538.jpg',\n",
       " 'test_539.jpg',\n",
       " 'test_540.jpg',\n",
       " 'test_541.jpg',\n",
       " 'test_542.jpg',\n",
       " 'test_543.jpg',\n",
       " 'test_544.jpg',\n",
       " 'test_545.jpg',\n",
       " 'test_546.jpg',\n",
       " 'test_547.jpg',\n",
       " 'test_548.jpg',\n",
       " 'test_549.jpg',\n",
       " 'test_550.jpg',\n",
       " 'test_551.jpg',\n",
       " 'test_552.jpg',\n",
       " 'test_553.jpg',\n",
       " 'test_554.jpg',\n",
       " 'test_555.jpg',\n",
       " 'test_556.jpg',\n",
       " 'test_557.jpg',\n",
       " 'test_558.jpg',\n",
       " 'test_559.jpg',\n",
       " 'test_560.jpg',\n",
       " 'test_561.jpg',\n",
       " 'test_562.jpg',\n",
       " 'test_563.jpg',\n",
       " 'test_564.jpg',\n",
       " 'test_565.jpg',\n",
       " 'test_566.jpg',\n",
       " 'test_567.jpg',\n",
       " 'test_568.jpg',\n",
       " 'test_569.jpg',\n",
       " 'test_570.jpg',\n",
       " 'test_571.jpg',\n",
       " 'test_572.jpg',\n",
       " 'test_573.jpg',\n",
       " 'test_574.jpg',\n",
       " 'test_575.jpg',\n",
       " 'test_576.jpg',\n",
       " 'test_577.jpg',\n",
       " 'test_578.jpg',\n",
       " 'test_579.jpg',\n",
       " 'test_580.jpg',\n",
       " 'test_581.jpg',\n",
       " 'test_582.jpg',\n",
       " 'test_583.jpg',\n",
       " 'test_584.jpg',\n",
       " 'test_585.jpg',\n",
       " 'test_586.jpg',\n",
       " 'test_587.jpg',\n",
       " 'test_588.jpg',\n",
       " 'test_589.jpg',\n",
       " 'test_590.jpg',\n",
       " 'test_591.jpg',\n",
       " 'test_592.jpg',\n",
       " 'test_593.jpg',\n",
       " 'test_594.jpg',\n",
       " 'test_595.jpg',\n",
       " 'test_596.jpg',\n",
       " 'test_597.jpg',\n",
       " 'test_598.jpg',\n",
       " 'test_599.jpg',\n",
       " 'test_600.jpg',\n",
       " 'test_601.jpg',\n",
       " 'test_602.jpg',\n",
       " 'test_603.jpg',\n",
       " 'test_604.jpg',\n",
       " 'test_605.jpg',\n",
       " 'test_606.jpg',\n",
       " 'test_607.jpg',\n",
       " 'test_608.jpg',\n",
       " 'test_609.jpg',\n",
       " 'test_610.jpg',\n",
       " 'test_611.jpg',\n",
       " 'test_612.jpg',\n",
       " 'test_613.jpg',\n",
       " 'test_614.jpg',\n",
       " 'test_615.jpg',\n",
       " 'test_616.jpg',\n",
       " 'test_617.jpg',\n",
       " 'test_618.jpg',\n",
       " 'test_619.jpg',\n",
       " 'test_620.jpg',\n",
       " 'test_621.jpg',\n",
       " 'test_622.jpg',\n",
       " 'test_623.jpg',\n",
       " 'test_624.jpg',\n",
       " 'test_625.jpg',\n",
       " 'test_626.jpg',\n",
       " 'test_627.jpg',\n",
       " 'test_628.jpg',\n",
       " 'test_629.jpg',\n",
       " 'test_630.jpg',\n",
       " 'test_631.jpg',\n",
       " 'test_632.jpg',\n",
       " 'test_633.jpg',\n",
       " 'test_634.jpg',\n",
       " 'test_635.jpg',\n",
       " 'test_636.jpg',\n",
       " 'test_637.jpg',\n",
       " 'test_638.jpg',\n",
       " 'test_639.jpg',\n",
       " 'test_640.jpg',\n",
       " 'test_641.jpg',\n",
       " 'test_642.jpg',\n",
       " 'test_643.jpg',\n",
       " 'test_644.jpg',\n",
       " 'test_645.jpg',\n",
       " 'test_646.jpg',\n",
       " 'test_647.jpg',\n",
       " 'test_648.jpg',\n",
       " 'test_649.jpg',\n",
       " 'test_650.jpg',\n",
       " 'test_651.jpg',\n",
       " 'test_652.jpg',\n",
       " 'test_653.jpg',\n",
       " 'test_654.jpg',\n",
       " 'test_655.jpg',\n",
       " 'test_656.jpg',\n",
       " 'test_657.jpg',\n",
       " 'test_658.jpg',\n",
       " 'test_659.jpg',\n",
       " 'test_660.jpg',\n",
       " 'test_661.jpg',\n",
       " 'test_662.jpg',\n",
       " 'test_663.jpg',\n",
       " 'test_664.jpg',\n",
       " 'test_665.jpg',\n",
       " 'test_666.jpg',\n",
       " 'test_667.jpg',\n",
       " 'test_668.jpg',\n",
       " 'test_669.jpg',\n",
       " 'test_670.jpg',\n",
       " 'test_671.jpg',\n",
       " 'test_672.jpg',\n",
       " 'test_673.jpg',\n",
       " 'test_674.jpg',\n",
       " 'test_675.jpg',\n",
       " 'test_676.jpg',\n",
       " 'test_677.jpg',\n",
       " 'test_678.jpg',\n",
       " 'test_679.jpg',\n",
       " 'test_680.jpg',\n",
       " 'test_681.jpg',\n",
       " 'test_682.jpg',\n",
       " 'test_683.jpg',\n",
       " 'test_684.jpg',\n",
       " 'test_685.jpg',\n",
       " 'test_686.jpg',\n",
       " 'test_687.jpg',\n",
       " 'test_688.jpg',\n",
       " 'test_689.jpg',\n",
       " 'test_690.jpg',\n",
       " 'test_691.jpg',\n",
       " 'test_692.jpg',\n",
       " 'test_693.jpg',\n",
       " 'test_694.jpg',\n",
       " 'test_695.jpg',\n",
       " 'test_696.jpg',\n",
       " 'test_697.jpg',\n",
       " 'test_698.jpg',\n",
       " 'test_699.jpg',\n",
       " 'test_700.jpg',\n",
       " 'test_701.jpg',\n",
       " 'test_702.jpg',\n",
       " 'test_703.jpg',\n",
       " 'test_704.jpg',\n",
       " 'test_705.jpg',\n",
       " 'test_706.jpg',\n",
       " 'test_707.jpg',\n",
       " 'test_708.jpg',\n",
       " 'test_709.jpg',\n",
       " 'test_710.jpg',\n",
       " 'test_711.jpg',\n",
       " 'test_712.jpg',\n",
       " 'test_713.jpg',\n",
       " 'test_714.jpg',\n",
       " 'test_715.jpg',\n",
       " 'test_716.jpg',\n",
       " 'test_717.jpg',\n",
       " 'test_718.jpg',\n",
       " 'test_719.jpg',\n",
       " 'test_720.jpg',\n",
       " 'test_721.jpg',\n",
       " 'test_722.jpg',\n",
       " 'test_723.jpg',\n",
       " 'test_724.jpg',\n",
       " 'test_725.jpg',\n",
       " 'test_726.jpg',\n",
       " 'test_727.jpg',\n",
       " 'test_728.jpg',\n",
       " 'test_729.jpg',\n",
       " 'test_730.jpg',\n",
       " 'test_731.jpg',\n",
       " 'test_732.jpg',\n",
       " 'test_733.jpg',\n",
       " 'test_734.jpg',\n",
       " 'test_735.jpg',\n",
       " 'test_736.jpg',\n",
       " 'test_737.jpg',\n",
       " 'test_738.jpg',\n",
       " 'test_739.jpg',\n",
       " 'test_740.jpg',\n",
       " 'test_741.jpg',\n",
       " 'test_742.jpg',\n",
       " 'test_743.jpg',\n",
       " 'test_744.jpg',\n",
       " 'test_745.jpg',\n",
       " 'test_746.jpg',\n",
       " 'test_747.jpg',\n",
       " 'test_748.jpg',\n",
       " 'test_749.jpg',\n",
       " 'test_750.jpg',\n",
       " 'test_751.jpg',\n",
       " 'test_752.jpg',\n",
       " 'test_753.jpg',\n",
       " 'test_754.jpg',\n",
       " 'test_755.jpg',\n",
       " 'test_756.jpg',\n",
       " 'test_757.jpg',\n",
       " 'test_758.jpg',\n",
       " 'test_759.jpg',\n",
       " 'test_760.jpg',\n",
       " 'test_761.jpg',\n",
       " 'test_762.jpg',\n",
       " 'test_763.jpg',\n",
       " 'test_764.jpg',\n",
       " 'test_765.jpg',\n",
       " 'test_766.jpg',\n",
       " 'test_767.jpg',\n",
       " 'test_768.jpg',\n",
       " 'test_769.jpg',\n",
       " 'test_770.jpg',\n",
       " 'test_771.jpg',\n",
       " 'test_772.jpg',\n",
       " 'test_773.jpg',\n",
       " 'test_774.jpg',\n",
       " 'test_775.jpg',\n",
       " 'test_776.jpg',\n",
       " 'test_777.jpg',\n",
       " 'test_778.jpg',\n",
       " 'test_779.jpg',\n",
       " 'test_780.jpg',\n",
       " 'test_781.jpg',\n",
       " 'test_782.jpg',\n",
       " 'test_783.jpg',\n",
       " 'test_784.jpg',\n",
       " 'test_785.jpg',\n",
       " 'test_786.jpg',\n",
       " 'test_787.jpg',\n",
       " 'test_788.jpg',\n",
       " 'test_789.jpg',\n",
       " 'test_790.jpg',\n",
       " 'test_791.jpg',\n",
       " 'test_792.jpg',\n",
       " 'test_793.jpg',\n",
       " 'test_794.jpg',\n",
       " 'test_795.jpg',\n",
       " 'test_796.jpg',\n",
       " 'test_797.jpg',\n",
       " 'test_798.jpg',\n",
       " 'test_799.jpg',\n",
       " 'test_800.jpg',\n",
       " 'test_801.jpg',\n",
       " 'test_802.jpg',\n",
       " 'test_803.jpg',\n",
       " 'test_804.jpg',\n",
       " 'test_805.jpg',\n",
       " 'test_806.jpg',\n",
       " 'test_807.jpg',\n",
       " 'test_808.jpg',\n",
       " 'test_809.jpg',\n",
       " 'test_810.jpg',\n",
       " 'test_811.jpg',\n",
       " 'test_812.jpg',\n",
       " 'test_813.jpg',\n",
       " 'test_814.jpg',\n",
       " 'test_815.jpg',\n",
       " 'test_816.jpg',\n",
       " 'test_817.jpg',\n",
       " 'test_818.jpg',\n",
       " 'test_819.jpg',\n",
       " 'test_820.jpg',\n",
       " 'test_821.jpg',\n",
       " 'test_822.jpg',\n",
       " 'test_823.jpg',\n",
       " 'test_824.jpg',\n",
       " 'test_825.jpg',\n",
       " 'test_826.jpg',\n",
       " 'test_827.jpg',\n",
       " 'test_828.jpg',\n",
       " 'test_829.jpg',\n",
       " 'test_830.jpg',\n",
       " 'test_831.jpg',\n",
       " 'test_832.jpg',\n",
       " 'test_833.jpg',\n",
       " 'test_834.jpg',\n",
       " 'test_835.jpg',\n",
       " 'test_836.jpg',\n",
       " 'test_837.jpg',\n",
       " 'test_838.jpg',\n",
       " 'test_839.jpg',\n",
       " 'test_840.jpg',\n",
       " 'test_841.jpg',\n",
       " 'test_842.jpg',\n",
       " 'test_843.jpg',\n",
       " 'test_844.jpg',\n",
       " 'test_845.jpg',\n",
       " 'test_846.jpg',\n",
       " 'test_847.jpg',\n",
       " 'test_848.jpg',\n",
       " 'test_849.jpg',\n",
       " 'test_850.jpg',\n",
       " 'test_851.jpg',\n",
       " 'test_852.jpg',\n",
       " 'test_853.jpg',\n",
       " 'test_854.jpg',\n",
       " 'test_855.jpg',\n",
       " 'test_856.jpg',\n",
       " 'test_857.jpg',\n",
       " 'test_858.jpg',\n",
       " 'test_859.jpg',\n",
       " 'test_860.jpg',\n",
       " 'test_861.jpg',\n",
       " 'test_862.jpg',\n",
       " 'test_863.jpg',\n",
       " 'test_864.jpg',\n",
       " 'test_865.jpg',\n",
       " 'test_866.jpg',\n",
       " 'test_867.jpg',\n",
       " 'test_868.jpg',\n",
       " 'test_869.jpg',\n",
       " 'test_870.jpg',\n",
       " 'test_871.jpg',\n",
       " 'test_872.jpg',\n",
       " 'test_873.jpg',\n",
       " 'test_874.jpg',\n",
       " 'test_875.jpg',\n",
       " 'test_876.jpg',\n",
       " 'test_877.jpg',\n",
       " 'test_878.jpg',\n",
       " 'test_879.jpg',\n",
       " 'test_880.jpg',\n",
       " 'test_881.jpg',\n",
       " 'test_882.jpg',\n",
       " 'test_883.jpg',\n",
       " 'test_884.jpg',\n",
       " 'test_885.jpg',\n",
       " 'test_886.jpg',\n",
       " 'test_887.jpg',\n",
       " 'test_888.jpg',\n",
       " 'test_889.jpg',\n",
       " 'test_890.jpg',\n",
       " 'test_891.jpg',\n",
       " 'test_892.jpg',\n",
       " 'test_893.jpg',\n",
       " 'test_894.jpg',\n",
       " 'test_895.jpg',\n",
       " 'test_896.jpg',\n",
       " 'test_897.jpg',\n",
       " 'test_898.jpg',\n",
       " 'test_899.jpg',\n",
       " 'test_900.jpg',\n",
       " 'test_901.jpg',\n",
       " 'test_902.jpg',\n",
       " 'test_903.jpg',\n",
       " 'test_904.jpg',\n",
       " 'test_905.jpg',\n",
       " 'test_906.jpg',\n",
       " 'test_907.jpg',\n",
       " 'test_908.jpg',\n",
       " 'test_909.jpg',\n",
       " 'test_910.jpg',\n",
       " 'test_911.jpg',\n",
       " 'test_912.jpg',\n",
       " 'test_913.jpg',\n",
       " 'test_914.jpg',\n",
       " 'test_915.jpg',\n",
       " 'test_916.jpg',\n",
       " 'test_917.jpg',\n",
       " 'test_918.jpg',\n",
       " 'test_919.jpg',\n",
       " 'test_920.jpg',\n",
       " 'test_921.jpg',\n",
       " 'test_922.jpg',\n",
       " 'test_923.jpg',\n",
       " 'test_924.jpg',\n",
       " 'test_925.jpg',\n",
       " 'test_926.jpg',\n",
       " 'test_927.jpg',\n",
       " 'test_928.jpg',\n",
       " 'test_929.jpg',\n",
       " 'test_930.jpg',\n",
       " 'test_931.jpg',\n",
       " 'test_932.jpg',\n",
       " 'test_933.jpg',\n",
       " 'test_934.jpg',\n",
       " 'test_935.jpg',\n",
       " 'test_936.jpg',\n",
       " 'test_937.jpg',\n",
       " 'test_938.jpg',\n",
       " 'test_939.jpg',\n",
       " 'test_940.jpg',\n",
       " 'test_941.jpg',\n",
       " 'test_942.jpg',\n",
       " 'test_943.jpg',\n",
       " 'test_944.jpg',\n",
       " 'test_945.jpg',\n",
       " 'test_946.jpg',\n",
       " 'test_947.jpg',\n",
       " 'test_948.jpg',\n",
       " 'test_949.jpg',\n",
       " 'test_950.jpg',\n",
       " 'test_951.jpg',\n",
       " 'test_952.jpg',\n",
       " 'test_953.jpg',\n",
       " 'test_954.jpg',\n",
       " 'test_955.jpg',\n",
       " 'test_956.jpg',\n",
       " 'test_957.jpg',\n",
       " 'test_958.jpg',\n",
       " 'test_959.jpg',\n",
       " 'test_960.jpg',\n",
       " 'test_961.jpg',\n",
       " 'test_962.jpg',\n",
       " 'test_963.jpg',\n",
       " 'test_964.jpg',\n",
       " 'test_965.jpg',\n",
       " 'test_966.jpg',\n",
       " 'test_967.jpg',\n",
       " 'test_968.jpg',\n",
       " 'test_969.jpg',\n",
       " 'test_970.jpg',\n",
       " 'test_971.jpg',\n",
       " 'test_972.jpg',\n",
       " 'test_973.jpg',\n",
       " 'test_974.jpg',\n",
       " 'test_975.jpg',\n",
       " 'test_976.jpg',\n",
       " 'test_977.jpg',\n",
       " 'test_978.jpg',\n",
       " 'test_979.jpg',\n",
       " 'test_980.jpg',\n",
       " 'test_981.jpg',\n",
       " 'test_982.jpg',\n",
       " 'test_983.jpg',\n",
       " 'test_984.jpg',\n",
       " 'test_985.jpg',\n",
       " 'test_986.jpg',\n",
       " 'test_987.jpg',\n",
       " 'test_988.jpg',\n",
       " 'test_989.jpg',\n",
       " 'test_990.jpg',\n",
       " 'test_991.jpg',\n",
       " 'test_992.jpg',\n",
       " 'test_993.jpg',\n",
       " 'test_994.jpg',\n",
       " 'test_995.jpg',\n",
       " 'test_996.jpg',\n",
       " 'test_997.jpg',\n",
       " 'test_998.jpg',\n",
       " 'test_999.jpg',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natsorted(test_data_names, key=lambda y: y.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.jpg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_names[0].split('_')[1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
