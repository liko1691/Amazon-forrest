{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from natsort import natsorted, ns\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PathDetermination(user):\n",
    "    if user == 'litvin_home':\n",
    "        core_dir = 'C:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "    \n",
    "    if user == 'litvin_office':\n",
    "        core_dir = 'D:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "        train_dir = core_dir + 'train-jpg\\\\'\n",
    "        test_dir = core_dir + 'test-jpg\\\\'\n",
    "        add_test_dir = core_dir + 'test-jpg-additional\\\\'\n",
    "    \n",
    "    if user == 'savina':\n",
    "        core_dir = 'Z:\\\\Kaggle Amazon Rainforest\\\\'\n",
    "        train_dir = ''\n",
    "        test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg\\\\'\n",
    "        add_test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg-additional\\\\'\n",
    "    \n",
    "    return core_dir, train_dir, test_dir, add_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_dir, train_dir, test_dir, add_test_dir = PathDetermination('litvin_office')\n",
    "#core_dir, test_dir, add_test_dir = path_determination('savina')\n",
    "\n",
    "test_data_names = natsorted(os.listdir(test_dir), key=lambda y: y.lower())\n",
    "add_test_data_names = os.listdir(add_test_dir)\n",
    "\n",
    "\n",
    "cathegories = ['agriculture', 'artisinal_mine', 'bare_ground', \n",
    "                      'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', \n",
    "                      'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', \n",
    "                      'road', 'selective_logging', 'slash_burn', 'water']\n",
    "\n",
    "train_data = pd.read_csv(core_dir + 'train_v2.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BinimialPrediction(X, treshold = 0.5):\n",
    "    result = np.zeros(X.shape)\n",
    "    \n",
    "    if type(treshold) == int or type(treshold) == float:\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if X[i, j] >= treshold:\n",
    "                    result[i, j] = 1\n",
    "    elif type(treshold) == list:\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if X[i, j] >= treshold[j]:\n",
    "                    result[i, j] = 1\n",
    "    else:\n",
    "        print('treshold type must be int, float or list')\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def FBettaScore(x_true, x_predicted, betta = 2):\n",
    "    if len(x_true) == len(x_predicted):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        for i in range(len(x_predicted)):\n",
    "            if x_true[i] == 1 and x_predicted[i] == 1:\n",
    "                tp += 1\n",
    "            \n",
    "            if x_true[i] == 0 and x_predicted[i] == 1:\n",
    "                fp += 1\n",
    "            \n",
    "            if x_true[i] == 1 and x_predicted[i] == 0:\n",
    "                fn += 1\n",
    "        \n",
    "        if tp == 0 or (tp + fp) == 0 or (tp + fn) == 0:\n",
    "            return(0)\n",
    "        else:\n",
    "            precision = tp/(tp + fp)\n",
    "            recall = tp/(tp + fn)\n",
    "            \n",
    "            return((1 + betta**2)*precision*recall/(betta**2*precision + recall))\n",
    "    else:\n",
    "        print('FBettaScore error! len(x_true) != len(x_predicted)')\n",
    "\n",
    "def AvgFBettaScore(x_true, x_predicted, betta = 2, treshold = 0.5):\n",
    "    result = 0\n",
    "    n = x_true.shape[0]\n",
    "    \n",
    "    x_predicted = BinimialPrediction(x_predicted, treshold)\n",
    "    \n",
    "    for i in range(n):\n",
    "        result += FBettaScore(x_true[i, :], x_predicted[i, :], betta)\n",
    "    \n",
    "    return(result/n)\n",
    "\n",
    "def cathegory_intersections(data):\n",
    "    intersections = np.zeros((len(cathegories), len(cathegories)))\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        true_ind = []\n",
    "        for j in range(data.shape[1]):\n",
    "            if data[i, j] == 1:\n",
    "                true_ind.append(j)\n",
    "        \n",
    "        for k1 in true_ind:\n",
    "            for k2 in true_ind:\n",
    "                intersections[k1, k2] += 1\n",
    "    \n",
    "    intersections = intersections.astype(int)\n",
    "    df = pd.DataFrame(intersections, index= cathegories, columns= cathegories)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DataPreperation(data_type, img_size = (32, 32), rotation = [0], \n",
    "                    test_img_dir = [test_dir, add_test_dir], shuffle = False):\n",
    "    if data_type == 'train':\n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        for img_name in tqdm(train_data.image_name.values):\n",
    "    \n",
    "            img = cv2.imread(train_dir + img_name + '.jpg')\n",
    "            img_resized = cv2.resize(img, img_size)\n",
    "            \n",
    "            img_tags = train_data[train_data['image_name'] == img_name]['tags'].values[0].split(' ')\n",
    "            y = np.zeros(len(cathegories))\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                if cathegories[i] in img_tags:\n",
    "                    y[i] = 1\n",
    "            \n",
    "            for angle in rotation:\n",
    "                img = imutils.rotate(img_resized, angle)\n",
    "                X.append(img)\n",
    "                Y.append(y) \n",
    "        \n",
    "        X = np.array(X, np.float16) / 255.\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        if shuffle:\n",
    "            ind = np.random.choice(np.arange(X.shape[0]), X.shape[0], replace= False)\n",
    "            X = X[ind]\n",
    "            Y = Y[ind]\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    if data_type == 'test':\n",
    "        X_test = []\n",
    "        X_test_names = []\n",
    "\n",
    "        for img_dir in test_img_dir:\n",
    "            img_dir_names = os.listdir(img_dir)\n",
    "            \n",
    "            for img_name in tqdm(img_dir_names):\n",
    "                if img_name.endswith(\"jpg\"):\n",
    "                    img = cv2.imread(img_dir + img_name)\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    \n",
    "                    X_test.append(img)\n",
    "                    X_test_names.append(img_name)\n",
    "    \n",
    "        X_test = np.array(X_test, np.float16) / 255.\n",
    "        \n",
    "        return X_test, X_test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40479/40479 [16:40<00:00, 40.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = DataPreperation('train', rotation = [0, 90, 180, 270], shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиение выборки на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пересечения категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agriculture</th>\n",
       "      <th>artisinal_mine</th>\n",
       "      <th>bare_ground</th>\n",
       "      <th>blooming</th>\n",
       "      <th>blow_down</th>\n",
       "      <th>clear</th>\n",
       "      <th>cloudy</th>\n",
       "      <th>conventional_mine</th>\n",
       "      <th>cultivation</th>\n",
       "      <th>habitation</th>\n",
       "      <th>haze</th>\n",
       "      <th>partly_cloudy</th>\n",
       "      <th>primary</th>\n",
       "      <th>road</th>\n",
       "      <th>selective_logging</th>\n",
       "      <th>slash_burn</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agriculture</th>\n",
       "      <td>49260</td>\n",
       "      <td>152</td>\n",
       "      <td>900</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>36600</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>13508</td>\n",
       "      <td>10948</td>\n",
       "      <td>2688</td>\n",
       "      <td>9972</td>\n",
       "      <td>47888</td>\n",
       "      <td>24136</td>\n",
       "      <td>260</td>\n",
       "      <td>476</td>\n",
       "      <td>10848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artisinal_mine</th>\n",
       "      <td>152</td>\n",
       "      <td>1356</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>1296</td>\n",
       "      <td>440</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_ground</th>\n",
       "      <td>900</td>\n",
       "      <td>160</td>\n",
       "      <td>3448</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>2988</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>356</td>\n",
       "      <td>652</td>\n",
       "      <td>164</td>\n",
       "      <td>296</td>\n",
       "      <td>2732</td>\n",
       "      <td>1292</td>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blooming</th>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1328</td>\n",
       "      <td>4</td>\n",
       "      <td>1244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>1328</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blow_down</th>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>392</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>392</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear</th>\n",
       "      <td>36600</td>\n",
       "      <td>1228</td>\n",
       "      <td>2988</td>\n",
       "      <td>1244</td>\n",
       "      <td>340</td>\n",
       "      <td>113724</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>14108</td>\n",
       "      <td>12360</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110672</td>\n",
       "      <td>25180</td>\n",
       "      <td>1232</td>\n",
       "      <td>692</td>\n",
       "      <td>22008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cloudy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8356</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conventional_mine</th>\n",
       "      <td>96</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>16</td>\n",
       "      <td>144</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>376</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cultivation</th>\n",
       "      <td>13508</td>\n",
       "      <td>72</td>\n",
       "      <td>356</td>\n",
       "      <td>140</td>\n",
       "      <td>32</td>\n",
       "      <td>14108</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>17908</td>\n",
       "      <td>3580</td>\n",
       "      <td>808</td>\n",
       "      <td>2992</td>\n",
       "      <td>17820</td>\n",
       "      <td>5176</td>\n",
       "      <td>232</td>\n",
       "      <td>504</td>\n",
       "      <td>3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>habitation</th>\n",
       "      <td>10948</td>\n",
       "      <td>116</td>\n",
       "      <td>652</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>12360</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>3580</td>\n",
       "      <td>14640</td>\n",
       "      <td>516</td>\n",
       "      <td>1764</td>\n",
       "      <td>13876</td>\n",
       "      <td>11144</td>\n",
       "      <td>52</td>\n",
       "      <td>164</td>\n",
       "      <td>3660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haze</th>\n",
       "      <td>2688</td>\n",
       "      <td>20</td>\n",
       "      <td>164</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>808</td>\n",
       "      <td>516</td>\n",
       "      <td>10788</td>\n",
       "      <td>0</td>\n",
       "      <td>10680</td>\n",
       "      <td>1576</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>2452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partly_cloudy</th>\n",
       "      <td>9972</td>\n",
       "      <td>108</td>\n",
       "      <td>296</td>\n",
       "      <td>68</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>2992</td>\n",
       "      <td>1764</td>\n",
       "      <td>0</td>\n",
       "      <td>29044</td>\n",
       "      <td>28700</td>\n",
       "      <td>5528</td>\n",
       "      <td>108</td>\n",
       "      <td>132</td>\n",
       "      <td>5180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary</th>\n",
       "      <td>47888</td>\n",
       "      <td>1296</td>\n",
       "      <td>2732</td>\n",
       "      <td>1328</td>\n",
       "      <td>392</td>\n",
       "      <td>110672</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>17820</td>\n",
       "      <td>13876</td>\n",
       "      <td>10680</td>\n",
       "      <td>28700</td>\n",
       "      <td>150052</td>\n",
       "      <td>30912</td>\n",
       "      <td>1360</td>\n",
       "      <td>836</td>\n",
       "      <td>28004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road</th>\n",
       "      <td>24136</td>\n",
       "      <td>440</td>\n",
       "      <td>1292</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>25180</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>5176</td>\n",
       "      <td>11144</td>\n",
       "      <td>1576</td>\n",
       "      <td>5528</td>\n",
       "      <td>30912</td>\n",
       "      <td>32284</td>\n",
       "      <td>604</td>\n",
       "      <td>144</td>\n",
       "      <td>8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selective_logging</th>\n",
       "      <td>260</td>\n",
       "      <td>24</td>\n",
       "      <td>52</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>1232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>1360</td>\n",
       "      <td>604</td>\n",
       "      <td>1360</td>\n",
       "      <td>8</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slash_burn</th>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>504</td>\n",
       "      <td>164</td>\n",
       "      <td>12</td>\n",
       "      <td>132</td>\n",
       "      <td>836</td>\n",
       "      <td>144</td>\n",
       "      <td>8</td>\n",
       "      <td>836</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>10848</td>\n",
       "      <td>1196</td>\n",
       "      <td>824</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>22008</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>3472</td>\n",
       "      <td>3660</td>\n",
       "      <td>2452</td>\n",
       "      <td>5180</td>\n",
       "      <td>28004</td>\n",
       "      <td>8500</td>\n",
       "      <td>196</td>\n",
       "      <td>96</td>\n",
       "      <td>29644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   agriculture  artisinal_mine  bare_ground  blooming  \\\n",
       "agriculture              49260             152          900       128   \n",
       "artisinal_mine             152            1356          160         0   \n",
       "bare_ground                900             160         3448        12   \n",
       "blooming                   128               0           12      1328   \n",
       "blow_down                   88               0           16         4   \n",
       "clear                    36600            1228         2988      1244   \n",
       "cloudy                       0               0            0         0   \n",
       "conventional_mine           96              16           40         0   \n",
       "cultivation              13508              72          356       140   \n",
       "habitation               10948             116          652        16   \n",
       "haze                      2688              20          164        16   \n",
       "partly_cloudy             9972             108          296        68   \n",
       "primary                  47888            1296         2732      1328   \n",
       "road                     24136             440         1292        40   \n",
       "selective_logging          260              24           52        28   \n",
       "slash_burn                 476               0           40         8   \n",
       "water                    10848            1196          824        64   \n",
       "\n",
       "                   blow_down   clear  cloudy  conventional_mine  cultivation  \\\n",
       "agriculture               88   36600       0                 96        13508   \n",
       "artisinal_mine             0    1228       0                 16           72   \n",
       "bare_ground               16    2988       0                 40          356   \n",
       "blooming                   4    1244       0                  0          140   \n",
       "blow_down                392     340       0                  0           32   \n",
       "clear                    340  113724       0                280        14108   \n",
       "cloudy                     0       0    8356                  0            0   \n",
       "conventional_mine          0     280       0                400           16   \n",
       "cultivation               32   14108       0                 16        17908   \n",
       "habitation                12   12360       0                144         3580   \n",
       "haze                       0       0       0                  8          808   \n",
       "partly_cloudy             52       0       0                112         2992   \n",
       "primary                  392  110672       0                376        17820   \n",
       "road                       8   25180       0                236         5176   \n",
       "selective_logging          4    1232       0                  0          232   \n",
       "slash_burn                 8     692       0                  0          504   \n",
       "water                     12   22008       0                104         3472   \n",
       "\n",
       "                   habitation   haze  partly_cloudy  primary   road  \\\n",
       "agriculture             10948   2688           9972    47888  24136   \n",
       "artisinal_mine            116     20            108     1296    440   \n",
       "bare_ground               652    164            296     2732   1292   \n",
       "blooming                   16     16             68     1328     40   \n",
       "blow_down                  12      0             52      392      8   \n",
       "clear                   12360      0              0   110672  25180   \n",
       "cloudy                      0      0              0        0      0   \n",
       "conventional_mine         144      8            112      376    236   \n",
       "cultivation              3580    808           2992    17820   5176   \n",
       "habitation              14640    516           1764    13876  11144   \n",
       "haze                      516  10788              0    10680   1576   \n",
       "partly_cloudy            1764      0          29044    28700   5528   \n",
       "primary                 13876  10680          28700   150052  30912   \n",
       "road                    11144   1576           5528    30912  32284   \n",
       "selective_logging          52     20            108     1360    604   \n",
       "slash_burn                164     12            132      836    144   \n",
       "water                    3660   2452           5180    28004   8500   \n",
       "\n",
       "                   selective_logging  slash_burn  water  \n",
       "agriculture                      260         476  10848  \n",
       "artisinal_mine                    24           0   1196  \n",
       "bare_ground                       52          40    824  \n",
       "blooming                          28           8     64  \n",
       "blow_down                          4           8     12  \n",
       "clear                           1232         692  22008  \n",
       "cloudy                             0           0      0  \n",
       "conventional_mine                  0           0    104  \n",
       "cultivation                      232         504   3472  \n",
       "habitation                        52         164   3660  \n",
       "haze                              20          12   2452  \n",
       "partly_cloudy                    108         132   5180  \n",
       "primary                         1360         836  28004  \n",
       "road                             604         144   8500  \n",
       "selective_logging               1360           8    196  \n",
       "slash_burn                         8         836     96  \n",
       "water                            196          96  29644  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cathegory_intersections(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x37f91d68>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAFNCAYAAABL8stCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9k8vFjrHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x39f2bd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cathegory_intersection_zeros = np.zeros((len(cathegories), len(cathegories)))\n",
    "inter = cathegory_intersections(Y)\n",
    "\n",
    "for i in range(cathegory_intersection_zeros.shape[0]):\n",
    "    for j in range(cathegory_intersection_zeros.shape[1]):\n",
    "        if inter.values[i, j] != 0:\n",
    "            cathegory_intersection_zeros[i, j] = 1\n",
    "        \n",
    "sns.heatmap(pd.DataFrame(cathegory_intersection_zeros, index= cathegories, columns= cathegories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Настройка и  обучение сети для всех категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 150 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# загрузка изображений и их приведение к подходящему для обработки виду\n",
    "num_train = X.shape[0]\n",
    "depth = 32 \n",
    "height = 32\n",
    "width = 3 \n",
    "\n",
    "num_classes = len(cathegories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113341 samples, validate on 48575 samples\n",
      "Epoch 1/150\n",
      "113341/113341 [==============================] - 660s - loss: 0.1886 - acc: 0.9264 - val_loss: 0.1486 - val_acc: 0.9412\n",
      "Epoch 2/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.1440 - acc: 0.9436 - val_loss: 0.1310 - val_acc: 0.9474\n",
      "Epoch 3/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.1336 - acc: 0.9476 - val_loss: 0.1279 - val_acc: 0.9491\n",
      "Epoch 4/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.1277 - acc: 0.9500 - val_loss: 0.1221 - val_acc: 0.9516\n",
      "Epoch 5/150\n",
      "113341/113341 [==============================] - 663s - loss: 0.1235 - acc: 0.9517 - val_loss: 0.1208 - val_acc: 0.9523\n",
      "Epoch 6/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.1202 - acc: 0.9529 - val_loss: 0.1186 - val_acc: 0.9533\n",
      "Epoch 7/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.1174 - acc: 0.9542 - val_loss: 0.1106 - val_acc: 0.9564\n",
      "Epoch 8/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.1147 - acc: 0.9551 - val_loss: 0.1112 - val_acc: 0.9562\n",
      "Epoch 9/150\n",
      "113341/113341 [==============================] - 656s - loss: 0.1130 - acc: 0.9559 - val_loss: 0.1132 - val_acc: 0.9557\n",
      "Epoch 10/150\n",
      "113341/113341 [==============================] - 655s - loss: 0.1117 - acc: 0.9561 - val_loss: 0.1116 - val_acc: 0.9555\n",
      "Epoch 11/150\n",
      "113341/113341 [==============================] - 656s - loss: 0.1103 - acc: 0.9567 - val_loss: 0.1089 - val_acc: 0.9575\n",
      "Epoch 12/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.1087 - acc: 0.9573 - val_loss: 0.1077 - val_acc: 0.9577\n",
      "Epoch 13/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.1073 - acc: 0.9578 - val_loss: 0.1080 - val_acc: 0.9576\n",
      "Epoch 14/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.1059 - acc: 0.9582 - val_loss: 0.1067 - val_acc: 0.9580\n",
      "Epoch 15/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.1044 - acc: 0.9587 - val_loss: 0.1073 - val_acc: 0.9578\n",
      "Epoch 16/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.1036 - acc: 0.9590 - val_loss: 0.1075 - val_acc: 0.9581\n",
      "Epoch 17/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.1024 - acc: 0.9595 - val_loss: 0.1062 - val_acc: 0.9583\n",
      "Epoch 18/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.1019 - acc: 0.9596 - val_loss: 0.1063 - val_acc: 0.9583\n",
      "Epoch 19/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.1000 - acc: 0.9602 - val_loss: 0.1070 - val_acc: 0.9585\n",
      "Epoch 20/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.0994 - acc: 0.9606 - val_loss: 0.1075 - val_acc: 0.9584\n",
      "Epoch 21/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.0980 - acc: 0.9609 - val_loss: 0.1072 - val_acc: 0.9583\n",
      "Epoch 22/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.0966 - acc: 0.9615 - val_loss: 0.1085 - val_acc: 0.9580\n",
      "Epoch 23/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.0959 - acc: 0.9618 - val_loss: 0.1088 - val_acc: 0.9576\n",
      "Epoch 24/150\n",
      "113341/113341 [==============================] - 660s - loss: 0.0956 - acc: 0.9619 - val_loss: 0.1084 - val_acc: 0.9580\n",
      "Epoch 25/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.0944 - acc: 0.9624 - val_loss: 0.1085 - val_acc: 0.9585\n",
      "Epoch 26/150\n",
      "113341/113341 [==============================] - 658s - loss: 0.0933 - acc: 0.9627 - val_loss: 0.1104 - val_acc: 0.9575\n",
      "Epoch 27/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.0928 - acc: 0.9630 - val_loss: 0.1098 - val_acc: 0.9581\n",
      "Epoch 28/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.0915 - acc: 0.9634 - val_loss: 0.1080 - val_acc: 0.9589\n",
      "Epoch 29/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0909 - acc: 0.9636 - val_loss: 0.1110 - val_acc: 0.9578\n",
      "Epoch 30/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0902 - acc: 0.9639 - val_loss: 0.1147 - val_acc: 0.9569\n",
      "Epoch 31/150\n",
      "113341/113341 [==============================] - 660s - loss: 0.0894 - acc: 0.9643 - val_loss: 0.1097 - val_acc: 0.9583\n",
      "Epoch 32/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.0886 - acc: 0.9644 - val_loss: 0.1106 - val_acc: 0.9582\n",
      "Epoch 33/150\n",
      "113341/113341 [==============================] - 660s - loss: 0.0878 - acc: 0.9649 - val_loss: 0.1114 - val_acc: 0.9579\n",
      "Epoch 34/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0875 - acc: 0.9650 - val_loss: 0.1135 - val_acc: 0.9578\n",
      "Epoch 35/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.0862 - acc: 0.9654 - val_loss: 0.1160 - val_acc: 0.9567\n",
      "Epoch 36/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0866 - acc: 0.9654 - val_loss: 0.1134 - val_acc: 0.9574\n",
      "Epoch 37/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0865 - acc: 0.9652 - val_loss: 0.1136 - val_acc: 0.9581\n",
      "Epoch 38/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0848 - acc: 0.9660 - val_loss: 0.1129 - val_acc: 0.9578\n",
      "Epoch 39/150\n",
      "113341/113341 [==============================] - 663s - loss: 0.0847 - acc: 0.9659 - val_loss: 0.1127 - val_acc: 0.9581\n",
      "Epoch 40/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0839 - acc: 0.9663 - val_loss: 0.1147 - val_acc: 0.9579\n",
      "Epoch 41/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0837 - acc: 0.9663 - val_loss: 0.1140 - val_acc: 0.9581\n",
      "Epoch 42/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.0829 - acc: 0.9668 - val_loss: 0.1148 - val_acc: 0.9579\n",
      "Epoch 43/150\n",
      "113341/113341 [==============================] - 661s - loss: 0.0828 - acc: 0.9666 - val_loss: 0.1156 - val_acc: 0.9579\n",
      "Epoch 44/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0825 - acc: 0.9668 - val_loss: 0.1150 - val_acc: 0.9574\n",
      "Epoch 45/150\n",
      "113341/113341 [==============================] - 663s - loss: 0.0821 - acc: 0.9670 - val_loss: 0.1129 - val_acc: 0.9584\n",
      "Epoch 46/150\n",
      "113341/113341 [==============================] - 690s - loss: 0.0813 - acc: 0.9673 - val_loss: 0.1165 - val_acc: 0.9576\n",
      "Epoch 47/150\n",
      "113341/113341 [==============================] - 751s - loss: 0.0811 - acc: 0.9675 - val_loss: 0.1154 - val_acc: 0.9578\n",
      "Epoch 48/150\n",
      "113341/113341 [==============================] - 753s - loss: 0.0808 - acc: 0.9675 - val_loss: 0.1149 - val_acc: 0.9577\n",
      "Epoch 49/150\n",
      "113341/113341 [==============================] - 772s - loss: 0.0801 - acc: 0.9677 - val_loss: 0.1174 - val_acc: 0.9575\n",
      "Epoch 50/150\n",
      "113341/113341 [==============================] - 734s - loss: 0.0792 - acc: 0.9682 - val_loss: 0.1152 - val_acc: 0.9580\n",
      "Epoch 51/150\n",
      "113341/113341 [==============================] - 742s - loss: 0.0792 - acc: 0.9682 - val_loss: 0.1171 - val_acc: 0.9579\n",
      "Epoch 52/150\n",
      "113341/113341 [==============================] - 731s - loss: 0.0791 - acc: 0.9681 - val_loss: 0.1182 - val_acc: 0.9575\n",
      "Epoch 53/150\n",
      "113341/113341 [==============================] - 744s - loss: 0.0790 - acc: 0.9682 - val_loss: 0.1180 - val_acc: 0.9584\n",
      "Epoch 54/150\n",
      "113341/113341 [==============================] - 733s - loss: 0.0783 - acc: 0.9686 - val_loss: 0.1183 - val_acc: 0.9579\n",
      "Epoch 55/150\n",
      "113341/113341 [==============================] - 745s - loss: 0.0781 - acc: 0.9684 - val_loss: 0.1171 - val_acc: 0.9578\n",
      "Epoch 56/150\n",
      "113341/113341 [==============================] - 746s - loss: 0.0778 - acc: 0.9688 - val_loss: 0.1171 - val_acc: 0.9578\n",
      "Epoch 57/150\n",
      "113341/113341 [==============================] - 745s - loss: 0.0773 - acc: 0.9689 - val_loss: 0.1189 - val_acc: 0.9577\n",
      "Epoch 58/150\n",
      "113341/113341 [==============================] - 743s - loss: 0.0774 - acc: 0.9690 - val_loss: 0.1184 - val_acc: 0.9576\n",
      "Epoch 59/150\n",
      "113341/113341 [==============================] - 739s - loss: 0.0765 - acc: 0.9693 - val_loss: 0.1173 - val_acc: 0.9581\n",
      "Epoch 60/150\n",
      "113341/113341 [==============================] - 744s - loss: 0.0768 - acc: 0.9690 - val_loss: 0.1198 - val_acc: 0.9573\n",
      "Epoch 61/150\n",
      "113341/113341 [==============================] - 715s - loss: 0.0761 - acc: 0.9693 - val_loss: 0.1205 - val_acc: 0.9578\n",
      "Epoch 62/150\n",
      "113341/113341 [==============================] - 746s - loss: 0.0762 - acc: 0.9693 - val_loss: 0.1190 - val_acc: 0.9576\n",
      "Epoch 63/150\n",
      "113341/113341 [==============================] - 796s - loss: 0.0757 - acc: 0.9695 - val_loss: 0.1204 - val_acc: 0.9574\n",
      "Epoch 64/150\n",
      "113341/113341 [==============================] - 817s - loss: 0.0759 - acc: 0.9696 - val_loss: 0.1200 - val_acc: 0.9576\n",
      "Epoch 65/150\n",
      "113341/113341 [==============================] - 821s - loss: 0.0754 - acc: 0.9696 - val_loss: 0.1191 - val_acc: 0.9573\n",
      "Epoch 66/150\n",
      "113341/113341 [==============================] - 805s - loss: 0.0748 - acc: 0.9699 - val_loss: 0.1206 - val_acc: 0.9575\n",
      "Epoch 67/150\n",
      "113341/113341 [==============================] - 743s - loss: 0.0744 - acc: 0.9701 - val_loss: 0.1201 - val_acc: 0.9577\n",
      "Epoch 68/150\n",
      "113341/113341 [==============================] - 744s - loss: 0.0742 - acc: 0.9702 - val_loss: 0.1211 - val_acc: 0.9571\n",
      "Epoch 69/150\n",
      "113341/113341 [==============================] - 741s - loss: 0.0744 - acc: 0.9702 - val_loss: 0.1223 - val_acc: 0.9571\n",
      "Epoch 70/150\n",
      "113341/113341 [==============================] - 745s - loss: 0.0744 - acc: 0.9702 - val_loss: 0.1205 - val_acc: 0.9575\n",
      "Epoch 71/150\n",
      "113341/113341 [==============================] - 744s - loss: 0.0737 - acc: 0.9702 - val_loss: 0.1229 - val_acc: 0.9573\n",
      "Epoch 72/150\n",
      "113341/113341 [==============================] - 742s - loss: 0.0739 - acc: 0.9703 - val_loss: 0.1207 - val_acc: 0.9575\n",
      "Epoch 73/150\n",
      "113341/113341 [==============================] - 749s - loss: 0.0734 - acc: 0.9707 - val_loss: 0.1199 - val_acc: 0.9575\n",
      "Epoch 74/150\n",
      "113341/113341 [==============================] - 752s - loss: 0.0734 - acc: 0.9705 - val_loss: 0.1235 - val_acc: 0.9569\n",
      "Epoch 75/150\n",
      "113341/113341 [==============================] - 752s - loss: 0.0726 - acc: 0.9708 - val_loss: 0.1199 - val_acc: 0.9576\n",
      "Epoch 76/150\n",
      "113341/113341 [==============================] - 749s - loss: 0.0727 - acc: 0.9708 - val_loss: 0.1215 - val_acc: 0.9573\n",
      "Epoch 77/150\n",
      "113341/113341 [==============================] - 749s - loss: 0.0728 - acc: 0.9709 - val_loss: 0.1213 - val_acc: 0.9580\n",
      "Epoch 78/150\n",
      "113341/113341 [==============================] - 751s - loss: 0.0720 - acc: 0.9710 - val_loss: 0.1203 - val_acc: 0.9575\n",
      "Epoch 79/150\n",
      "113341/113341 [==============================] - 749s - loss: 0.0723 - acc: 0.9709 - val_loss: 0.1228 - val_acc: 0.9571\n",
      "Epoch 80/150\n",
      "113341/113341 [==============================] - 751s - loss: 0.0719 - acc: 0.9711 - val_loss: 0.1237 - val_acc: 0.9577\n",
      "Epoch 81/150\n",
      "113341/113341 [==============================] - 746s - loss: 0.0713 - acc: 0.9713 - val_loss: 0.1218 - val_acc: 0.9575\n",
      "Epoch 82/150\n",
      "113341/113341 [==============================] - 719s - loss: 0.0709 - acc: 0.9716 - val_loss: 0.1228 - val_acc: 0.9574\n",
      "Epoch 83/150\n",
      "113341/113341 [==============================] - 675s - loss: 0.0715 - acc: 0.9712 - val_loss: 0.1216 - val_acc: 0.9573\n",
      "Epoch 84/150\n",
      "113341/113341 [==============================] - 719s - loss: 0.0716 - acc: 0.9712 - val_loss: 0.1218 - val_acc: 0.9575\n",
      "Epoch 85/150\n",
      "113341/113341 [==============================] - 738s - loss: 0.0712 - acc: 0.9714 - val_loss: 0.1228 - val_acc: 0.9575\n",
      "Epoch 86/150\n",
      "113341/113341 [==============================] - 823s - loss: 0.0710 - acc: 0.9715 - val_loss: 0.1221 - val_acc: 0.9559\n",
      "Epoch 87/150\n",
      "113341/113341 [==============================] - 784s - loss: 0.0708 - acc: 0.9716 - val_loss: 0.1228 - val_acc: 0.9573\n",
      "Epoch 88/150\n",
      "113341/113341 [==============================] - 736s - loss: 0.0707 - acc: 0.9718 - val_loss: 0.1205 - val_acc: 0.9576\n",
      "Epoch 89/150\n",
      "113341/113341 [==============================] - 689s - loss: 0.0701 - acc: 0.9720 - val_loss: 0.1220 - val_acc: 0.9567\n",
      "Epoch 90/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0703 - acc: 0.9718 - val_loss: 0.1242 - val_acc: 0.9569\n",
      "Epoch 91/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0702 - acc: 0.9720 - val_loss: 0.1297 - val_acc: 0.9568\n",
      "Epoch 92/150\n",
      "113341/113341 [==============================] - 682s - loss: 0.0700 - acc: 0.9719 - val_loss: 0.1239 - val_acc: 0.9572\n",
      "Epoch 93/150\n",
      "113341/113341 [==============================] - 673s - loss: 0.0700 - acc: 0.9720 - val_loss: 0.1257 - val_acc: 0.9576\n",
      "Epoch 94/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0697 - acc: 0.9722 - val_loss: 0.1267 - val_acc: 0.9573\n",
      "Epoch 95/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0698 - acc: 0.9720 - val_loss: 0.1264 - val_acc: 0.9575\n",
      "Epoch 96/150\n",
      "113341/113341 [==============================] - 663s - loss: 0.0690 - acc: 0.9724 - val_loss: 0.1241 - val_acc: 0.9577\n",
      "Epoch 97/150\n",
      "113341/113341 [==============================] - 663s - loss: 0.0700 - acc: 0.9719 - val_loss: 0.1233 - val_acc: 0.9574\n",
      "Epoch 98/150\n",
      "113341/113341 [==============================] - 663s - loss: 0.0687 - acc: 0.9726 - val_loss: 0.1221 - val_acc: 0.9573\n",
      "Epoch 99/150\n",
      "113341/113341 [==============================] - 676s - loss: 0.0688 - acc: 0.9724 - val_loss: 0.1283 - val_acc: 0.9568\n",
      "Epoch 100/150\n",
      "113341/113341 [==============================] - 717s - loss: 0.0687 - acc: 0.9726 - val_loss: 0.1244 - val_acc: 0.9571\n",
      "Epoch 101/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0685 - acc: 0.9725 - val_loss: 0.1242 - val_acc: 0.9572\n",
      "Epoch 102/150\n",
      "113341/113341 [==============================] - 659s - loss: 0.0689 - acc: 0.9725 - val_loss: 0.1227 - val_acc: 0.9570\n",
      "Epoch 103/150\n",
      "113341/113341 [==============================] - 660s - loss: 0.0689 - acc: 0.9723 - val_loss: 0.1268 - val_acc: 0.9571\n",
      "Epoch 104/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0684 - acc: 0.9726 - val_loss: 0.1254 - val_acc: 0.9568\n",
      "Epoch 105/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0684 - acc: 0.9727 - val_loss: 0.1262 - val_acc: 0.9572\n",
      "Epoch 106/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0676 - acc: 0.9729 - val_loss: 0.1267 - val_acc: 0.9570\n",
      "Epoch 107/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0685 - acc: 0.9726 - val_loss: 0.1265 - val_acc: 0.9575\n",
      "Epoch 108/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0676 - acc: 0.9729 - val_loss: 0.1239 - val_acc: 0.9576\n",
      "Epoch 109/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0675 - acc: 0.9730 - val_loss: 0.1248 - val_acc: 0.9574\n",
      "Epoch 110/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0671 - acc: 0.9732 - val_loss: 0.1292 - val_acc: 0.9569\n",
      "Epoch 111/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0675 - acc: 0.9729 - val_loss: 0.1261 - val_acc: 0.9571\n",
      "Epoch 112/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0678 - acc: 0.9730 - val_loss: 0.1275 - val_acc: 0.9574\n",
      "Epoch 113/150\n",
      "113341/113341 [==============================] - 662s - loss: 0.0674 - acc: 0.9730 - val_loss: 0.1262 - val_acc: 0.9570\n",
      "Epoch 114/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0666 - acc: 0.9734 - val_loss: 0.1257 - val_acc: 0.9569\n",
      "Epoch 115/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0670 - acc: 0.9730 - val_loss: 0.1256 - val_acc: 0.9577\n",
      "Epoch 116/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0670 - acc: 0.9733 - val_loss: 0.1334 - val_acc: 0.9566\n",
      "Epoch 117/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0670 - acc: 0.9733 - val_loss: 0.1255 - val_acc: 0.9575\n",
      "Epoch 118/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0662 - acc: 0.9734 - val_loss: 0.1258 - val_acc: 0.9571\n",
      "Epoch 119/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0665 - acc: 0.9735 - val_loss: 0.1274 - val_acc: 0.9574\n",
      "Epoch 120/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.0663 - acc: 0.9735 - val_loss: 0.1296 - val_acc: 0.9567\n",
      "Epoch 121/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0668 - acc: 0.9733 - val_loss: 0.1282 - val_acc: 0.9567\n",
      "Epoch 122/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0660 - acc: 0.9737 - val_loss: 0.1312 - val_acc: 0.9570\n",
      "Epoch 123/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.0659 - acc: 0.9737 - val_loss: 0.1273 - val_acc: 0.9575\n",
      "Epoch 124/150\n",
      "113341/113341 [==============================] - 664s - loss: 0.0660 - acc: 0.9736 - val_loss: 0.1252 - val_acc: 0.9570\n",
      "Epoch 125/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.0660 - acc: 0.9737 - val_loss: 0.1255 - val_acc: 0.9565\n",
      "Epoch 126/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.0664 - acc: 0.9736 - val_loss: 0.1265 - val_acc: 0.9575\n",
      "Epoch 127/150\n",
      "113341/113341 [==============================] - 671s - loss: 0.0655 - acc: 0.9738 - val_loss: 0.1303 - val_acc: 0.9575\n",
      "Epoch 128/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0659 - acc: 0.9736 - val_loss: 0.1268 - val_acc: 0.9571\n",
      "Epoch 129/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0659 - acc: 0.9737 - val_loss: 0.1280 - val_acc: 0.9572\n",
      "Epoch 130/150\n",
      "113341/113341 [==============================] - 665s - loss: 0.0654 - acc: 0.9739 - val_loss: 0.1257 - val_acc: 0.9571\n",
      "Epoch 131/150\n",
      "113341/113341 [==============================] - 670s - loss: 0.0656 - acc: 0.9739 - val_loss: 0.1271 - val_acc: 0.9573\n",
      "Epoch 132/150\n",
      "113341/113341 [==============================] - 671s - loss: 0.0650 - acc: 0.9740 - val_loss: 0.1310 - val_acc: 0.9572\n",
      "Epoch 133/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0658 - acc: 0.9738 - val_loss: 0.1298 - val_acc: 0.9578\n",
      "Epoch 134/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0652 - acc: 0.9740 - val_loss: 0.1298 - val_acc: 0.9571\n",
      "Epoch 135/150\n",
      "113341/113341 [==============================] - 671s - loss: 0.0650 - acc: 0.9741 - val_loss: 0.1267 - val_acc: 0.9574\n",
      "Epoch 136/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0648 - acc: 0.9740 - val_loss: 0.1269 - val_acc: 0.9575\n",
      "Epoch 137/150\n",
      "113341/113341 [==============================] - 671s - loss: 0.0649 - acc: 0.9740 - val_loss: 0.1263 - val_acc: 0.9570\n",
      "Epoch 138/150\n",
      "113341/113341 [==============================] - 670s - loss: 0.0648 - acc: 0.9741 - val_loss: 0.1287 - val_acc: 0.9573\n",
      "Epoch 139/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.0647 - acc: 0.9741 - val_loss: 0.1284 - val_acc: 0.9574\n",
      "Epoch 140/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0646 - acc: 0.9742 - val_loss: 0.1300 - val_acc: 0.9568\n",
      "Epoch 141/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0643 - acc: 0.9743 - val_loss: 0.1286 - val_acc: 0.9574\n",
      "Epoch 142/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0647 - acc: 0.9741 - val_loss: 0.1282 - val_acc: 0.9575\n",
      "Epoch 143/150\n",
      "113341/113341 [==============================] - 669s - loss: 0.0646 - acc: 0.9742 - val_loss: 0.1284 - val_acc: 0.9570\n",
      "Epoch 144/150\n",
      "113341/113341 [==============================] - 667s - loss: 0.0641 - acc: 0.9743 - val_loss: 0.1295 - val_acc: 0.9573\n",
      "Epoch 145/150\n",
      "113341/113341 [==============================] - 666s - loss: 0.0641 - acc: 0.9745 - val_loss: 0.1251 - val_acc: 0.9571\n",
      "Epoch 146/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0645 - acc: 0.9744 - val_loss: 0.1330 - val_acc: 0.9571\n",
      "Epoch 147/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0641 - acc: 0.9744 - val_loss: 0.1298 - val_acc: 0.9570\n",
      "Epoch 148/150\n",
      "113341/113341 [==============================] - 668s - loss: 0.0638 - acc: 0.9745 - val_loss: 0.1326 - val_acc: 0.9564\n",
      "Epoch 149/150\n",
      "113341/113341 [==============================] - 669s - loss: 0.0638 - acc: 0.9745 - val_loss: 0.1287 - val_acc: 0.9570\n",
      "Epoch 150/150\n",
      "113341/113341 [==============================] - 669s - loss: 0.0641 - acc: 0.9744 - val_loss: 0.1294 - val_acc: 0.9571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dbb5f60>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(depth, height, width)) # N.B. depth goes first in Keras!\n",
    "\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "\n",
    "flat = Flatten()(drop_2)\n",
    "\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "out = Dense(num_classes, activation='sigmoid')(drop_3)\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"test_model_50e rotate.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "\n",
    "model.save_weights(\"test_model_50e rotate.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('test_model_50e rotate.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('test_model_50e rotate.h5')\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для формирования тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40669/40669 [09:41<00:00, 69.90it/s]\n",
      "100%|████████████████████████████████████| 20522/20522 [03:57<00:00, 86.39it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, X_test_names = DataPreperation('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakePrediction(data, model, img_names = X_test_names, treshold = 0.4, file_name = 'prediction.csv'):\n",
    "    if type(treshold) == float:\n",
    "        prediction = model.predict(data)\n",
    "        f = open(file_name, 'w')\n",
    "        f.write('image_name,tags\\n')\n",
    "        \n",
    "        for i in tqdm(range(prediction.shape[0])):\n",
    "            line = img_names[i][0:-4] + ','\n",
    "            for j in range(prediction.shape[1]):\n",
    "                if prediction[i, j] >= treshold:\n",
    "                    line += cathegories[j] + ' '\n",
    "            \n",
    "            f.write(line + '\\n')\n",
    "        \n",
    "        f.close()\n",
    "    \n",
    "    elif type(treshold) == list:\n",
    "        if len(treshold) == len(cathegories):\n",
    "            prediction = model.predict(data)\n",
    "            f = open(file_name, 'w')\n",
    "            f.write('image_name,tags\\n')\n",
    "\n",
    "            for i in tqdm(range(prediction.shape[0])):\n",
    "                line = img_names[i][0:-4] + ','\n",
    "                for j in range(prediction.shape[1]):\n",
    "                    if prediction[i, j] >= treshold[j]:\n",
    "                        line += cathegories[j] + ' '\n",
    "                \n",
    "                f.write(line + '\\n')\n",
    "            \n",
    "            f.close()\n",
    "        \n",
    "        else:\n",
    "            print('wrong treshold length. must be %d' %data.shape[1])\n",
    "    \n",
    "    else:\n",
    "        print('treshold type must be float or list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_prediction = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('x_test_prediction.csv', x_test_prediction, delimiter=',')\n",
    "np.savetxt('x_test_real.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.9000874635200844 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.14000000000000001, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "5 0.9011172139227087 [0.4, 0.16, 0.19, 0.22, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "1 0.901144562387377 [0.4, 0.059999999999999998, 0.19, 0.22, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "3 0.9011485208010113 [0.4, 0.059999999999999998, 0.19, 0.32000000000000001, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "9 0.901601188940217 [0.4, 0.059999999999999998, 0.19, 0.32000000000000001, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.12, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "2 0.9016386750795555 [0.4, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.12, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "11 0.902766057099518 [0.4, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "13 0.9037486919503214 [0.4, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.14, 0.2]\n",
      "0 0.9051616735648806 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.18, 0.089999999999999997, 0.14, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.14, 0.2]\n",
      "6 0.9052805182439626 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.18, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.14, 0.2]\n",
      "15 0.9053494153169824 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.18, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.89000000000000001, 0.2]\n",
      "4 0.9053666533358984 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.12, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.89000000000000001, 0.2]\n",
      "16 0.9057904515619364 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.12, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.28, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.89000000000000001, 0.12]\n",
      "8 0.9061755822003299 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.12, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.19, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.25, 0.89000000000000001, 0.12]\n",
      "14 0.9061776857157547 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.12, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.19, 0.12, 0.24, 0.040000000000000001, 0.27, 0.13, 0.20000000000000001, 0.89000000000000001, 0.12]\n",
      "10 0.9063750202868099 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.12, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.19, 0.12, 0.14000000000000001, 0.040000000000000001, 0.27, 0.13, 0.20000000000000001, 0.89000000000000001, 0.12]\n",
      "12 0.9065800401719656 [0.16, 0.059999999999999998, 0.20999999999999999, 0.32000000000000001, 0.12, 0.089999999999999997, 0.080000000000000002, 0.14000000000000001, 0.19, 0.12, 0.14000000000000001, 0.040000000000000001, 0.19, 0.13, 0.20000000000000001, 0.89000000000000001, 0.12]\n"
     ]
    }
   ],
   "source": [
    "#0.9067175893205208\n",
    "#initial_treshold = [0.21, 0.05, 0.16, 0.53, 0.18, 0.13, 0.03, 0.11, 0.27, 0.15, 0.2, 0.13, 0.25, 0.13, 0.22, 0.83, 0.19]\n",
    "\n",
    "#0.95018400851129\n",
    "#initial_treshold = [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
    "\n",
    "#0.949632296362803 all\n",
    "#initial_treshold = [0.38, 0.25, 0.19, 0.21, 0.19, 0.33, 0.14, 0.29, 0.26, 0.26, 0.19, 0.28, 0.23, 0.33, 0.22, 0.15, 0.21]\n",
    "\n",
    "initial_treshold = [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
    "\n",
    "\n",
    "def OptimalTreshold(x_predicted, x_real, current_treshold = initial_treshold):\n",
    "\n",
    "    current_best_result = AvgFBettaScore(x_real, x_predicted, treshold = current_treshold) \n",
    "\n",
    "    random_sequence = np.random.choice(range(len(current_treshold)), len(current_treshold), replace= False)\n",
    "\n",
    "    for k in random_sequence:\n",
    "        for i in np.arange(0, 1.01, 0.01):\n",
    "            tr = current_treshold.copy()\n",
    "            tr[k] = i\n",
    "\n",
    "            current_result = AvgFBettaScore(x_real, x_predicted, treshold = tr)\n",
    "\n",
    "            if current_result > current_best_result:\n",
    "                current_best_result = current_result\n",
    "                current_treshold = tr\n",
    "\n",
    "        print(k, current_best_result, current_treshold)\n",
    "    \n",
    "    return(current_treshold, current_best_result)\n",
    "\n",
    "x_test_predicted = loaded_model.predict(x_test)\n",
    "optimat_treshold, optimal_treshold_result = OptimalTreshold(x_test_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 61191/61191 [00:01<00:00, 33455.99it/s]\n"
     ]
    }
   ],
   "source": [
    "MakePrediction(X_test, loaded_model, treshold = optimat_treshold, file_name = 'prediction opt_tres0954 rot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "10 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "1 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "9 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "4 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "8 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "14 0.47347321382006025 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "11 0.47349272839689155 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "7 0.47349272839689155 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "3 0.47349272839689155 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "5 0.4735387055976899 [0.4, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "13 0.4735387055976899 [0.4, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "12 0.47355380258899676 [0.4, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.0, 0.34, 0.25, 0.14, 0.2]\n",
      "2 0.47355380258899676 [0.4, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.0, 0.34, 0.25, 0.14, 0.2]\n",
      "15 0.47355380258899676 [0.4, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.0, 0.34, 0.25, 0.14, 0.2]\n",
      "6 0.47355380258899676 [0.4, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.0, 0.34, 0.25, 0.14, 0.2]\n",
      "0 0.47355997863089505 [0.0, 0.16, 0.19, 0.22, 0.18, 0.0, 0.14, 0.25, 0.28, 0.25, 0.24, 0.0, 0.0, 0.34, 0.25, 0.14, 0.2]\n"
     ]
    }
   ],
   "source": [
    "x_test_predicted = loaded_model.predict(X)\n",
    "optimat_treshold, optimal_treshold_result = OptimalTreshold(Y, x_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9546428761278668"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimat_treshold = [0.38, 0.25, 0.19, 0.21, 0.19, 0.33, 0.14, 0.29, 0.26, 0.26, 0.19, 0.28, 0.23, 0.33, 0.22, 0.15, 0.21]\n",
    "AvgFBettaScore(Y, x_test_predicted, treshold = optimat_treshold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_output = loaded_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RandomForrestTresholds(x, y):\n",
    "    rf_models = []\n",
    "    \n",
    "    for i in tqdm(range(x.shape[1])):\n",
    "        model = RandomForestClassifier(n_estimators= 50)\n",
    "        model.fit(x, y[:, i])\n",
    "        \n",
    "        rf_models.append(model)\n",
    "    \n",
    "    return(rf_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [03:28<00:00, 13.43s/it]\n"
     ]
    }
   ],
   "source": [
    "random_forrest_models = RandomForrestTresholds(cnn_output, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_cnn_out_test = loaded_model.predict(x_test)\n",
    "x_rf_predictid = np.zeros(x_cnn_out_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                   | 0/17 [00:00<?, ?it/s]\n",
      "  6%|██▌                                        | 1/17 [00:00<00:05,  3.17it/s]\n",
      " 12%|█████                                      | 2/17 [00:00<00:04,  3.75it/s]\n",
      " 18%|███████▌                                   | 3/17 [00:00<00:03,  4.03it/s]\n",
      " 24%|██████████                                 | 4/17 [00:00<00:03,  4.22it/s]\n",
      " 29%|████████████▋                              | 5/17 [00:01<00:02,  4.55it/s]\n",
      " 35%|███████████████▏                           | 6/17 [00:01<00:02,  4.50it/s]\n",
      " 41%|█████████████████▋                         | 7/17 [00:01<00:02,  4.80it/s]\n",
      " 47%|████████████████████▏                      | 8/17 [00:01<00:01,  5.24it/s]\n",
      " 53%|██████████████████████▊                    | 9/17 [00:01<00:01,  4.73it/s]\n",
      " 59%|████████████████████████▋                 | 10/17 [00:02<00:01,  4.37it/s]\n",
      " 65%|███████████████████████████▏              | 11/17 [00:02<00:01,  4.45it/s]\n",
      " 71%|█████████████████████████████▋            | 12/17 [00:02<00:01,  4.68it/s]\n",
      " 76%|████████████████████████████████          | 13/17 [00:02<00:00,  4.63it/s]\n",
      " 82%|██████████████████████████████████▌       | 14/17 [00:03<00:00,  4.12it/s]\n",
      " 88%|█████████████████████████████████████     | 15/17 [00:03<00:00,  4.42it/s]\n",
      " 94%|███████████████████████████████████████▌  | 16/17 [00:03<00:00,  4.79it/s]\n",
      "100%|██████████████████████████████████████████| 17/17 [00:03<00:00,  4.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.04],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.06],\n",
       "       ..., \n",
       "       [ 0.98,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.86],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.02],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.14]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tqdm(range(len(random_forrest_models))):\n",
    "    x_rf_predictid[:, i] = random_forrest_models[i].predict_proba(x_cnn_out_test)[:, 1]\n",
    "\n",
    "x_rf_predictid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8957030072180471"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AvgFBettaScore(y_test, x_rf_predictid, betta = 2, treshold = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61191, 17)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cnn_out = loaded_model.predict(X_test)\n",
    "X_predictid = np.zeros(X_cnn_out.shape)\n",
    "X_predictid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(random_forrest_models))):\n",
    "    X_predictid[:, i] = random_forrest_models[i].predict_proba(X_cnn_out)[:, 1]\n",
    "\n",
    "X_predictid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                | 0/61191 [00:00<?, ?it/s]\n",
      " 24%|███████▌                        | 14457/61191 [00:00<00:00, 143138.51it/s]\n",
      " 48%|███████████████▍                | 29545/61191 [00:00<00:00, 144957.30it/s]\n",
      " 73%|███████████████████████▍        | 44726/61191 [00:00<00:00, 146947.20it/s]\n",
      " 89%|████████████████████████████▌   | 54522/61191 [00:00<00:00, 122965.92it/s]\n",
      "100%|████████████████████████████████| 61191/61191 [00:00<00:00, 132448.08it/s]"
     ]
    }
   ],
   "source": [
    "f = open('prediction + rf.csv', 'w')\n",
    "f.write('image_name,tags\\n')\n",
    "\n",
    "prediction = X_predictid\n",
    "img_names = X_test_names\n",
    "treshold = 0.2\n",
    "\n",
    "for i in tqdm(range(prediction.shape[0])):\n",
    "    line = img_names[i][0:-4] + ','\n",
    "    for j in range(prediction.shape[1]):\n",
    "        if prediction[i, j] >= treshold:\n",
    "            line += cathegories[j] + ' '\n",
    "\n",
    "    f.write(line + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
