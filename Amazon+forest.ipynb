{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from natsort import natsorted, ns\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PathDetermination(user):\n",
    "    if user == 'litvin_home':\n",
    "        core_dir = 'C:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "    \n",
    "    if user == 'litvin_office':\n",
    "        core_dir = 'D:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "        train_dir = core_dir + 'train-jpg\\\\'\n",
    "        test_dir = core_dir + 'test-jpg\\\\'\n",
    "        add_test_dir = core_dir + 'test-jpg-additional\\\\'\n",
    "    \n",
    "    if user == 'savina':\n",
    "        core_dir = 'Z:\\\\Kaggle Amazon Rainforest\\\\'\n",
    "        train_dir = ''\n",
    "        test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg\\\\'\n",
    "        add_test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg-additional\\\\'\n",
    "    \n",
    "    return core_dir, train_dir, test_dir, add_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_dir, train_dir, test_dir, add_test_dir = PathDetermination('litvin_office')\n",
    "#core_dir, test_dir, add_test_dir = path_determination('savina')\n",
    "\n",
    "test_data_names = natsorted(os.listdir(test_dir), key=lambda y: y.lower())\n",
    "add_test_data_names = os.listdir(add_test_dir)\n",
    "\n",
    "\n",
    "cathegories = ['agriculture', 'artisinal_mine', 'bare_ground', \n",
    "                      'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', \n",
    "                      'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', \n",
    "                      'road', 'selective_logging', 'slash_burn', 'water']\n",
    "\n",
    "train_data = pd.read_csv(core_dir + 'train_v2.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BinimialPrediction(x, treshold = 0.5):\n",
    "    result = np.zeros(x.shape)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if x[i, j] >= treshold:\n",
    "                result[i, j] = 1\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def FBettaScore(x_true, x_predicted, betta = 2):\n",
    "    if len(x_true) == len(x_predicted):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        for i in range(len(x_predicted)):\n",
    "            if x_true[i] == 1 and x_predicted[i] == 1:\n",
    "                tp += 1\n",
    "            \n",
    "            if x_true[i] == 0 and x_predicted[i] == 1:\n",
    "                fp += 1\n",
    "            \n",
    "            if x_true[i] == 1 and x_predicted[i] == 0:\n",
    "                fn += 1\n",
    "        \n",
    "        if tp == 0 or (tp + fp) == 0 or (tp + fn) == 0:\n",
    "            return(0)\n",
    "        else:\n",
    "            precision = tp/(tp + fp)\n",
    "            recall = tp/(tp + fn)\n",
    "            \n",
    "            return((1 + betta**2)*precision*recall/(betta**2*precision + recall))\n",
    "    else:\n",
    "        print('FBettaScore error! len(x_true) != len(x_predicted)')\n",
    "\n",
    "def AvgFBettaScore(x_true, x_predicted, betta = 2):\n",
    "    result = 0\n",
    "    n = x_true.shape[0]\n",
    "    \n",
    "    x_predicted = BinimialPrediction(x_predicted)\n",
    "    \n",
    "    for i in range(n):\n",
    "        result += FBettaScore(x_true[i, :], x_predicted[i, :], betta)\n",
    "    \n",
    "    return(result/n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DataPreperation(data_type, img_size = (32, 32), rotation = [0], \n",
    "                    test_img_dir = [test_dir, add_test_dir], shuffle = False):\n",
    "    if data_type == 'train':\n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        for img_name in tqdm(train_data.image_name.values):\n",
    "    \n",
    "            img = cv2.imread(train_dir + img_name + '.jpg')\n",
    "            img_resized = cv2.resize(img, img_size)\n",
    "            \n",
    "            img_tags = train_data[train_data['image_name'] == img_name]['tags'].values[0].split(' ')\n",
    "            y = np.zeros(len(cathegories))\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                if cathegories[i] in img_tags:\n",
    "                    y[i] = 1\n",
    "            \n",
    "            for angle in rotation:\n",
    "                img = imutils.rotate(img_resized, angle)\n",
    "                X.append(img)\n",
    "                Y.append(y) \n",
    "        \n",
    "        X = np.array(X, np.float16) / 255.\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        if shuffle:\n",
    "            ind = np.random.choice(np.arange(X.shape[0]), X.shape[0], replace= False)\n",
    "            X = X[ind]\n",
    "            Y = Y[ind]\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    if data_type == 'test':\n",
    "        X_test = []\n",
    "        X_test_names = []\n",
    "\n",
    "        for img_dir in test_img_dir:\n",
    "            img_dir_names = os.listdir(img_dir)\n",
    "            \n",
    "            for img_name in tqdm(img_dir_names):\n",
    "                if img_name.endswith(\"jpg\"):\n",
    "                    img = cv2.imread(img_dir + img_name)\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    \n",
    "                    X_test.append(img)\n",
    "                    X_test_names.append(img_name)\n",
    "    \n",
    "        X_test = np.array(X_test, np.float16) / 255.\n",
    "        \n",
    "        return X_test, X_test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 40479/40479 [03:44<00:00, 180.00it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = DataPreperation('train', rotation = [0, 90, 180, 270], shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиение выборки на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Настройка и  обучение сети для всех категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 100 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# загрузка изображений и их приведение к подходящему для обработки виду\n",
    "num_train = X.shape[0]\n",
    "depth = 32 \n",
    "height = 32\n",
    "width = 3 \n",
    "\n",
    "num_classes = len(cathegories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:35: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.1896 - acc: 0.9259   \n",
      "Epoch 2/200\n",
      "113341/113341 [==============================] - 478s - loss: 0.1460 - acc: 0.9429   \n",
      "Epoch 3/200\n",
      "113341/113341 [==============================] - 482s - loss: 0.1349 - acc: 0.9469   \n",
      "Epoch 4/200\n",
      "113341/113341 [==============================] - 473s - loss: 0.1289 - acc: 0.9493   \n",
      "Epoch 5/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.1236 - acc: 0.9517   \n",
      "Epoch 6/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.1204 - acc: 0.9529   \n",
      "Epoch 7/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1177 - acc: 0.9539   \n",
      "Epoch 8/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1159 - acc: 0.9547   \n",
      "Epoch 9/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1135 - acc: 0.9556   \n",
      "Epoch 10/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.1119 - acc: 0.9562   \n",
      "Epoch 11/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1104 - acc: 0.9566   \n",
      "Epoch 12/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1089 - acc: 0.9572   \n",
      "Epoch 13/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.1078 - acc: 0.9575   \n",
      "Epoch 14/200\n",
      "113341/113341 [==============================] - 449s - loss: 0.1063 - acc: 0.9580   \n",
      "Epoch 15/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1052 - acc: 0.9585   \n",
      "Epoch 16/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.1043 - acc: 0.9588   \n",
      "Epoch 17/200\n",
      "113341/113341 [==============================] - 450s - loss: 0.1030 - acc: 0.9592   \n",
      "Epoch 18/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.1023 - acc: 0.9595   \n",
      "Epoch 19/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.1009 - acc: 0.9600   \n",
      "Epoch 20/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0997 - acc: 0.9603   \n",
      "Epoch 21/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0988 - acc: 0.9607   \n",
      "Epoch 22/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0977 - acc: 0.9612   \n",
      "Epoch 23/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0971 - acc: 0.9615   \n",
      "Epoch 24/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0962 - acc: 0.9618   \n",
      "Epoch 25/200\n",
      "113341/113341 [==============================] - 453s - loss: 0.0951 - acc: 0.9619   \n",
      "Epoch 26/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0940 - acc: 0.9623   \n",
      "Epoch 27/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0933 - acc: 0.9626   \n",
      "Epoch 28/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0923 - acc: 0.9630   \n",
      "Epoch 29/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0918 - acc: 0.9631   \n",
      "Epoch 30/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0909 - acc: 0.9634   \n",
      "Epoch 31/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0902 - acc: 0.9637   \n",
      "Epoch 32/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0893 - acc: 0.9642   \n",
      "Epoch 33/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0893 - acc: 0.9641   \n",
      "Epoch 34/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0881 - acc: 0.9645   \n",
      "Epoch 35/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0874 - acc: 0.9648   \n",
      "Epoch 36/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0874 - acc: 0.9650   \n",
      "Epoch 37/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0864 - acc: 0.9651   \n",
      "Epoch 38/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0860 - acc: 0.9653   \n",
      "Epoch 39/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0855 - acc: 0.9656   \n",
      "Epoch 40/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0851 - acc: 0.9658   \n",
      "Epoch 41/200\n",
      "113341/113341 [==============================] - 451s - loss: 0.0845 - acc: 0.9660   \n",
      "Epoch 42/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0839 - acc: 0.9661   \n",
      "Epoch 43/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0834 - acc: 0.9665   \n",
      "Epoch 44/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0830 - acc: 0.9664   \n",
      "Epoch 45/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0823 - acc: 0.9669   \n",
      "Epoch 46/200\n",
      "113341/113341 [==============================] - 453s - loss: 0.0818 - acc: 0.9670   \n",
      "Epoch 47/200\n",
      "113341/113341 [==============================] - 453s - loss: 0.0815 - acc: 0.9671   \n",
      "Epoch 48/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0815 - acc: 0.9672   \n",
      "Epoch 49/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0812 - acc: 0.9673   \n",
      "Epoch 50/200\n",
      "113341/113341 [==============================] - 453s - loss: 0.0803 - acc: 0.9677   \n",
      "Epoch 51/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0803 - acc: 0.9675   \n",
      "Epoch 52/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0800 - acc: 0.9678   \n",
      "Epoch 53/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0794 - acc: 0.9680   \n",
      "Epoch 54/200\n",
      "113341/113341 [==============================] - 452s - loss: 0.0794 - acc: 0.9679   \n",
      "Epoch 55/200\n",
      "113341/113341 [==============================] - 453s - loss: 0.0786 - acc: 0.9683   \n",
      "Epoch 56/200\n",
      "113341/113341 [==============================] - 453s - loss: 0.0780 - acc: 0.9686   \n",
      "Epoch 57/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0779 - acc: 0.9686   \n",
      "Epoch 58/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0778 - acc: 0.9686   \n",
      "Epoch 59/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0776 - acc: 0.9688   \n",
      "Epoch 60/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0778 - acc: 0.9687   \n",
      "Epoch 61/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0771 - acc: 0.9690   \n",
      "Epoch 62/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0767 - acc: 0.9691   \n",
      "Epoch 63/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0763 - acc: 0.9692   \n",
      "Epoch 64/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0762 - acc: 0.9694   \n",
      "Epoch 65/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0759 - acc: 0.9693   \n",
      "Epoch 66/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0756 - acc: 0.9696   \n",
      "Epoch 67/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0753 - acc: 0.9695   \n",
      "Epoch 68/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0750 - acc: 0.9698   \n",
      "Epoch 69/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0748 - acc: 0.9698   \n",
      "Epoch 70/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0747 - acc: 0.9698   \n",
      "Epoch 71/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0745 - acc: 0.9700   \n",
      "Epoch 72/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0739 - acc: 0.9702   \n",
      "Epoch 73/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0739 - acc: 0.9702   \n",
      "Epoch 74/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0735 - acc: 0.9704   \n",
      "Epoch 75/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0733 - acc: 0.9704   \n",
      "Epoch 76/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0729 - acc: 0.9707   \n",
      "Epoch 77/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0730 - acc: 0.9707   \n",
      "Epoch 78/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0732 - acc: 0.9706   \n",
      "Epoch 79/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0725 - acc: 0.9708   \n",
      "Epoch 80/200\n",
      "113341/113341 [==============================] - 454s - loss: 0.0720 - acc: 0.9710   \n",
      "Epoch 81/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0720 - acc: 0.9709   \n",
      "Epoch 82/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0727 - acc: 0.9706   \n",
      "Epoch 83/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0721 - acc: 0.9710   \n",
      "Epoch 84/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0718 - acc: 0.9711   \n",
      "Epoch 85/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0715 - acc: 0.9712   \n",
      "Epoch 86/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0715 - acc: 0.9712   \n",
      "Epoch 87/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0709 - acc: 0.9715   \n",
      "Epoch 88/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0711 - acc: 0.9715   \n",
      "Epoch 89/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0712 - acc: 0.9714   \n",
      "Epoch 90/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0701 - acc: 0.9718   \n",
      "Epoch 91/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0706 - acc: 0.9715   \n",
      "Epoch 92/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0702 - acc: 0.9716   \n",
      "Epoch 93/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0702 - acc: 0.9718   \n",
      "Epoch 94/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0698 - acc: 0.9720   \n",
      "Epoch 95/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0700 - acc: 0.9718   \n",
      "Epoch 96/200\n",
      "113341/113341 [==============================] - 455s - loss: 0.0700 - acc: 0.9720   \n",
      "Epoch 97/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0695 - acc: 0.9721   \n",
      "Epoch 98/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0690 - acc: 0.9723   \n",
      "Epoch 99/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0694 - acc: 0.9721   \n",
      "Epoch 100/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0695 - acc: 0.9720   \n",
      "Epoch 101/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0697 - acc: 0.9721   \n",
      "Epoch 102/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0688 - acc: 0.9723   \n",
      "Epoch 103/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0684 - acc: 0.9726   \n",
      "Epoch 104/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0682 - acc: 0.9725   \n",
      "Epoch 105/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0681 - acc: 0.9726   \n",
      "Epoch 106/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0687 - acc: 0.9725   \n",
      "Epoch 107/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0683 - acc: 0.9726   \n",
      "Epoch 108/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0679 - acc: 0.9728   \n",
      "Epoch 109/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0675 - acc: 0.9730   \n",
      "Epoch 110/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0677 - acc: 0.9727   \n",
      "Epoch 111/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0683 - acc: 0.9726   \n",
      "Epoch 112/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0676 - acc: 0.9728   \n",
      "Epoch 113/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0674 - acc: 0.9730   \n",
      "Epoch 114/200\n",
      "113341/113341 [==============================] - 456s - loss: 0.0672 - acc: 0.9730   \n",
      "Epoch 115/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0673 - acc: 0.9731   \n",
      "Epoch 116/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0671 - acc: 0.9731   \n",
      "Epoch 117/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0670 - acc: 0.9732   \n",
      "Epoch 118/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0669 - acc: 0.9731   \n",
      "Epoch 119/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0666 - acc: 0.9733   \n",
      "Epoch 120/200\n",
      "113341/113341 [==============================] - 459s - loss: 0.0663 - acc: 0.9734   \n",
      "Epoch 121/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.0663 - acc: 0.9733   \n",
      "Epoch 122/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0672 - acc: 0.9732   \n",
      "Epoch 123/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0666 - acc: 0.9734   \n",
      "Epoch 124/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0662 - acc: 0.9735   \n",
      "Epoch 125/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.0662 - acc: 0.9735   \n",
      "Epoch 126/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.0662 - acc: 0.9734   \n",
      "Epoch 127/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.0661 - acc: 0.9736   \n",
      "Epoch 128/200\n",
      "113341/113341 [==============================] - 457s - loss: 0.0659 - acc: 0.9736   \n",
      "Epoch 129/200\n",
      "113341/113341 [==============================] - 458s - loss: 0.0656 - acc: 0.9737   \n",
      "Epoch 130/200\n",
      " 75392/113341 [==================>...........] - ETA: 153s - loss: 0.0651 - acc: 0.9740"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e0586446583a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m model.fit(x_train, y_train, # Train the model using the training set...\n\u001b[0;32m     34\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m           verbose=1, validation_split= 0) # ...holding out 10% of the data for validation\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1141\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1142\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2103\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(depth, height, width)) # N.B. depth goes first in Keras!\n",
    "\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "\n",
    "flat = Flatten()(drop_2)\n",
    "\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "out = Dense(num_classes, activation='sigmoid')(drop_3)\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_split= 0) # ...holding out 10% of the data for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"test_model_100e rotate.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "\n",
    "model.save_weights(\"test_model_100e rotate.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('test_model_100e rotate.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('test_model_100e rotate.h5')\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для формирования тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, X_test_names = DataPreperation('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakePrediction(data, model, img_names = X_test_names, treshold = 0.4, file_name = 'prediction.csv'):\n",
    "    if type(treshold) == float:\n",
    "        prediction = model.predict(data)\n",
    "        f = open(file_name, 'w')\n",
    "        f.write('image_name, tags\\n')\n",
    "        \n",
    "        for i in range(prediction.shape[0]):\n",
    "            line = img_names[i] + ','\n",
    "            for j in range(prediction.shape[1]):\n",
    "                if prediction[i, j] >= treshold:\n",
    "                    line += cathegories[j] + ' '\n",
    "            \n",
    "            f.write(line + '\\n')\n",
    "        \n",
    "        f.close()\n",
    "    \n",
    "    elif type(treshold) == list:\n",
    "        if len(treshold) == data.shape[1]:\n",
    "            prediction = model.predict(data)\n",
    "            f = open(file_name, 'w')\n",
    "            f.write('image_name, tags\\n')\n",
    "\n",
    "            for i in range(prediction.shape[0]):\n",
    "                line = img_names[i] + ','\n",
    "                for j in range(prediction.shape[1]):\n",
    "                    if prediction[i, j] >= treshold[j]:\n",
    "                        line += cathegories[j] + ' '\n",
    "                \n",
    "                f.write(line + '\\n')\n",
    "            \n",
    "            f.close()\n",
    "        \n",
    "        else:\n",
    "            print('wrong treshold length. must be %d' %data.shape[1])\n",
    "    \n",
    "    else:\n",
    "        print('treshold type must be float or list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MakePrediction(X_test, loaded_model, file_name = 'prediction 04 rot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
