{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from natsort import natsorted, ns\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PathDetermination(user):\n",
    "    if user == 'litvin_home':\n",
    "        core_dir = 'C:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "    \n",
    "    if user == 'litvin_office':\n",
    "        core_dir = 'D:\\\\Kaggle\\\\Understanding the Amazon from Space\\\\'\n",
    "        train_dir = core_dir + 'train-jpg\\\\'\n",
    "        test_dir = core_dir + 'test-jpg\\\\'\n",
    "        add_test_dir = core_dir + 'test-jpg-additional\\\\'\n",
    "    \n",
    "    if user == 'savina':\n",
    "        core_dir = 'Z:\\\\Kaggle Amazon Rainforest\\\\'\n",
    "        train_dir = ''\n",
    "        test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg\\\\'\n",
    "        add_test_dir = 'C:\\\\Users\\\\horch\\\\Desktop\\\\Local_data\\\\test-jpg-additional\\\\'\n",
    "    \n",
    "    return core_dir, train_dir, test_dir, add_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_dir, train_dir, test_dir, add_test_dir = PathDetermination('litvin_office')\n",
    "#core_dir, test_dir, add_test_dir = path_determination('savina')\n",
    "\n",
    "test_data_names = natsorted(os.listdir(test_dir), key=lambda y: y.lower())\n",
    "add_test_data_names = os.listdir(add_test_dir)\n",
    "\n",
    "\n",
    "cathegories = ['agriculture', 'artisinal_mine', 'bare_ground', \n",
    "                      'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', \n",
    "                      'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', \n",
    "                      'road', 'selective_logging', 'slash_burn', 'water']\n",
    "\n",
    "train_data = pd.read_csv(core_dir + 'train_v2.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BinimialPrediction(X, treshold = 0.5):\n",
    "    result = np.zeros(X.shape)\n",
    "    \n",
    "    if type(treshold) == int or type(treshold) == float:\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if X[i, j] >= treshold:\n",
    "                    result[i, j] = 1\n",
    "    elif type(treshold) == list:\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if X[i, j] >= treshold[j]:\n",
    "                    result[i, j] = 1\n",
    "    else:\n",
    "        print('treshold type must be int, float or list')\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def FBettaScore(x_true, x_predicted, betta = 2):\n",
    "    if len(x_true) == len(x_predicted):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "\n",
    "        for i in range(len(x_predicted)):\n",
    "            if x_true[i] == 1 and x_predicted[i] == 1:\n",
    "                tp += 1\n",
    "            \n",
    "            if x_true[i] == 0 and x_predicted[i] == 1:\n",
    "                fp += 1\n",
    "            \n",
    "            if x_true[i] == 1 and x_predicted[i] == 0:\n",
    "                fn += 1\n",
    "        \n",
    "        if tp == 0 or (tp + fp) == 0 or (tp + fn) == 0:\n",
    "            return(0)\n",
    "        else:\n",
    "            precision = tp/(tp + fp)\n",
    "            recall = tp/(tp + fn)\n",
    "            \n",
    "            return((1 + betta**2)*precision*recall/(betta**2*precision + recall))\n",
    "    else:\n",
    "        print('FBettaScore error! len(x_true) != len(x_predicted)')\n",
    "\n",
    "def AvgFBettaScore(x_true, x_predicted, betta = 2, treshold = 0.5):\n",
    "    result = 0\n",
    "    n = x_true.shape[0]\n",
    "    \n",
    "    x_predicted = BinimialPrediction(x_predicted, treshold)\n",
    "    \n",
    "    for i in range(n):\n",
    "        result += FBettaScore(x_true[i, :], x_predicted[i, :], betta)\n",
    "    \n",
    "    return(result/n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формирование обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DataPreperation(data_type, img_size = (32, 32), rotation = [0], \n",
    "                    test_img_dir = [test_dir, add_test_dir], shuffle = False):\n",
    "    if data_type == 'train':\n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        for img_name in tqdm(train_data.image_name.values):\n",
    "    \n",
    "            img = cv2.imread(train_dir + img_name + '.jpg')\n",
    "            img_resized = cv2.resize(img, img_size)\n",
    "            \n",
    "            img_tags = train_data[train_data['image_name'] == img_name]['tags'].values[0].split(' ')\n",
    "            y = np.zeros(len(cathegories))\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                if cathegories[i] in img_tags:\n",
    "                    y[i] = 1\n",
    "            \n",
    "            for angle in rotation:\n",
    "                img = imutils.rotate(img_resized, angle)\n",
    "                X.append(img)\n",
    "                Y.append(y) \n",
    "        \n",
    "        X = np.array(X, np.float16) / 255.\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        if shuffle:\n",
    "            ind = np.random.choice(np.arange(X.shape[0]), X.shape[0], replace= False)\n",
    "            X = X[ind]\n",
    "            Y = Y[ind]\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    if data_type == 'test':\n",
    "        X_test = []\n",
    "        X_test_names = []\n",
    "\n",
    "        for img_dir in test_img_dir:\n",
    "            img_dir_names = os.listdir(img_dir)\n",
    "            \n",
    "            for img_name in tqdm(img_dir_names):\n",
    "                if img_name.endswith(\"jpg\"):\n",
    "                    img = cv2.imread(img_dir + img_name)\n",
    "                    img = cv2.resize(img, img_size)\n",
    "                    \n",
    "                    X_test.append(img)\n",
    "                    X_test_names.append(img_name)\n",
    "    \n",
    "        X_test = np.array(X_test, np.float16) / 255.\n",
    "        \n",
    "        return X_test, X_test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40479/40479 [12:32<00:00, 53.76it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = DataPreperation('train', rotation = [0, 90, 180, 270], shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиение выборки на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Настройка и  обучение сети для всех категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 50 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# загрузка изображений и их приведение к подходящему для обработки виду\n",
    "num_train = X.shape[0]\n",
    "depth = 32 \n",
    "height = 32\n",
    "width = 3 \n",
    "\n",
    "num_classes = len(cathegories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "161916/161916 [==============================] - 811s - loss: 0.1774 - acc: 0.9307   \n",
      "Epoch 2/50\n",
      "161916/161916 [==============================] - 826s - loss: 0.1391 - acc: 0.9453   \n",
      "Epoch 3/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.1303 - acc: 0.9488   \n",
      "Epoch 4/50\n",
      "161916/161916 [==============================] - 823s - loss: 0.1239 - acc: 0.9517   \n",
      "Epoch 5/50\n",
      "161916/161916 [==============================] - 822s - loss: 0.1199 - acc: 0.9533   \n",
      "Epoch 6/50\n",
      "161916/161916 [==============================] - 822s - loss: 0.1163 - acc: 0.9546   \n",
      "Epoch 7/50\n",
      "161916/161916 [==============================] - 827s - loss: 0.1143 - acc: 0.9554   \n",
      "Epoch 8/50\n",
      "161916/161916 [==============================] - 823s - loss: 0.1125 - acc: 0.9560   \n",
      "Epoch 9/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.1108 - acc: 0.9567   \n",
      "Epoch 10/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.1088 - acc: 0.9574   \n",
      "Epoch 11/50\n",
      "161916/161916 [==============================] - 822s - loss: 0.1073 - acc: 0.9579   \n",
      "Epoch 12/50\n",
      "161916/161916 [==============================] - 826s - loss: 0.1063 - acc: 0.9583   \n",
      "Epoch 13/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.1048 - acc: 0.9587   \n",
      "Epoch 14/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.1042 - acc: 0.9590   \n",
      "Epoch 15/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.1026 - acc: 0.9595   \n",
      "Epoch 16/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.1018 - acc: 0.9597   \n",
      "Epoch 17/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.1012 - acc: 0.9602   \n",
      "Epoch 18/50\n",
      "161916/161916 [==============================] - 823s - loss: 0.1000 - acc: 0.9604   \n",
      "Epoch 19/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.0997 - acc: 0.9607   \n",
      "Epoch 20/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.0986 - acc: 0.9610   \n",
      "Epoch 21/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.0976 - acc: 0.9613   \n",
      "Epoch 22/50\n",
      "161916/161916 [==============================] - 823s - loss: 0.0968 - acc: 0.9616   \n",
      "Epoch 23/50\n",
      "161916/161916 [==============================] - 822s - loss: 0.0964 - acc: 0.9618   \n",
      "Epoch 24/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.0954 - acc: 0.9621   \n",
      "Epoch 25/50\n",
      "161916/161916 [==============================] - 831s - loss: 0.0948 - acc: 0.9623   \n",
      "Epoch 26/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.0942 - acc: 0.9623   \n",
      "Epoch 27/50\n",
      "161916/161916 [==============================] - 826s - loss: 0.0934 - acc: 0.9628   \n",
      "Epoch 28/50\n",
      "161916/161916 [==============================] - 824s - loss: 0.0931 - acc: 0.9628   \n",
      "Epoch 29/50\n",
      "161916/161916 [==============================] - 827s - loss: 0.0925 - acc: 0.9631   \n",
      "Epoch 30/50\n",
      "161916/161916 [==============================] - 828s - loss: 0.0920 - acc: 0.9633   \n",
      "Epoch 31/50\n",
      "161916/161916 [==============================] - 825s - loss: 0.0915 - acc: 0.9636   \n",
      "Epoch 32/50\n",
      "161916/161916 [==============================] - 826s - loss: 0.0910 - acc: 0.9637   \n",
      "Epoch 33/50\n",
      "161916/161916 [==============================] - 839s - loss: 0.0903 - acc: 0.9639   \n",
      "Epoch 34/50\n",
      "161916/161916 [==============================] - 830s - loss: 0.0899 - acc: 0.9641   \n",
      "Epoch 35/50\n",
      "161916/161916 [==============================] - 829s - loss: 0.0893 - acc: 0.9642   \n",
      "Epoch 36/50\n",
      "161916/161916 [==============================] - 829s - loss: 0.0890 - acc: 0.9643   \n",
      "Epoch 37/50\n",
      "161916/161916 [==============================] - 828s - loss: 0.0887 - acc: 0.9644   \n",
      "Epoch 38/50\n",
      "161916/161916 [==============================] - 829s - loss: 0.0883 - acc: 0.9646   \n",
      "Epoch 39/50\n",
      "161916/161916 [==============================] - 834s - loss: 0.0880 - acc: 0.9647   \n",
      "Epoch 40/50\n",
      "161916/161916 [==============================] - 831s - loss: 0.0876 - acc: 0.9649   \n",
      "Epoch 41/50\n",
      "161916/161916 [==============================] - 832s - loss: 0.0868 - acc: 0.9653   \n",
      "Epoch 42/50\n",
      "161916/161916 [==============================] - 836s - loss: 0.0867 - acc: 0.9653   \n",
      "Epoch 43/50\n",
      "161916/161916 [==============================] - 838s - loss: 0.0862 - acc: 0.9655   \n",
      "Epoch 44/50\n",
      "161916/161916 [==============================] - 839s - loss: 0.0858 - acc: 0.9657   \n",
      "Epoch 45/50\n",
      "161916/161916 [==============================] - 838s - loss: 0.0861 - acc: 0.9655   \n",
      "Epoch 46/50\n",
      "161916/161916 [==============================] - 844s - loss: 0.0852 - acc: 0.9658   \n",
      "Epoch 47/50\n",
      "161916/161916 [==============================] - 843s - loss: 0.0849 - acc: 0.9659   \n",
      "Epoch 48/50\n",
      "161916/161916 [==============================] - 826s - loss: 0.0846 - acc: 0.9661   \n",
      "Epoch 49/50\n",
      "161916/161916 [==============================] - 827s - loss: 0.0842 - acc: 0.9661   \n",
      "Epoch 50/50\n",
      "161916/161916 [==============================] - 828s - loss: 0.0839 - acc: 0.9663   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x101f7eb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(depth, height, width)) # N.B. depth goes first in Keras!\n",
    "\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "\n",
    "conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "\n",
    "flat = Flatten()(drop_2)\n",
    "\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "out = Dense(num_classes, activation='sigmoid')(drop_3)\n",
    "\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(x_train, y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"test_model_50e rotate.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "\n",
    "model.save_weights(\"test_model_50e rotate.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('test_model_50e rotate.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('test_model_50e rotate.h5')\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для формирования тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 40669/40669 [09:21<00:00, 72.41it/s]\n",
      "100%|███████████████████████████████████| 20522/20522 [04:05<00:00, 103.43it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, X_test_names = DataPreperation('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakePrediction(data, model, img_names = X_test_names, treshold = 0.4, file_name = 'prediction.csv'):\n",
    "    if type(treshold) == float:\n",
    "        prediction = model.predict(data)\n",
    "        f = open(file_name, 'w')\n",
    "        f.write('image_name,tags\\n')\n",
    "        \n",
    "        for i in tqdm(range(prediction.shape[0])):\n",
    "            line = img_names[i][0:-4] + ','\n",
    "            for j in range(prediction.shape[1]):\n",
    "                if prediction[i, j] >= treshold:\n",
    "                    line += cathegories[j] + ' '\n",
    "            \n",
    "            f.write(line + '\\n')\n",
    "        \n",
    "        f.close()\n",
    "    \n",
    "    elif type(treshold) == list:\n",
    "        if len(treshold) == len(cathegories):\n",
    "            prediction = model.predict(data)\n",
    "            f = open(file_name, 'w')\n",
    "            f.write('image_name,tags\\n')\n",
    "\n",
    "            for i in tqdm(range(prediction.shape[0])):\n",
    "                line = img_names[i][0:-4] + ','\n",
    "                for j in range(prediction.shape[1]):\n",
    "                    if prediction[i, j] >= treshold[j]:\n",
    "                        line += cathegories[j] + ' '\n",
    "                \n",
    "                f.write(line + '\\n')\n",
    "            \n",
    "            f.close()\n",
    "        \n",
    "        else:\n",
    "            print('wrong treshold length. must be %d' %data.shape[1])\n",
    "    \n",
    "    else:\n",
    "        print('treshold type must be float or list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "14 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "1 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "2 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "3 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "13 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "9 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "6 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "4 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "10 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "7 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "5 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "15 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "0 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "16 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "12 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
      "8 0.95018400851129 [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n"
     ]
    }
   ],
   "source": [
    "#0.9067175893205208\n",
    "#initial_treshold = [0.21, 0.05, 0.16, 0.53, 0.18, 0.13, 0.03, 0.11, 0.27, 0.15, 0.2, 0.13, 0.25, 0.13, 0.22, 0.83, 0.19]\n",
    "\n",
    "#0.95018400851129\n",
    "#initial_treshold = [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
    "\n",
    "initial_treshold = [0.4, 0.16, 0.19, 0.22, 0.18, 0.33, 0.14, 0.25, 0.28, 0.25, 0.24, 0.28, 0.27, 0.34, 0.25, 0.14, 0.2]\n",
    "\n",
    "\n",
    "def OptimalTreshold(x_predicted, x_real, current_treshold = initial_treshold):\n",
    "\n",
    "    current_best_result = AvgFBettaScore(x_real, x_predicted, treshold = current_treshold) \n",
    "\n",
    "    random_sequence = np.random.choice(range(len(current_treshold)), len(current_treshold), replace= False)\n",
    "\n",
    "    for k in random_sequence:\n",
    "        for i in np.arange(0, 1.01, 0.01):\n",
    "            tr = current_treshold.copy()\n",
    "            tr[k] = i\n",
    "\n",
    "            current_result = AvgFBettaScore(x_real, x_predicted, treshold = tr)\n",
    "\n",
    "            if current_result > current_best_result:\n",
    "                current_best_result = current_result\n",
    "                current_treshold = tr\n",
    "\n",
    "        print(k, current_best_result, current_treshold)\n",
    "    \n",
    "    return(current_treshold, current_best_result)\n",
    "\n",
    "x_test_predicted = loaded_model.predict(x_test)\n",
    "optimat_treshold, optimal_treshold_result = OptimalTreshold(x_test_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 61191/61191 [00:02<00:00, 25707.31it/s]\n"
     ]
    }
   ],
   "source": [
    "MakePrediction(X_test, loaded_model, treshold = optimat_treshold, file_name = 'prediction opt_tres095 rot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
